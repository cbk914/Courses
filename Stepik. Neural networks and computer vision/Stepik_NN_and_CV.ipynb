{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"id":"5d1bbe7b-1773-4c53-8ae2-81b8842696ad"},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Реализуйте при помощи pyTorch функцию, которая возвращает сумму (x.sum()) элементов тензора X, строго превышающих значение limit, которое является входным значением алгоритма.\n\nВходная матрица: X = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","metadata":{"id":"39c099c7-b6a1-4736-88f8-6f087fd8dc83"}},{"cell_type":"code","source":"X = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nlimit = int(input())\n\nlarger_than_limit_sum = (X[X > limit]).sum()\n\nprint(larger_than_limit_sum)","metadata":{"id":"16ac1f55-3163-4023-8d11-76ab99d99cc1","outputId":"1ac87447-676c-46ae-a66c-20e24b3bca52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Неделя 2. Строим первую нейронную сеть","metadata":{"id":"sRVurRpho41A"}},{"cell_type":"markdown","source":"2.6 Семинар: Реализация градиентного спуска (часть 1)","metadata":{"id":"nr1AD9_ipLvS"}},{"cell_type":"code","source":"import torch","metadata":{"id":"3yDtjFC9t4zZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Допустим, у нас есть функция f(x)=loge(x +3)f(x) = log_{e}(x + 3)f(x)=loge​(x +3). Мы выбрали начальное приближение xt=0 =7x^{t=0} = 7xt=0 =7 . И шаг градиентного спуска α=10\\alpha=10α=10\n\nЧему будет равен xt=1x^{t=1}xt=1? ","metadata":{"id":"pg34YJN7pUar"}},{"cell_type":"code","source":"x = torch.tensor(7.0, requires_grad=True)\nf = torch.log(x + 3)\nf.backward()\nlr = 10\nx.data = x.data - lr * x.grad.data\nprint(x.data)","metadata":{"id":"tyN-GjZBoP4y","outputId":"4e2486bb-eb16-4dd0-9167-18f4c608612c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Допустим, у нас есть функция f(X)=∑loge(xij+1)f(X) = \\sum{log_{e}(x_{ij} + 1)}f(X)=∑loge​(xij​+1), где XXX - тензор размера 2x2. Мы выбрали начальное приближение Xt=0 =[[1,2],[4,5]]X^{t=0} = [[1, 2], [4, 5]]Xt=0 =[[1,2],[4,5]] . И шаг градиентного спуска α=10\\alpha=10α=10\n\nЧему будет равен Xt=1X^{t=1}Xt=1? ","metadata":{"id":"-aK8kii25EUh"}},{"cell_type":"code","source":"X = torch.tensor([[1.0, 2.0], [4.0, 5.0]], requires_grad=True)\nf = torch.log(X + 1).sum()\nf.backward()\nlr = 10\nX.data = X.data - lr * X.grad.data\nprint(X.data)","metadata":{"id":"MLGRmDtD5JOp","outputId":"a9756524-7d0e-46d4-95d9-372b649a13ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Реализуйте расчет градиента для функции f(w)=∏i,jloge(loge(wi,j +7))f(w) = \\prod\\limits_{i,j}{log_{e}(log_{e}({w_{i,j} + 7}}))f(w)=i,j∏​loge​(loge​(wi,j​ +7)) в точке w =[[5,10],[1,2]]w = [[5, 10], [1, 2]]w =[[5,10],[1,2]]\n\nПодсказка: перемножить все значения функции можно с помощью метода .prod()","metadata":{"id":"g46Ogfd-Ieqw"}},{"cell_type":"code","source":"w = torch.tensor([[5.,10.],\n                  [1.,2.]], requires_grad=True)\n\n#######\ndevice = torch.device('cuda:0' \n                      if torch.cuda.is_available() \n                      else 'cpu')\nw = w.to(device)\n#######\n\nfunction = torch.log(torch.log(w + 7)).prod()\n#function = (w+7).log().log().prod()\n\nfunction.backward()\n\nprint(w.grad, '<- gradient')","metadata":{"id":"6Et8Zc8F6295","outputId":"4aece58a-0d78-436c-d28b-35d47d76abdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Неделя 3. Задачи решаемые при помощи нейронных сетей","metadata":{"id":"exhMKEUm4bSz"}},{"cell_type":"markdown","source":"**3.4 Теоретические задачи: Функции потерь**","metadata":{"id":"23GpcpHHh3A_"}},{"cell_type":"markdown","source":"Пусть имеется монетка, которую мы подбрасывали NNN раз, и MMM раз монетка выпала орлом вверх. Мы будем восстанавливать вероятность выпадения орла ppp при помощи минимизации бинарной кросс-энтропии:\n\np~=arg⁡min⁡p∑i=1N(−tilog⁡p−(1−ti)log⁡(1−p)),\\tilde{p} = \\arg\\min_p \\sum_{i=1}^N \\left( - t_i \\log p - (1 - t_i) \\log(1 - p) \\right),\np~​=argpmin​i=1∑N​(−ti​logp−(1−ti​)log(1−p)),где arg⁡min⁡xf(x)\\arg\\min_x f(x)argminx​f(x) -- значение xxx, при котором fff минимальна, ti=1t_i = 1ti​=1 в том случае, когда выпал орел и ti=0t_i = 0ti​=0 в том случае, когда выпала решка.","metadata":{"id":"-kVmn-BOG6Gv"}},{"cell_type":"code","source":"import sympy as sp\n\np, n, m = sp.symbols(\"p n m\")\nexpr = -(m * sp.log(p) + (n-m) * sp.log(1-p)) # расписали сумму\ndiff_expr = expr.diff(p) # производная\nsp.solve(diff_expr, 0, p) # решение","metadata":{"id":"ueRLZZqAKkdT","outputId":"c38567f2-5b17-43b6-b8a6-e54c133a7817"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.5 Семинар: Строим первую нейронную сеть**","metadata":{"id":"ThF6JqiJu2mo"}},{"cell_type":"markdown","source":"Давайте попрактикуемся с SineNet:\n\n1) Добавим еще один fc-слой\n\n2) Заменим активацию между слоями на гиперболический тангенс","metadata":{"id":"jdQFQio-vBc5"}},{"cell_type":"code","source":"#было\nclass SineNet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons):\n        super(SineNet, self).__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n        self.act1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act1(x)\n        x = self.fc2(x)\n        return x\n\nsine_net = SineNet(50)","metadata":{"id":"DBEcj-uTHE_l","outputId":"ad642f0d-1260-4629-f1c5-59605235a2a8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Решение\nimport torch\n\nclass SineNet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons):\n        super(SineNet, self).__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n        self.act1 = torch.nn.Tanh()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        self.act2 = torch.nn.Tanh()\n        self.fc3 = torch.nn.Linear(n_hidden_neurons, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act1(x)\n        x = self.fc2(x)\n        x = self.act2(x)\n        x = self.fc3(x)\n        return x\n\nsine_net = SineNet(int(input()))\nsine_net.forward(torch.Tensor([1.]))\n\nprint(sine_net)","metadata":{"id":"202YwM5iu_yJ","outputId":"e59bf899-0b3d-4dab-9be1-b6e2ece3dfeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.5.6 Семинар: Строим первую нейронную сеть","metadata":{"id":"9eyOrdOe8Ttl"}},{"cell_type":"markdown","source":"Обучим нейронную сеть для задачи регрессии:\n\nВозьмем более сложную функцию в качестве таргета: y=2xsin(2−x)y=2^x sin(2^{-x})y=2xsin(2−x).\n\nКроме того, мы хотим получить хорошую метрику MAE на валидации: MAE=1l∑i=1l∣y_predi−y_targeti∣{MAE} = {\\frac {1}{l}}\\sum _{i=1}^{l}{|y\\_pred_{i}-{y\\_target_{i}}|}MAE=l1​∑i=1l​∣y_predi​−y_targeti​∣, тогда как знакомая нам MSE выглядит как MSE=1l∑i=1l(y_predi−y_targeti)2{MSE} = {\\frac {1}{l}}\\sum _{i=1}^{l}(y\\_pred_{i}-{y\\_target_{i}})^{2} MSE=l1​∑i=1l​(y_predi​−y_targeti​)2\n\nВот пример того, как нейросеть может отрабатывать на данной функции:\n\nДанный пример показывает MAE на валидации ~0.021 . Получите метрику не хуже 0.03","metadata":{"id":"Yt1F47w88Qz-"}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (13.0, 5.0)\n\ndef target_function(x):\n    return 2**x * torch.sin(2**-x)\n\nclass RegressionNet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons):\n        super(RegressionNet, self).__init__()\n        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n        self.act1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act1(x)\n        x = self.fc2(x)\n        return x\n\nnet = RegressionNet(100)\n\n# ------Dataset preparation start--------:\nx_train =  torch.linspace(-10, 5, 100)\ny_train = target_function(x_train)\nnoise = torch.randn(y_train.shape) / 20.\ny_train = y_train + noise\nx_train.unsqueeze_(1)\ny_train.unsqueeze_(1)\n\nx_validation = torch.linspace(-10, 5, 100)\ny_validation = target_function(x_validation)\nx_validation.unsqueeze_(1)\ny_validation.unsqueeze_(1)\n# ------Dataset preparation end--------:\n\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n\ndef loss(pred, target):\n    return ((pred - target) ** 2).mean()\n\nfor epoch_index in range(2000):\n    optimizer.zero_grad()\n\n    y_pred = net.forward(x_train)\n    loss_value = loss(y_pred, y_train)\n    loss_value.backward()\n    optimizer.step()\n\n#Проверка осуществляется вызовом кода:\ndef metric(pred, target):\n    return (pred - target).abs().mean()\n\nprint(metric(net.forward(x_validation), y_validation).item())\n# (раскомментируйте, если решаете задание локально)","metadata":{"id":"59z7cwx7v4XE","outputId":"29c46e2c-0f55-4093-e294-ae72eee1b82a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(net, x, y):\n    y_pred = net.forward(x)\n\n    plt.plot(x.numpy(), y.numpy(), 'o', label='Groud truth')\n    plt.plot(x.numpy(), y_pred.data.numpy(), 'o', c='r', label='Prediction');\n    plt.legend(loc='upper left')\n    plt.xlabel('$x$')\n    plt.ylabel('$y$')\n\npredict(net, x_validation, y_validation)","metadata":{"id":"pXxiggUhZMG0","outputId":"35de0280-c9f9-47c0-ea7b-f43b823ae6f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#решение от https://stepik.org/lesson/236236/step/15?discussion=1641231&thread=solutions&unit=208641\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.rcParams['figure.figsize'] = (15.0, 5.0)\n\n# Создание набора тренировочных и тестовых данных\ndef target_function(x):\n    return 2**x * torch.sin(2**-x)\n\n# ------Dataset preparation start--------:\nx_train =  torch.linspace(-10, 5, 100)\ny_train = target_function(x_train)\nnoise = torch.randn(y_train.shape) / 20.\ny_train = y_train + noise\nx_train.unsqueeze_(1)\ny_train.unsqueeze_(1)\n\nx_validation = torch.linspace(-10, 5, 100)\ny_validation = target_function(x_validation)\nx_validation.unsqueeze_(1)\ny_validation.unsqueeze_(1)\n# ------Dataset preparation end--------:\n\n# Визуализация подготовленных данных\nplt.plot(x_train.numpy(), y_train.numpy(), 'o', label='train dataset')\n# Визуализация тестовых данных\nplt.plot(x_validation.numpy(), y_validation.numpy(), '-', label='validation dataset')\n# Визуализация валидационных данных\nplt.title('2**x * torch.sin(2**-x)')\nplt.legend(loc='upper left')\nplt.xlabel('x_validation')\nplt.ylabel('y_validation')\nplt.show()","metadata":{"id":"xSv0Eua2aSbk","outputId":"dcf178e2-914e-4271-e07a-42baf230af00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Функция оценки качества работы неросейти\ndef metric(pred, target):\n    return (pred - target).abs().mean()\n\n# Создание класса нейросети L-S-L-S-L (два скрытых слоя нейронов)\nclass RegressionNet(torch.nn.Module):\n  def __init__(self, n_hidden_neurons):\n    super(RegressionNet, self).__init__()\n    self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n    self.act1 = torch.nn.Sigmoid()\n    self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n    self.act2 = torch.nn.Sigmoid()\n    self.fc3 = torch.nn.Linear(n_hidden_neurons, 1)\n\n  # Функция определяющая последовательность применения слоев. x - это входное значение.\n  # значение x последовательно обрабатывается слоями и активационными функциями.\n  def forward(self, x):                               \n    x = self.fc1(x)                                   \n    x = self.act1(x)\n    x = self.fc2(x)\n    x = self.act2(x)\n    x = self.fc3(x)\n    return x\n\n# Создаем экземпляр класса нейросети\nnet = RegressionNet(5)\n\n# Задаем оптимизатор для нейросети\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n\n# Задаем фенкцию потерь\ndef loss(pred, target):\n  # mae = abs(pred - target)\n  # return mae.mean()\n  squares = (pred - target) ** 2\n  return squares.mean()","metadata":{"id":"k0j79MifbgFE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"При обучении нейросети записывается история изменения функции потерь (loss функции)\nи при завершении обучения сети выводиться график это функции.\nТаким образом можно визуализировать процесс обучения сети, наблюдать как меняется ошибка в процессе обучения.","metadata":{"id":"ifF7IYC7cAA_"}},{"cell_type":"code","source":"epoch_num = 2000\nloss_history = [[0,0] for i in range(epoch_num)]\n\nfor epoch_index in range(epoch_num):\n  optimizer.zero_grad()\n\n  y_pred = net.forward(x_train)\n  loss_value = loss(y_pred, y_train)\n  \n  loss_history[epoch_index][0] = epoch_index\n  loss_history[epoch_index][1] = loss_value.data.numpy().tolist()\n\n  loss_value.backward()\n  optimizer.step()\n\n# При построении отсекается первые 100 значений,\n# так как функция сначала имеет большие значения и начинает резко сходиться\nplt.plot([row[0] for row in loss_history][100:], [row[1] for row in loss_history][100:], '.')\nplt.title(label='Loss function')\nplt.xlabel('Epoch_index')\nplt.ylabel('Error');\nplt.show()","metadata":{"id":"Ohf6z2gjbjD7","outputId":"b793f67e-a130-4d5a-92f0-134899c962f4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь смотрим на результат работы нейросети","metadata":{"id":"nYSnPhYbcUzg"}},{"cell_type":"code","source":"def predict(net, x, y):\n  y_pred = net.forward(x)\n  \n  # Визуализация тестовых данных\n  plt.plot(x.numpy(), y.numpy(), '-', label='Ground trurh')                     \n  # Визуализация предсказания нейросети данных\n  plt.plot(x.numpy(), y_pred.data.numpy(), 'x', c='g', label='Prediction')      \n  plt.title('2**x * torch.sin(2**-x)')\n  plt.legend(loc='upper left')\n  plt.xlabel('x')\n  plt.ylabel('y')                                                             \n  plt.show()\n\n# Визуализация работы нейросети\npredict(net, x_validation, y_validation)\n\n# Проверка качества нейросети (погрешность)\nprint(metric(net.forward(x_validation), y_validation).item())","metadata":{"id":"hUWxKlnvcCU9","outputId":"ad505237-9775-4d97-f380-9a16111ef101"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.seed(464466656562)","metadata":{"id":"gfq4PZ8gcXEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.randint(0, 100)","metadata":{"id":"Ppj4NApvjPUX","outputId":"7f152480-e94f-4d46-e2e3-3516d098c2e3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.6 Семинар: Классификация в PyTorch**","metadata":{"id":"MI1ezK-KVna2"}},{"cell_type":"markdown","source":"Давайте попрактикуемся с WineNet. Измените архитектуру так, чтобы на вход принимались все 13 признаков и проведите следующие эксперименты:\n\n1. Поэкспериментируйте с количеством нейронов в скрытых слоях. Попробуйте поставить очень маленькое число. Существует ли пороговое значение количества скрытых нейронов, при котором обучение становится невозможным?\n\n2. Попробуйте передавать различные значения test_size в функцию train_test_split. При каком значении test_size сеть предсказывает хуже чем Base Rate*? И какой Base Rate у датасета вин?\n\n3. Зависит ли время обучения на одной эпохе от размера батча? Исследуйте эту зависимость.\n\nПоделитесь своими выводами в комментариях :)\n\n*Base Rate - значение accuracy для случая, когда модель для всех объектов предсказывает самый частотный класс в датасете","metadata":{"id":"ovkfKNWKVs4_"}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nimport pandas as pd\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"VhgKEB2SeOrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.datasets\nwine = sklearn.datasets.load_wine()\nwine.data.shape","metadata":{"id":"KrqdNkF0eOrK","outputId":"dde7bbaf-2445-4aac-8af7-b5740fc149ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.data.shape[1]","metadata":{"id":"RQmOAsuPXEk7","outputId":"24bdb32c-05fb-403e-bf66-9504b28a72c9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(wine.data)","metadata":{"id":"wl6gB6PgWbPD","outputId":"802c4720-c07b-4653-fd4d-0085ff27b34b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(wine.target).value_counts()","metadata":{"id":"3jGrkARCWvhb","outputId":"2d86ea18-1a6d-4dca-df71-76f1aa17eae0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_rate = pd.DataFrame(wine.target).value_counts().max()/pd.DataFrame(wine.target).count()\nbase_rate","metadata":{"id":"7bdddqLMX8il","outputId":"7801c1cc-6c9b-4ff2-9ba9-7e4cd6a3ad6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    wine.data, \n    wine.target, \n    test_size=0.3, \n    shuffle=True)\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)","metadata":{"id":"UH2lA6bfeOrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WineNet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons):\n        super(WineNet, self).__init__()\n        \n        self.fc1 = torch.nn.Linear(13, n_hidden_neurons)\n        self.activ1 = torch.nn.Sigmoid()\n        # self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        # self.activ2 = torch.nn.Sigmoid()\n        self.fc3 = torch.nn.Linear(n_hidden_neurons, 3)\n        self.sm = torch.nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activ1(x)\n        # x = self.fc2(x)\n        # x = self.activ2(x)\n        x = self.fc3(x)\n        return x\n\n    def inference(self, x):\n        x = self.forward(x)\n        x = self.sm(x)\n        return x\n    \nwine_net = WineNet(30)","metadata":{"id":"UtKzUXp1eOrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(wine_net.parameters(), \n                             lr=1.0e-3)","metadata":{"id":"WYfQoRIWeOrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1000\n\nfor epoch in range(5000):\n    order = np.random.permutation(len(X_train))\n    for start_index in range(0, len(X_train), batch_size):\n        optimizer.zero_grad()\n        \n        batch_indexes = order[start_index:start_index+batch_size]\n        \n        x_batch = X_train[batch_indexes]\n        y_batch = y_train[batch_indexes]\n        \n        preds = wine_net.forward(x_batch) \n        \n        loss_value = loss(preds, y_batch)\n        loss_value.backward()\n        \n        optimizer.step()\n        \n    if epoch % 100 == 0:\n        test_preds = wine_net.forward(X_test)\n        test_preds = test_preds.argmax(dim=1)\n        print((test_preds == y_test).float().mean())","metadata":{"id":"nF7ewzm-eOrR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#4 Неделя. Методы оптимизации.","metadata":{"id":"maM6ncsPe3ld"}},{"cell_type":"markdown","source":"**4.4.1 Семинар: Классификация рукописных чисел полносвязанной сетью**","metadata":{"id":"Zg7BoV0q3BE7"}},{"cell_type":"markdown","source":"Попрактикуемся с методом reshape. У нас есть трехмерный тензор размерности (6000, 28, 28) . Сопоставьте операцию над этим тензором и её результатат:","metadata":{"id":"vGKn91Ek3LO5"}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np","metadata":{"id":"75CMBjOSXSAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(np.random.default_rng(42).random((6000,28,28)))\nx.shape","metadata":{"id":"FhjRg9FM3KFA","outputId":"da5196eb-f3ca-4a54-fb22-668f4ae9cc9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(-1).shape","metadata":{"id":"RSpX5daq4SP0","outputId":"37073c03-4d76-458f-bf10-3177498d2014"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(-1,1,1).shape","metadata":{"id":"_UW_e9pb6TV7","outputId":"9e164986-d0fa-4f11-f786-36647c6d4245"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(len(x[1]), len(x), len(x[2])).shape","metadata":{"id":"P9ryDUyJ7QZy","outputId":"8b35a869-379c-4f75-e171-faa9ab995133"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(-1,14,32,7).shape","metadata":{"id":"RzFA-_S37a6N","outputId":"9e663ffa-6d32-4f11-9d6b-919517b5d925"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(-1,9).shape","metadata":{"id":"_8QwUWyM7e-l","outputId":"cc8dad63-0924-4e56-d4f7-48c4e85576b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.reshape(-1,6000).shape","metadata":{"id":"w6h9BD4y7lv8","outputId":"37f3f3f7-29f0-435f-8730-1759393393b6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверяем доступность GPU","metadata":{"id":"g5CDJnup-2Nq"}},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"id":"9YmHLly27seG","outputId":"2447913f-ada1-4863-e32d-19962841cf58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"CYUTfi7x_Aox","outputId":"50f32a7a-d8ba-4998-b893-e046a243b15d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"M9S_Swd-_GmZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.4.5 Семинар: Классификация рукописных чисел полносвязанной сетью**","metadata":{"id":"wsX1Ikhb3uiK"}},{"cell_type":"markdown","source":"Запустите код из видео на GPU. В последнем шаге мы рисовали график accuracy и loss на валидации. А что с ними происходит на train'е?\n\n    Постройте на одном графике loss для train и validation.\n    Правда ли, что loss на train и validation падает одинаково быстро и выходит на одинаковое значение, или же у нас есть переобучение?\n    Ведет ли увеличение количества эпох (40 эпох -> 200 эпох) к улучшению метрик на валидации?\n    Замерьте время вычисления 100 эпох на CPU и на GPU. Какое ускорение вы наблюдаете?\n    Замедляет ли torch.backends.cudnn.deterministic = True обучение на практике? Если да, то насколько?\n    Попробуйте разные методы градиентного спуска, которые были в лекции. Как выбор градиентного спуска влияет на accuracy? Для уверенности лучше проводить один эксперимент 3-5 раз на разных random seed: так вы поймете, действительно ли сказывается влияние метода или дело в случайности.\n","metadata":{"id":"8BzYa5sQ3y7y"}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"nkZNu2muAzxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets\nMNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\nMNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)","metadata":{"id":"-4eIRW9q4W_C","outputId":"cf224a3e-51af-4507-f0dc-668272f1868f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = MNIST_train.train_data\ny_train = MNIST_train.train_labels\nX_test = MNIST_test.test_data\ny_test = MNIST_test.test_labels","metadata":{"id":"7Gn04nUt4YjM","outputId":"4c5e4759-63d0-4a1f-a45b-02c6273de4b2","execution":{"iopub.status.busy":"2021-11-26T05:56:37.649647Z","iopub.execute_input":"2021-11-26T05:56:37.649925Z","iopub.status.idle":"2021-11-26T05:56:37.657039Z","shell.execute_reply.started":"2021-11-26T05:56:37.649895Z","shell.execute_reply":"2021-11-26T05:56:37.656229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.dtype, y_train.dtype","metadata":{"id":"8Rs237qF4fJr","outputId":"6ee24b96-35ba-4dcb-e1c5-7a375f052bce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.float()\nX_test = X_test.float()","metadata":{"id":"sHQlmMjs4i6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"id":"wLmrvicO4kbz","outputId":"62498217-4c39-4365-b00c-2fb7cd36bac6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape, y_test.shape","metadata":{"id":"BL5rC7ev4qbN","outputId":"0787ee20-33d0-4b80-f064-da2f7476d58b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(X_train[0, :, :])\nplt.show()\nprint(y_train[0])","metadata":{"id":"bqqEvJP54sn8","outputId":"2c14953f-10f1-4fc3-93f2-b617eabb3af7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape([-1, 28 * 28])\nX_test = X_test.reshape([-1, 28 * 28])","metadata":{"id":"peAFZJL-4xUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTNet(torch.nn.Module):\n\n    def __init__(self, n_hidden_neurons):\n        super(MNISTNet, self).__init__()\n        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n        self.ac1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.fc2(x)\n        return x\n\nmnist_net = MNISTNet(100)\n","metadata":{"id":"RMAiVlBT40j1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmnist_net = mnist_net.to(device)\n#list(mnist_net.parameters())","metadata":{"id":"4ZAGUg1H444X","execution":{"iopub.status.busy":"2021-11-26T05:57:06.074663Z","iopub.execute_input":"2021-11-26T05:57:06.074948Z","iopub.status.idle":"2021-11-26T05:57:06.091978Z","shell.execute_reply.started":"2021-11-26T05:57:06.074916Z","shell.execute_reply":"2021-11-26T05:57:06.090866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)","metadata":{"id":"Vy_UcIEc5BsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.to(device)\ny_test = y_test.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T05:57:11.936577Z","iopub.execute_input":"2021-11-26T05:57:11.93738Z","iopub.status.idle":"2021-11-26T05:57:11.943628Z","shell.execute_reply.started":"2021-11-26T05:57:11.937341Z","shell.execute_reply":"2021-11-26T05:57:11.942797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2021-11-26T05:57:17.328641Z","iopub.execute_input":"2021-11-26T05:57:17.329344Z","iopub.status.idle":"2021-11-26T05:57:17.338954Z","shell.execute_reply.started":"2021-11-26T05:57:17.329302Z","shell.execute_reply":"2021-11-26T05:57:17.338087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 100\n\ntest_accuracy_history = []\ntest_loss_history = []\ntrain_accuracy_history = []\ntrain_loss_history = []\n\nX_test = X_test.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(100):\n    order = np.random.permutation(len(X_train))\n\n    train_accuracy_epoch = []\n    train_loss_epoch = []\n    \n    for start_index in range(0, len(X_train), batch_size):\n        optimizer.zero_grad()\n        \n        batch_indexes = order[start_index:start_index+batch_size]\n        \n        X_batch = X_train[batch_indexes].to(device)\n        y_batch = y_train[batch_indexes].to(device)\n        \n        preds = mnist_net.forward(X_batch)\n        \n        loss_value = loss(preds, y_batch)\n        train_loss_epoch.append(loss_value)\n\n        loss_value.backward()\n        \n        train_accuracy_epoch.append((preds.argmax(dim=1) == y_batch).float().mean())\n\n        optimizer.step()\n\n    test_preds = mnist_net.forward(X_test)\n    test_loss_history.append(loss(test_preds, y_test))\n    train_loss_history.append(torch.stack(train_loss_epoch).float().mean())\n\n    accuracy_test = (test_preds.argmax(dim=1) == y_test).float().mean()\n    accuracy_train = torch.stack(train_accuracy_epoch).float().mean()\n    test_accuracy_history.append(accuracy_test)\n    train_accuracy_history.append(accuracy_train)\n    print('epoch = {}, accuracy_test = {}, accuracy_train = {}'.format(epoch, accuracy_test, accuracy_train))","metadata":{"id":"Z8VnaNTk4_Yq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_accuracy_history) #blue\nplt.plot(train_accuracy_history) #orange","metadata":{"id":"W_20Y92A5FU3","outputId":"4259ce9b-da07-42bd-c79a-d1452388ceb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_loss_history)  #blue\nplt.plot(train_loss_history) #orange","metadata":{"id":"lpTrk0gfGSDB","outputId":"5d59060c-4c31-493a-9df2-3279391aab6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"TL0mkzVv1YWS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Решение с функцией, для гибкой настройки параметров и профилирования","metadata":{"id":"gDmPZXSI1ZPD"}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (15.0, 5.0)\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"ELdpzZNx10Qd","execution":{"iopub.status.busy":"2021-11-26T06:06:11.90151Z","iopub.execute_input":"2021-11-26T06:06:11.902092Z","iopub.status.idle":"2021-11-26T06:06:13.148896Z","shell.execute_reply.started":"2021-11-26T06:06:11.902051Z","shell.execute_reply":"2021-11-26T06:06:13.148068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets\nMNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\nMNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)","metadata":{"id":"dTxq1vz010Qd","outputId":"bde8a042-84bc-45cb-e574-95f999a09c9d","execution":{"iopub.status.busy":"2021-11-26T06:06:13.150651Z","iopub.execute_input":"2021-11-26T06:06:13.150908Z","iopub.status.idle":"2021-11-26T06:06:14.839426Z","shell.execute_reply.started":"2021-11-26T06:06:13.150872Z","shell.execute_reply":"2021-11-26T06:06:14.838729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTNet(torch.nn.Module):\n\n    def __init__(self, n_hidden_neurons):\n        super(MNISTNet, self).__init__()\n        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n        self.ac1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.fc2(x)\n        return x\n\n#Create function\ndef learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=100, batch_size=100, device='cpu', verbose=True):\n    learn_history = {}\n    learn_history['accuracy_test'] = []\n    learn_history['accuracy_train'] = []\n    learn_history['loss_test'] = []\n    learn_history['loss_train'] = []\n    X_train = MNIST_train.data\n    y_train = MNIST_train.targets\n    X_test = MNIST_test.data\n    y_test = MNIST_test.targets\n\n    X_train = X_train.float()\n    X_test = X_test.float()\n    X_train = X_train.reshape([-1, 28 * 28])\n    X_test = X_test.reshape([-1, 28 * 28])  \n\n    X_test = X_test.to(device)\n    y_test = y_test.to(device)\n\n    for epoch in range(epoch_num):\n        order = np.random.permutation(len(X_train))\n\n        train_accuracy_epoch = []\n        train_loss_epoch = []\n        \n        for start_index in range(0, len(X_train), batch_size):\n            optimizer.zero_grad()\n            \n            batch_indexes = order[start_index:start_index+batch_size]\n            \n            X_batch = X_train[batch_indexes].to(device)\n            y_batch = y_train[batch_indexes].to(device)\n            \n            preds = mnist_net.forward(X_batch)\n            \n            loss_value = loss(preds, y_batch)\n            train_loss_epoch.append(loss_value)\n\n            loss_value.backward()\n            \n            train_accuracy_epoch.append((preds.argmax(dim=1) == y_batch).float().mean())\n\n            optimizer.step()\n\n        test_preds = mnist_net.forward(X_test)\n        learn_history['loss_test'].append(loss(test_preds, y_test))\n        learn_history['loss_train'].append(torch.stack(train_loss_epoch).float().mean())\n\n        accuracy_test = (test_preds.argmax(dim=1) == y_test).float().mean()\n        accuracy_train = torch.stack(train_accuracy_epoch).float().mean()\n        learn_history['accuracy_test'].append(accuracy_test)\n        learn_history['accuracy_train'].append(accuracy_train)\n        if verbose:\n            print('epoch = {}, accuracy_test = {:.3f}, accuracy_train = {:.3f}'.format(epoch, accuracy_test, accuracy_train))\n\n    return learn_history","metadata":{"id":"WZneYGGJNyBM","execution":{"iopub.status.busy":"2021-11-26T06:06:23.640828Z","iopub.execute_input":"2021-11-26T06:06:23.641156Z","iopub.status.idle":"2021-11-26T06:06:23.658295Z","shell.execute_reply.started":"2021-11-26T06:06:23.641124Z","shell.execute_reply":"2021-11-26T06:06:23.657507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs=42\nrandom.seed(rs)\nnp.random.seed(rs)\ntorch.manual_seed(rs)\ntorch.cuda.manual_seed(rs)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"VgaIUIHj8oSg","execution":{"iopub.status.busy":"2021-11-26T06:06:25.493049Z","iopub.execute_input":"2021-11-26T06:06:25.493297Z","iopub.status.idle":"2021-11-26T06:06:25.499374Z","shell.execute_reply.started":"2021-11-26T06:06:25.493269Z","shell.execute_reply":"2021-11-26T06:06:25.49844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\ndevice = 'cuda:0'\n\nmnist_net = MNISTNet(100)\nmnist_net = mnist_net.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2, momentum=0.9)\n\nlearn_history = learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=40, batch_size=1000, device=device, verbose=False)\n\nprint('Accuracy: test = {:.3f}, train = {:.3f}; Loss: test = {:.3f}, train = {:.3f};'.format(learn_history['accuracy_test'][-5].mean(), \n                                                       learn_history['accuracy_train'][-5].mean(),\n                                                       learn_history['loss_test'][-5].mean(), \n                                                       learn_history['loss_train'][-5].mean()))","metadata":{"id":"w5doBdTB1PMq","outputId":"c4413d4a-9a3c-4a06-b8ab-8849059fab37","execution":{"iopub.status.busy":"2021-11-26T06:08:46.028345Z","iopub.execute_input":"2021-11-26T06:08:46.029111Z","iopub.status.idle":"2021-11-26T06:09:43.120738Z","shell.execute_reply.started":"2021-11-26T06:08:46.029071Z","shell.execute_reply":"2021-11-26T06:09:43.120003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(learn_history['accuracy_test']) #blue\nplt.plot(learn_history['accuracy_train']) #orange","metadata":{"id":"RANZGNih731V","outputId":"7b72943a-6736-499f-aa46-aea4a0f60595"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(learn_history['loss_test'])  #blue\nplt.plot(learn_history['loss_train']) #orange","metadata":{"id":"N-_uoKbr731W","outputId":"5512634d-9a9b-43e5-936e-d822d87183de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Постройте на одном графике loss для train и validation. \n# - построил\n\n\n# Правда ли, что loss на train и validation падает одинаково быстро и выходит на одинаковое значение, или же у нас есть переобучение? \n# - нет, переобучения не замечено, падает одинаково\n\n\n# Ведет ли увеличение количества эпох (40 эпох -> 200 эпох) к улучшению метрик на валидации?\n# - да, ведет, до 100 однозначно видно улучшение метрик, со 100 до 200 рост значительно замедляется, но все же еще есть (сотые)\n\n# Замерьте время вычисления 100 эпох на CPU и на GPU. Какое ускорение вы наблюдаете?\n# - learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=10, batch_size=100, device='cuda:0', verbose=True) - 1 loop, best of 5: 12.7 s per loop\n# - learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=10, batch_size=100, device='cpu', verbose=True) - 1 loop, best of 5: 20.5 s per loop\n# - learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=10, batch_size=1000, device='cuda:0', verbose=True) - 1 loop, best of 5: 2.1 s per loop\n# - learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=10, batch_size=1000, device='cpu', verbose=True) - 1 loop, best of 5: 11.3 s per loop\n# Вывод: очень существенно сказывается на выигрыше по скорости размер батча, при батче 100 выигрыш GPU в 2 раза, при батче 1000 выигрыш GPU в 5 раз\n#        еще более сущетсвенная (на порядки) разница, наблюдается разница при увеличении кол-ва нейронов скрытых слоев, например до 1000\n\n\n# Замедляет ли torch.backends.cudnn.deterministic = True обучение на практике? Если да, то насколько?\n# - deterministic=True (neurons = 1000, batch = 1000, epoch = 10): 1 loop, best of 5: 2.58 s per loop\n# - deterministic=False (neurons = 1000, batch = 1000, epoch = 10): 1 loop, best of 5: 2.59 s per loop\n# Вывод, разницы не замечено\n\n# Попробуйте разные методы градиентного спуска, которые были в лекции. Как выбор градиентного спуска влияет на accuracy? Для уверенности лучше \n# проводить один эксперимент 3-5 раз на разных random seed: так вы поймете, действительно ли сказывается влияние метода или дело в случайности.\n# neurons = 100, batch = 1000, epoch=100, Adam(lr = 0.001) ----> Accuracy: test = 0.966, train = 0.978; Loss: test = 0.112, train = 0.072;\n# neurons = 100, batch = 1000, epoch=100, SGD(lr = 0.001) ----> Accuracy: test = 0.902, train = 0.902; Loss: test = 0.484, train = 0.484;\n# neurons = 100, batch = 1000, epoch=100, SGD(lr = 0.01) ----> Accuracy: test = 0.936, train = 0.957; Loss: test = 0.219, train = 0.163;\n# neurons = 100, batch = 1000, epoch=100, SGD(lr = 0.01, momentum=0.9) ---> Accuracy: test = 0.965, train = 0.985; Loss: test = 0.111, train = 0.061;\n# neurons = 100, batch = 1000, epoch=100, SGD(lr = 0.01, momentum=0.1) ---> Accuracy: test = 0.938, train = 0.959; Loss: test = 0.214, train = 0.156;","metadata":{"id":"7xjeStoL73OF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Пробую определить проблему из видео про нейронные сети, когда на примере рандомного изображения, показывается, что нейронная сеть работает не совсем так, как мы думали (в ролике) и она не строит никаких предположений по наличию колец, элементов и т.п.**","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (15.0, 5.0)\n\nrs=42\nrandom.seed(rs)\nnp.random.seed(rs)\ntorch.manual_seed(rs)\ntorch.cuda.manual_seed(rs)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"ELdpzZNx10Qd","execution":{"iopub.status.busy":"2021-11-26T06:37:42.385723Z","iopub.execute_input":"2021-11-26T06:37:42.386255Z","iopub.status.idle":"2021-11-26T06:37:42.392275Z","shell.execute_reply.started":"2021-11-26T06:37:42.386217Z","shell.execute_reply":"2021-11-26T06:37:42.391524Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets\nMNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\nMNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)","metadata":{"id":"dTxq1vz010Qd","outputId":"bde8a042-84bc-45cb-e574-95f999a09c9d","execution":{"iopub.status.busy":"2021-11-26T06:34:54.837057Z","iopub.execute_input":"2021-11-26T06:34:54.837437Z","iopub.status.idle":"2021-11-26T06:34:54.880344Z","shell.execute_reply.started":"2021-11-26T06:34:54.837393Z","shell.execute_reply":"2021-11-26T06:34:54.879588Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class MNISTNet(torch.nn.Module):\n\n    def __init__(self, n_hidden_neurons):\n        super(MNISTNet, self).__init__()\n        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n        self.ac1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.fc2(x)\n        return x\n\n#Create function\ndef learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=100, batch_size=100, device='cpu', verbose=True):\n    learn_history = {}\n    learn_history['accuracy_test'] = []\n    learn_history['accuracy_train'] = []\n    learn_history['loss_test'] = []\n    learn_history['loss_train'] = []\n\n    X_train = X_train.float()\n    X_test = X_test.float()\n    X_train = X_train.reshape([-1, 28 * 28])\n    X_test = X_test.reshape([-1, 28 * 28])\n\n    X_test = X_test.to(device)\n    y_test = y_test.to(device)\n\n    for epoch in range(epoch_num):\n        order = np.random.permutation(len(X_train))\n\n        train_accuracy_epoch = []\n        train_loss_epoch = []\n        \n        for start_index in range(0, len(X_train), batch_size):\n            optimizer.zero_grad()\n            \n            batch_indexes = order[start_index:start_index+batch_size]\n            \n            X_batch = X_train[batch_indexes].to(device)\n            y_batch = y_train[batch_indexes].to(device)\n            \n            preds = mnist_net.forward(X_batch)\n            \n            loss_value = loss(preds, y_batch)\n            train_loss_epoch.append(loss_value)\n\n            loss_value.backward()\n            \n            train_accuracy_epoch.append((preds.argmax(dim=1) == y_batch).float().mean())\n\n            optimizer.step()\n\n        test_preds = mnist_net.forward(X_test)\n        learn_history['loss_test'].append(loss(test_preds, y_test))\n        learn_history['loss_train'].append(torch.stack(train_loss_epoch).float().mean())\n\n        accuracy_test = (test_preds.argmax(dim=1) == y_test).float().mean()\n        accuracy_train = torch.stack(train_accuracy_epoch).float().mean()\n        learn_history['accuracy_test'].append(accuracy_test)\n        learn_history['accuracy_train'].append(accuracy_train)\n        if verbose:\n            print('epoch = {}, accuracy_test = {:.3f}, accuracy_train = {:.3f}'.format(epoch, accuracy_test, accuracy_train))\n\n    return learn_history","metadata":{"id":"WZneYGGJNyBM","execution":{"iopub.status.busy":"2021-11-26T06:34:54.882593Z","iopub.execute_input":"2021-11-26T06:34:54.883005Z","iopub.status.idle":"2021-11-26T06:34:54.900864Z","shell.execute_reply.started":"2021-11-26T06:34:54.882968Z","shell.execute_reply":"2021-11-26T06:34:54.900072Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train = MNIST_train.data\ny_train = MNIST_train.targets\nX_test = MNIST_test.data\ny_test = MNIST_test.targets","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:08:52.100323Z","iopub.execute_input":"2021-11-26T07:08:52.100898Z","iopub.status.idle":"2021-11-26T07:08:52.106492Z","shell.execute_reply.started":"2021-11-26T07:08:52.100848Z","shell.execute_reply":"2021-11-26T07:08:52.105253Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# X_train[0, :, :]","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:20:47.903905Z","iopub.execute_input":"2021-11-26T07:20:47.904300Z","iopub.status.idle":"2021-11-26T07:20:47.908858Z","shell.execute_reply.started":"2021-11-26T07:20:47.904254Z","shell.execute_reply":"2021-11-26T07:20:47.908063Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X_train[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()\nprint(y_train[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:13:53.961108Z","iopub.execute_input":"2021-11-26T07:13:53.961393Z","iopub.status.idle":"2021-11-26T07:13:54.147497Z","shell.execute_reply.started":"2021-11-26T07:13:53.961356Z","shell.execute_reply":"2021-11-26T07:13:54.146782Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"torch.randint(0, 2, (20, 20))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T06:56:02.750062Z","iopub.execute_input":"2021-11-26T06:56:02.750477Z","iopub.status.idle":"2021-11-26T06:56:02.767223Z","shell.execute_reply.started":"2021-11-26T06:56:02.750431Z","shell.execute_reply":"2021-11-26T06:56:02.766629Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"plt.imshow(torch.randint(0, 256, (1, 28, 28))[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:13:45.280134Z","iopub.execute_input":"2021-11-26T07:13:45.280388Z","iopub.status.idle":"2021-11-26T07:13:45.465023Z","shell.execute_reply.started":"2021-11-26T07:13:45.280357Z","shell.execute_reply":"2021-11-26T07:13:45.464343Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#%%timeit\ndevice = 'cuda:0'\n\nmnist_net = MNISTNet(32)\nmnist_net = mnist_net.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2, momentum=0.9)\n\nlearn_history = learn_MNISTNet(mnist_net, loss, optimizer, epoch_num=100, batch_size=1000, device=device, verbose=False)\n\nprint('Accuracy: test = {:.3f}, train = {:.3f}; Loss: test = {:.3f}, train = {:.3f};'.format(learn_history['accuracy_test'][-5].mean(), \n                                                       learn_history['accuracy_train'][-5].mean(),\n                                                       learn_history['loss_test'][-5].mean(), \n                                                       learn_history['loss_train'][-5].mean()))","metadata":{"id":"w5doBdTB1PMq","outputId":"c4413d4a-9a3c-4a06-b8ab-8849059fab37","execution":{"iopub.status.busy":"2021-11-26T07:08:59.047206Z","iopub.execute_input":"2021-11-26T07:08:59.047463Z","iopub.status.idle":"2021-11-26T07:09:16.470199Z","shell.execute_reply.started":"2021-11-26T07:08:59.047432Z","shell.execute_reply":"2021-11-26T07:09:16.469466Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(X_train[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:21:41.705254Z","iopub.execute_input":"2021-11-26T07:21:41.705512Z","iopub.status.idle":"2021-11-26T07:21:41.714588Z","shell.execute_reply.started":"2021-11-26T07:21:41.705482Z","shell.execute_reply":"2021-11-26T07:21:41.713484Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"random_img = torch.randint(0, 256, (1, 28, 28))\nplt.imshow(random_img[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:23:56.517163Z","iopub.execute_input":"2021-11-26T07:23:56.517539Z","iopub.status.idle":"2021-11-26T07:23:56.704064Z","shell.execute_reply.started":"2021-11-26T07:23:56.517487Z","shell.execute_reply":"2021-11-26T07:23:56.703383Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(random_img[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:30:40.513487Z","iopub.execute_input":"2021-11-26T07:30:40.513956Z","iopub.status.idle":"2021-11-26T07:30:40.527513Z","shell.execute_reply.started":"2021-11-26T07:30:40.513909Z","shell.execute_reply":"2021-11-26T07:30:40.526209Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"white_img = torch.randint(255, 256, (1, 28, 28))\nplt.imshow(white_img[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:29:11.026014Z","iopub.execute_input":"2021-11-26T07:29:11.026544Z","iopub.status.idle":"2021-11-26T07:29:11.207064Z","shell.execute_reply.started":"2021-11-26T07:29:11.026505Z","shell.execute_reply":"2021-11-26T07:29:11.206424Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(white_img[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:30:52.248654Z","iopub.execute_input":"2021-11-26T07:30:52.249288Z","iopub.status.idle":"2021-11-26T07:30:52.258888Z","shell.execute_reply.started":"2021-11-26T07:30:52.249250Z","shell.execute_reply":"2021-11-26T07:30:52.257937Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"black_img = torch.randint(0, 1, (1, 28, 28))\nplt.imshow(black_img[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:31:51.729483Z","iopub.execute_input":"2021-11-26T07:31:51.729762Z","iopub.status.idle":"2021-11-26T07:31:51.911168Z","shell.execute_reply.started":"2021-11-26T07:31:51.729732Z","shell.execute_reply":"2021-11-26T07:31:51.910392Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(black_img[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:31:53.136617Z","iopub.execute_input":"2021-11-26T07:31:53.136903Z","iopub.status.idle":"2021-11-26T07:31:53.147164Z","shell.execute_reply.started":"2021-11-26T07:31:53.136869Z","shell.execute_reply":"2021-11-26T07:31:53.146064Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"random_low_img = torch.randint(220, 256, (1, 28, 28))\nplt.imshow(random_low_img[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:34:16.077422Z","iopub.execute_input":"2021-11-26T07:34:16.077985Z","iopub.status.idle":"2021-11-26T07:34:16.261003Z","shell.execute_reply.started":"2021-11-26T07:34:16.077948Z","shell.execute_reply":"2021-11-26T07:34:16.260376Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(random_low_img[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:33:42.229254Z","iopub.execute_input":"2021-11-26T07:33:42.229925Z","iopub.status.idle":"2021-11-26T07:33:42.241451Z","shell.execute_reply.started":"2021-11-26T07:33:42.229881Z","shell.execute_reply":"2021-11-26T07:33:42.240320Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"plt.plot(learn_history['accuracy_test']) #blue\nplt.plot(learn_history['accuracy_train']) #orange","metadata":{"id":"RANZGNih731V","outputId":"7b72943a-6736-499f-aa46-aea4a0f60595","execution":{"iopub.status.busy":"2021-11-26T06:47:56.192811Z","iopub.execute_input":"2021-11-26T06:47:56.193611Z","iopub.status.idle":"2021-11-26T06:47:56.454166Z","shell.execute_reply.started":"2021-11-26T06:47:56.193550Z","shell.execute_reply":"2021-11-26T06:47:56.453502Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Модифицируем нейронную сеть и тестовые/трейновые датасеты, добавляем еще один класс 10, к которому относим все картинки без цифр (рандомный шум от 0 до 255, рандомный шум от 220 до 255, белый фон, черный фон)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T07:37:55.138429Z","iopub.execute_input":"2021-11-26T07:37:55.138705Z","iopub.status.idle":"2021-11-26T07:37:55.142087Z","shell.execute_reply.started":"2021-11-26T07:37:55.138674Z","shell.execute_reply":"2021-11-26T07:37:55.141418Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (15.0, 5.0)\n\nrs=42\nrandom.seed(rs)\nnp.random.seed(rs)\ntorch.manual_seed(rs)\ntorch.cuda.manual_seed(rs)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"ELdpzZNx10Qd","execution":{"iopub.status.busy":"2021-11-26T09:58:18.914055Z","iopub.execute_input":"2021-11-26T09:58:18.914328Z","iopub.status.idle":"2021-11-26T09:58:18.922509Z","shell.execute_reply.started":"2021-11-26T09:58:18.914297Z","shell.execute_reply":"2021-11-26T09:58:18.921821Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torchvision.datasets\nMNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)\nMNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)","metadata":{"id":"dTxq1vz010Qd","outputId":"bde8a042-84bc-45cb-e574-95f999a09c9d","execution":{"iopub.status.busy":"2021-11-26T09:58:18.924395Z","iopub.execute_input":"2021-11-26T09:58:18.925055Z","iopub.status.idle":"2021-11-26T09:58:21.102063Z","shell.execute_reply.started":"2021-11-26T09:58:18.925021Z","shell.execute_reply":"2021-11-26T09:58:21.101352Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_train = MNIST_train.data\ny_train = MNIST_train.targets\nX_test = MNIST_test.data\ny_test = MNIST_test.targets","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.103220Z","iopub.execute_input":"2021-11-26T09:58:21.103873Z","iopub.status.idle":"2021-11-26T09:58:21.108707Z","shell.execute_reply.started":"2021-11-26T09:58:21.103833Z","shell.execute_reply":"2021-11-26T09:58:21.107410Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape[0]/10) # на это количество добавляем картинок с 10 категорией (отсутсвие цифр)\nprint(X_test.shape[0]/10) # на это количество добавляем картинок с 10 категорией (отсутсвие цифр)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.109824Z","iopub.execute_input":"2021-11-26T09:58:21.110435Z","iopub.status.idle":"2021-11-26T09:58:21.119774Z","shell.execute_reply.started":"2021-11-26T09:58:21.110401Z","shell.execute_reply":"2021-11-26T09:58:21.118969Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_train_no_numbers = []\nX_test_no_numbers = []\nX_train_no_numbers.append(torch.randint(0, 1, (300, 28, 28))) #6000/10/2 = 300, черных картинок\nX_test_no_numbers.append(torch.randint(0, 1, (50, 28, 28))) #1000/10/2 = 50, черных картинок","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.121673Z","iopub.execute_input":"2021-11-26T09:58:21.122338Z","iopub.status.idle":"2021-11-26T09:58:21.142168Z","shell.execute_reply.started":"2021-11-26T09:58:21.122278Z","shell.execute_reply":"2021-11-26T09:58:21.141565Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train_no_numbers.append(torch.randint(255, 256, (300, 28, 28))) # + 6000/10/2 = 300, белых картинок\nX_test_no_numbers.append(torch.randint(255, 256, (50, 28, 28))) # + 1000/10/2 = 50, белых картинок","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.143203Z","iopub.execute_input":"2021-11-26T09:58:21.143769Z","iopub.status.idle":"2021-11-26T09:58:21.153131Z","shell.execute_reply.started":"2021-11-26T09:58:21.143733Z","shell.execute_reply":"2021-11-26T09:58:21.152424Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# + 6000/10*9 = 5400 с рандомным диапазоном шума\ntemp = []\nfor _ in range(5400):\n    low_rand = np.random.randint(0,256)\n    high_rand = np.random.randint(low_rand+1,256+1)\n    temp.append(torch.randint(low_rand, high_rand, (1, 28, 28)))\nX_train_no_numbers.append(torch.cat(temp))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.154760Z","iopub.execute_input":"2021-11-26T09:58:21.155490Z","iopub.status.idle":"2021-11-26T09:58:21.347298Z","shell.execute_reply.started":"2021-11-26T09:58:21.155435Z","shell.execute_reply":"2021-11-26T09:58:21.346571Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# + 100/10*9 = 900 с рандомным диапазоном шума\ntemp = []\nfor _ in range(900):\n    low_rand = np.random.randint(0,256)\n    high_rand = np.random.randint(low_rand+1,256+1)\n    temp.append(torch.randint(low_rand, high_rand, (1, 28, 28)))\nX_test_no_numbers.append(torch.cat(temp))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.348425Z","iopub.execute_input":"2021-11-26T09:58:21.348750Z","iopub.status.idle":"2021-11-26T09:58:21.383116Z","shell.execute_reply.started":"2021-11-26T09:58:21.348709Z","shell.execute_reply":"2021-11-26T09:58:21.382493Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X_train_no_numbers[2][31, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.384155Z","iopub.execute_input":"2021-11-26T09:58:21.384420Z","iopub.status.idle":"2021-11-26T09:58:21.586968Z","shell.execute_reply.started":"2021-11-26T09:58:21.384386Z","shell.execute_reply":"2021-11-26T09:58:21.586301Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Добавляем в наши обучающие и тестовые выборки данные по классу 10 (без цифр)\nX_train = torch.cat((X_train, torch.cat(X_train_no_numbers)))\ny_train = torch.cat((y_train, torch.full((6000,), 10)))\nX_test = torch.cat((X_test, torch.cat(X_test_no_numbers)))\ny_test = torch.cat((y_test, torch.full((1000,), 10)))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.588267Z","iopub.execute_input":"2021-11-26T09:58:21.588537Z","iopub.status.idle":"2021-11-26T09:58:21.906622Z","shell.execute_reply.started":"2021-11-26T09:58:21.588502Z","shell.execute_reply":"2021-11-26T09:58:21.905875Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.908011Z","iopub.execute_input":"2021-11-26T09:58:21.908270Z","iopub.status.idle":"2021-11-26T09:58:21.914438Z","shell.execute_reply.started":"2021-11-26T09:58:21.908237Z","shell.execute_reply":"2021-11-26T09:58:21.913521Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MNISTNet(torch.nn.Module):\n\n    def __init__(self, n_hidden_neurons):\n        super(MNISTNet, self).__init__()\n        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n        self.ac1 = torch.nn.Sigmoid()\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, 11)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.fc2(x)\n        return x\n\n#Create function\ndef learn_MNISTNet(mnist_net, X_train, X_test, y_train, y_test, loss, optimizer, epoch_num=100, batch_size=100, device='cpu', verbose=True):\n    learn_history = {}\n    learn_history['accuracy_test'] = []\n    learn_history['accuracy_train'] = []\n    learn_history['loss_test'] = []\n    learn_history['loss_train'] = []\n\n    X_train = X_train.float()\n    X_test = X_test.float()\n    X_train = X_train.reshape([-1, 28 * 28])\n    X_test = X_test.reshape([-1, 28 * 28])\n\n    X_test = X_test.to(device)\n    y_test = y_test.to(device)\n\n    for epoch in range(epoch_num):\n        order = np.random.permutation(len(X_train))\n\n        train_accuracy_epoch = []\n        train_loss_epoch = []\n        \n        for start_index in range(0, len(X_train), batch_size):\n            optimizer.zero_grad()\n            \n            batch_indexes = order[start_index:start_index+batch_size]\n            \n            X_batch = X_train[batch_indexes].to(device)\n            y_batch = y_train[batch_indexes].to(device)\n            \n            preds = mnist_net.forward(X_batch)\n            \n            loss_value = loss(preds, y_batch)\n            train_loss_epoch.append(loss_value)\n\n            loss_value.backward()\n            \n            train_accuracy_epoch.append((preds.argmax(dim=1) == y_batch).float().mean())\n\n            optimizer.step()\n\n        test_preds = mnist_net.forward(X_test)\n        learn_history['loss_test'].append(loss(test_preds, y_test))\n        learn_history['loss_train'].append(torch.stack(train_loss_epoch).float().mean())\n\n        accuracy_test = (test_preds.argmax(dim=1) == y_test).float().mean()\n        accuracy_train = torch.stack(train_accuracy_epoch).float().mean()\n        learn_history['accuracy_test'].append(accuracy_test)\n        learn_history['accuracy_train'].append(accuracy_train)\n        if verbose:\n            print('epoch = {}, accuracy_test = {:.3f}, accuracy_train = {:.3f}'.format(epoch, accuracy_test, accuracy_train))\n\n    return learn_history","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.916099Z","iopub.execute_input":"2021-11-26T09:58:21.916363Z","iopub.status.idle":"2021-11-26T09:58:21.934238Z","shell.execute_reply.started":"2021-11-26T09:58:21.916330Z","shell.execute_reply":"2021-11-26T09:58:21.933379Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#%%timeit\n#device = 'cuda:0'\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmnist_net = MNISTNet(100)\nmnist_net = mnist_net.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(mnist_net.parameters(), lr=1.0e-3)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2)\n#optimizer = torch.optim.SGD(mnist_net.parameters(), lr=1.0e-2, momentum=0.9)\n\nlearn_history = learn_MNISTNet(mnist_net, X_train, X_test, y_train, y_test, loss, optimizer, epoch_num=100, batch_size=1000, device=device, verbose=False)\n\nprint('Accuracy: test = {:.3f}, train = {:.3f}; Loss: test = {:.3f}, train = {:.3f};'.format(learn_history['accuracy_test'][-5].mean(), \n                                                       learn_history['accuracy_train'][-5].mean(),\n                                                       learn_history['loss_test'][-5].mean(), \n                                                       learn_history['loss_train'][-5].mean()))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:21.935771Z","iopub.execute_input":"2021-11-26T09:58:21.936261Z","iopub.status.idle":"2021-11-26T09:58:44.131123Z","shell.execute_reply.started":"2021-11-26T09:58:21.936225Z","shell.execute_reply":"2021-11-26T09:58:44.130351Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"random_img = torch.randint(200, 256, (1, 28, 28))\nplt.imshow(random_img[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:44.133714Z","iopub.execute_input":"2021-11-26T09:58:44.134227Z","iopub.status.idle":"2021-11-26T09:58:44.321119Z","shell.execute_reply.started":"2021-11-26T09:58:44.134187Z","shell.execute_reply":"2021-11-26T09:58:44.320415Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"preds = mnist_net.forward(random_img[0, :, :].float().reshape([-1, 28 * 28]).to(device))\nprint(preds)\nprint(preds.argmax(dim=1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:58:44.322257Z","iopub.execute_input":"2021-11-26T09:58:44.323137Z","iopub.status.idle":"2021-11-26T09:58:44.362966Z","shell.execute_reply.started":"2021-11-26T09:58:44.323096Z","shell.execute_reply":"2021-11-26T09:58:44.362145Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#смотрим на чем сеть ошибается.\ntest_preds = mnist_net.forward(X_test.float().reshape([-1, 28 * 28]).to(device))\nX_errors = X_test[test_preds.argmax(dim=1) != y_test.to(device)]","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:16:17.206997Z","iopub.execute_input":"2021-11-26T10:16:17.207247Z","iopub.status.idle":"2021-11-26T10:16:17.249676Z","shell.execute_reply.started":"2021-11-26T10:16:17.207219Z","shell.execute_reply":"2021-11-26T10:16:17.248955Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nplt.imshow(X_errors[0, :, :], cmap='gray', vmin=0, vmax=255)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:17:07.416060Z","iopub.execute_input":"2021-11-26T10:17:07.416310Z","iopub.status.idle":"2021-11-26T10:17:07.588018Z","shell.execute_reply.started":"2021-11-26T10:17:07.416281Z","shell.execute_reply":"2021-11-26T10:17:07.587303Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:14:18.411270Z","iopub.execute_input":"2021-11-26T10:14:18.411542Z","iopub.status.idle":"2021-11-26T10:14:18.420671Z","shell.execute_reply.started":"2021-11-26T10:14:18.411512Z","shell.execute_reply":"2021-11-26T10:14:18.419798Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**4.4.6 Семинар: Классификация рукописных чисел полносвязанной сетью**","metadata":{"id":"kL_NNsIvSu4_"}},{"cell_type":"markdown","source":"Как было сказано в предыдущем уроке, полносвязный слой может быть представлен как матричное умножение матрицы входов (X) и матрицы весов нейронов слоя (W), плюс вектор bias'ов слоя (b). \n\nВ документации к классу torch.nn.Linear (полносвязному слою) написано следующее: Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b . А здесь – это то, как PyTorch хранит веса слоя. Но чтобы эта матрица совпала с W из предыдущего урока, нужно её сперва транспонировать.\n\nДавайте реализуем функциональность torch.nn.Linear и сверим с оригиналом!\n\nПусть у нас будет 1 объект x на входе с двумя компонентами. Его мы передадим в полносвязный слой с 3-мя нейронами и получим, соотсветственно, 3 выхода. После напишем эту же функциональность с помощью матричного умножения. ","metadata":{"id":"hpMONSJNS77x"}},{"cell_type":"code","source":"import torch\n\n# Сперва создадим тензор x:\nx = torch.tensor([[10., 20.]])\n\n# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\nfc = torch.nn.Linear(2, 3)\n\n# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n\n# Давайте проставим свои значения в веса и bias'ы:\nw = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\nfc.weight.data = w\nfc.weight.data","metadata":{"id":"fSTBSEdeS650","outputId":"8d75195a-e20a-488c-8bcb-e83cbfeb7b72","execution":{"iopub.status.busy":"2021-11-26T09:57:28.772105Z","iopub.status.idle":"2021-11-26T09:57:28.772532Z","shell.execute_reply.started":"2021-11-26T09:57:28.772287Z","shell.execute_reply":"2021-11-26T09:57:28.772309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fc.weight.data.transpose(0,1)","metadata":{"id":"EKDP3Bf5VHUJ","outputId":"3886e775-79f0-4ea1-f6fa-845c3b3ec177","execution":{"iopub.status.busy":"2021-11-26T09:57:28.863180Z","iopub.status.idle":"2021-11-26T09:57:28.863595Z","shell.execute_reply.started":"2021-11-26T09:57:28.863383Z","shell.execute_reply":"2021-11-26T09:57:28.863402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = torch.tensor([[31., 32., 33.]])\nfc.bias.data = b\nfc.bias.data","metadata":{"id":"V6ttweW1S9qk","outputId":"5d4def41-6e8d-48dd-fec2-787caa51e100","execution":{"iopub.status.busy":"2021-11-26T09:57:28.864713Z","iopub.status.idle":"2021-11-26T09:57:28.865011Z","shell.execute_reply.started":"2021-11-26T09:57:28.864855Z","shell.execute_reply":"2021-11-26T09:57:28.864872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Получим выход fc-слоя:\nfc_out = fc(x)\nfc_out","metadata":{"id":"KkD3o2v6UWNB","outputId":"5173adb3-34f9-4418-f6e1-6ac594839a58","execution":{"iopub.status.busy":"2021-11-26T09:57:28.866273Z","iopub.status.idle":"2021-11-26T09:57:28.866759Z","shell.execute_reply.started":"2021-11-26T09:57:28.866600Z","shell.execute_reply":"2021-11-26T09:57:28.866619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Попробуем теперь получить аналогичные выходы с помощью матричного перемножения:\nfc_out_alternative =  x @ w.t() + b\nfc_out_alternative\n\n# Проверка осуществляется автоматически вызовом функции\n# print(fc_out == fc_out_alternative)\n# (раскомментируйте, если решаете задачу локально)","metadata":{"id":"b88u0etbUaSc","outputId":"fd852f20-1f68-4767-82d5-e66a00799ec3","execution":{"iopub.status.busy":"2021-11-26T09:57:28.867843Z","iopub.status.idle":"2021-11-26T09:57:28.868409Z","shell.execute_reply.started":"2021-11-26T09:57:28.868173Z","shell.execute_reply":"2021-11-26T09:57:28.868201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В предыдущем шаге мы написали функцию, эмулирующую fc-слой. Проверим, что по ней правильно считается градиент. \n\nФункцию backward() в PyTorch можно посчитать только от скалярной функции (выход из такой функции – одно число). Это логично, так как loss-функция выдает всегда одно число. Но fc-слой, который мы проэмулировали, имел 3 выхода. Предлагаем их просуммировать, чтобы получить в итоге скалярную функцию. Заметим, впрочем, что можно было бы выбрать любую агрегирующую операцию, например умножение.\n\nДополните код так, чтобы градиент по весам и смещениям (bias) совпадал с аналогичным градиентом в вашей фунции.\n\nЧем обусловлен полученный градиент? Изменится ли он, если мы подадим другие входы или другую инициализацию весов?","metadata":{"id":"DaiqrxVHmCHk"}},{"cell_type":"code","source":"import torch\n\n# Сперва создадим тензор x:\nx = torch.tensor([[10., 20.]])\n\n# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\nfc = torch.nn.Linear(2, 3)\n\n# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\n# fc.weight и fc.bias по умолчанию инициализируются случайными числами\n\n# Давайте проставим свои значения в веса и bias'ы:\nw = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\nfc.weight.data = w\n\nb = torch.tensor([[31., 32., 33.]])\nfc.bias.data = b\n\n# Получим выход fc-слоя:\nfc_out = fc(x)\n# Просуммируем выход fc-слоя, чтобы получить скаляр:\nfc_out_summed = fc_out.sum()\n\n# Посчитаем градиенты формулы fc_out_summed:\nfc_out_summed.backward()\nweight_grad = fc.weight.grad\nbias_grad = fc.bias.grad\n\n# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\n# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\nw.requires_grad_(True)\nb.requires_grad_(True)\n\n# Получим выход нашей формулы:\nour_formula = (x @ w.t() + b).sum() # SUM{x * w^T + b}\n\n# Сделайте backward для нашей формулы:\nour_formula.backward()\n\n# Проверка осуществляется автоматически, вызовом функций:\nprint('fc_weight_grad:', weight_grad)\nprint('our_weight_grad:', w.grad)\nprint('fc_bias_grad:', bias_grad)\nprint('out_bias_grad:', b.grad)\n# (раскомментируйте, если работаете над задачей локально)","metadata":{"id":"_ln7NUnDVeAZ","outputId":"14e794b4-af7f-4de4-a817-3f357e0d635a","execution":{"iopub.status.busy":"2021-11-26T09:57:28.869453Z","iopub.status.idle":"2021-11-26T09:57:28.870009Z","shell.execute_reply.started":"2021-11-26T09:57:28.869770Z","shell.execute_reply":"2021-11-26T09:57:28.869797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Неделя 5. Сверточные нейронные сети","metadata":{"id":"5fkeBgKmoc18"}},{"cell_type":"markdown","source":"5.1 Свёртка, каскад свёрток","metadata":{"id":"jLM9iSqQmg-p"}},{"cell_type":"markdown","source":"Пусть матрица признаков равна [[4, 2, -1],[-6, 0, 5],[3, 2, 2]], а ядро свертки -- [[0, 1, 2],[1, -1, 0],[1, 0, -2]]\n\nКаков будет результат применения свертки со stride=2, padding=1?\n\nЗапишите значения матрицы в одну строчку в порядке сверху вниз, слева направо, разделяя числа одной строки пробелами, а сами строки -- запятыми (например, запись \"1 2 3, -4 -5 -6\" соответствует матрице [[1, 2, 3], [-4, -5, -6]], осторожно, скобки писать не нужно).","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2021-11-26T11:19:38.083393Z","iopub.execute_input":"2021-11-26T11:19:38.083735Z","iopub.status.idle":"2021-11-26T11:19:39.325325Z","shell.execute_reply.started":"2021-11-26T11:19:38.083646Z","shell.execute_reply":"2021-11-26T11:19:39.324440Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"source = torch.tensor([[4, 2, -1],[-6, 0, 5],[3, 2, 2]])\nkernel = torch.tensor([[0, 1, 2],[1, -1, 0],[1, 0, -2]])\nsource = torch.nn.functional.pad(input=source, pad=(1, 1, 1, 1), mode='constant', value=0) #add padding","metadata":{"execution":{"iopub.status.busy":"2021-11-26T11:26:19.028209Z","iopub.execute_input":"2021-11-26T11:26:19.028959Z","iopub.status.idle":"2021-11-26T11:26:19.037867Z","shell.execute_reply.started":"2021-11-26T11:26:19.028912Z","shell.execute_reply":"2021-11-26T11:26:19.036971Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nconv = torch.nn.Conv2d(1,1,kernel_size=3, padding=1, stride=2, bias=False)\nX = torch.FloatTensor([[[\n    [4, 2, -1],\n    [-6, 0, 5],\n    [3, 2, 2]]]])\nconv.weight.data = torch.FloatTensor([[[\n    [0, 1, 2],\n    [1, -1, 0],\n    [1, 0, -2]]]])\nres = conv(X).data[0,0]\nprint(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nx = torch.tensor([[4, 2, -1],[-6, 0, 5],[3, 2, 2]], dtype=torch.float64).reshape(1,1,3,3)\nk = torch.tensor([[0, 1, 2],[1, -1, 0],[1, 0, -2]], dtype=torch.float64).reshape(1,1,3,3)\nconv = torch.nn.functional.conv2d(x, k, stride=2, padding=1).numpy()\nprint(*conv.reshape(-1).astype(int))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T11:33:38.381279Z","iopub.execute_input":"2021-11-26T11:33:38.382099Z","iopub.status.idle":"2021-11-26T11:33:38.396120Z","shell.execute_reply.started":"2021-11-26T11:33:38.382057Z","shell.execute_reply":"2021-11-26T11:33:38.395359Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Пусть размер матрицы признаков = 4(высота)х5(ширина)x2(кол-во каналов), размеры ядра свертки = 3x3, stride=2, padding=1, количество выходных фильтров = 8.\n\nСколько элементов будет в матрице, полученной в результате применения свёртки? В качестве ответа введите одно число.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nx = torch.ones(1,2,4,5)\nconv = nn.Conv2d(2, 8, kernel_size=3, stride=2, padding=1, bias=False)\nres = conv(x)\nres.view(-1).shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T12:09:42.249518Z","iopub.execute_input":"2021-11-26T12:09:42.249848Z","iopub.status.idle":"2021-11-26T12:09:43.481892Z","shell.execute_reply.started":"2021-11-26T12:09:42.249766Z","shell.execute_reply":"2021-11-26T12:09:43.481040Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}