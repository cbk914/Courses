{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('../input/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-23T13:41:03.916020Z","iopub.execute_input":"2021-12-23T13:41:03.916318Z","iopub.status.idle":"2021-12-23T13:41:05.703797Z","shell.execute_reply.started":"2021-12-23T13:41:03.916279Z","shell.execute_reply":"2021-12-23T13:41:05.702868Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/working/plates/'\nprint(os.listdir(data_root))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:05.706440Z","iopub.execute_input":"2021-12-23T13:41:05.706963Z","iopub.status.idle":"2021-12-23T13:41:05.712004Z","shell.execute_reply.started":"2021-12-23T13:41:05.706886Z","shell.execute_reply":"2021-12-23T13:41:05.710905Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#remove all files .DS_Store\nfor parent, dirnames, filenames in os.walk(data_root):\n    for fn in filenames:\n        if fn.lower().endswith('.ds_store'):\n            os.remove(os.path.join(parent, fn))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:05.713910Z","iopub.execute_input":"2021-12-23T13:41:05.714385Z","iopub.status.idle":"2021-12-23T13:41:05.727773Z","shell.execute_reply.started":"2021-12-23T13:41:05.714291Z","shell.execute_reply":"2021-12-23T13:41:05.726801Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# The shutil module offers a number of high-level operations on files and collections of files. \n# In particular, functions are provided which support file copying and removal.\nimport shutil\nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\n#create dirs ./train/cleaned, /train/dirty, ./val/cleaned, /val/dirty,\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\n#fill directories train (5/6)=83% and val (1/6)=17%, \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-12-23T13:41:06.288024Z","iopub.execute_input":"2021-12-23T13:41:06.288325Z","iopub.status.idle":"2021-12-23T13:41:06.318604Z","shell.execute_reply.started":"2021-12-23T13:41:06.288277Z","shell.execute_reply":"2021-12-23T13:41:06.317503Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport time\nimport copy\nfrom datetime import datetime\n\nfrom torchvision import transforms, models\n\n# rs = 0\n# random.seed(rs)\n# np.random.seed(rs)\n# torch.manual_seed(rs)\n# torch.cuda.manual_seed(rs)\n# torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:08.239699Z","iopub.execute_input":"2021-12-23T13:41:08.239994Z","iopub.status.idle":"2021-12-23T13:41:09.228963Z","shell.execute_reply.started":"2021-12-23T13:41:08.239954Z","shell.execute_reply":"2021-12-23T13:41:09.228215Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Transforms are common image transformations. They can be chained together using Compose. \n# Most transform classes have a function equivalent: functional transforms give fine-grained \n# control over the transformations. This is useful if you have to build a more complex transformation \n# pipeline (e.g. in the case of segmentation tasks).\n\n# ILLUSTRATION OF TRANSFORMS: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n\ntrain_transforms = transforms.Compose([\n    #!!очень спорное применение RandomResizedCrop для нашего случая, \n    #т.к. по итогу абсолютно получается рандомно вырезанный, сжатый/растянутый кусок исходного изображения\n    transforms.RandomResizedCrop(224), \n    transforms.RandomHorizontalFlip(), #с вероятностью 50% отражаем по горизонтали\n    transforms.ToTensor(),\n    #нормализуем в соостветсвии с тем, как, были предобработаны изображения imagenet1000, при обучении resnet\n    # mean = [0.485, 0.456, 0.406], эти значения вычитаются из RGB каналов изображения (т.е. нормализуем смещение)\n    # std = [0.229, 0.224, 0.225], на эти значения делим RGB каналы изображения \n    #      (т.е. нормализцем среднеквадратичное (стандартное) отклонение, std^2 = var)\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), #важно, не квадратные изображения будут сжиматься/растягиваться!\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n#получаем объект типа .ImageFolder, для дальнейшего использования через torch.utils.data.DataLoader\n#по сути связывает директорию с изображениями, разбитыми ко поддиректориям=классам, с трансформациями, \n#которые будут производиться при дальнейшей загрузке изображений в память (в том числе в композиции \n#трансформаций выше сразу зашито преобразование в тензор)\n#Важные особенности:\n# - указанная директория обязательно должна содержать поддиректории соответсвующие названиям классов, \n#   уже в свою очередь в которых должны находиться изображения\n# - по итогу, мы получаем объект типа ImageFolder, который на первый взгляд является последовательностью \n#   (sequential), длины = кол-ву изображений во всех поддиректориях, но по факту, является только ссылками\n#   на изображения, т.е. в память сразу они не загружаются, а только в момент обращения к конкретному\n#   объекту; простая проверка, если удалить изображение(-я) с диска, заранее созданный объект ImageFolder\n#   уже не сможет предоставить доступ к соответсвующему удаленному изображению объекту (с исключением о \n#   невозможности найти файл)\n# - важно понимать!, что каждый раз при обращении к конкретному элементу (например с индексом 0), весь \n#   процесс загрузки с диска и применение трансформаций будет запускаться заново, как следствие, если мы\n#   имеем рандомные трансформации, т.е. каждый раз при обращении к одному и тому же объекту мы будем \n#   получать разные результаты\n# - для нашего случая трансформаций (где присутсвует .ToTensor()), мы получим последовательность кортежей, \n#   из 2-х подэлементов, 0-й элемент = тензору соответсвующему изображению, 1-й элемент целое число, \n#   отнесение к классу (насколько я понял, классы назначаются в порядке алфавитной сортировки поддиректорий, \n#   т.е. для нашего случая cleaned = 0, dirty = 1)\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\nlen(train_dataset), len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:09.778580Z","iopub.execute_input":"2021-12-23T13:41:09.778895Z","iopub.status.idle":"2021-12-23T13:41:09.795040Z","shell.execute_reply.started":"2021-12-23T13:41:09.778846Z","shell.execute_reply":"2021-12-23T13:41:09.794098Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# На основе .ImageFolder генерируем подобие итератора/последовательности псевдо-батчей указанного размера \n# (в зависимости от shuffle, перемешиваем их или нет), псевдо-, потому что данные не загружаются сразу, \n# а только в момет обращения к ним, для этого указывается количество воркеров (для многопоточности).\n# Важные примечания:\n# - с учетом нашей реализации .ImageFolder, где происходят рандомные трансформации, каждый раз получем\n#   рандомно измененные изображения, даже для одинаковых батчей\n# - изображения в каждом батче, так же перемешиваются каждый раз при вызове запросе батча\n# - это все же, как понял это некоторый микс итератора/последовательности, т.к. например, у объекта есть \n#   размер len(dataloader), но при этом обратиться к индексу нельзя, просто вызвать next(dataloader)\n#   нельзя, зато можно вызвать предварительно приведя к классическому итератору через next(iter(dataloader))\n# - при получении следующего батча (пример выше), возвращает 2 объекта/датасета с фичами (X) и labels (y)\n#   (для нашего случая возвращает 2 тензора, каждый батч фичей будет torch.Size([8, 3, 224, 224]), батч \n#   лейблов torch.Size([8]))\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:11.904225Z","iopub.execute_input":"2021-12-23T13:41:11.904570Z","iopub.status.idle":"2021-12-23T13:41:11.911360Z","shell.execute_reply.started":"2021-12-23T13:41:11.904511Z","shell.execute_reply":"2021-12-23T13:41:11.910293Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:13.399544Z","iopub.execute_input":"2021-12-23T13:41:13.399843Z","iopub.status.idle":"2021-12-23T13:41:13.405113Z","shell.execute_reply.started":"2021-12-23T13:41:13.399802Z","shell.execute_reply":"2021-12-23T13:41:13.404453Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# X_batch, y_batch = next(iter(train_dataloader))\n# mean = np.array([0.485, 0.456, 0.406])\n# std = np.array([0.229, 0.224, 0.225])\n# #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n# #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n# #размерами изображения (на 2 месте)\n# #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n# #противоположную той, которую произвели в трансформере во время загрузки изображения\n# plt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean); ","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:14.359707Z","iopub.execute_input":"2021-12-23T13:41:14.360147Z","iopub.status.idle":"2021-12-23T13:41:14.364230Z","shell.execute_reply.started":"2021-12-23T13:41:14.359953Z","shell.execute_reply":"2021-12-23T13:41:14.363297Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def show_input(input_tensor, title='', \n               mean = np.array([0.485, 0.456, 0.406]), \n               std = np.array([0.229, 0.224, 0.225])):\n    '''Функция реализующая вывод изображений с заголовком = классу'''\n    \n    #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n    #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n    #размерами изображения (на 2 месте)\n    #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n    #противоположную той, которую произвели в трансформере во время загрузки изображения\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1)) #.clip загоняет все значения в заданный интервал, т.е. [0, 1]\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:15.905452Z","iopub.execute_input":"2021-12-23T13:41:15.905806Z","iopub.status.idle":"2021-12-23T13:41:18.469295Z","shell.execute_reply.started":"2021-12-23T13:41:15.905731Z","shell.execute_reply":"2021-12-23T13:41:18.468097Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def show_experiments_plots(accuracies, losses, figsize = (16.0, 6.0)):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in accuracies.keys():\n        print('{:-<100}'.format(experiment_id))\n        epoch_max_acc = np.array(accuracies[experiment_id]['val']).argmax()\n        print('Max val accuracy on: Epoch = {:>3},     ACCURACY: val = {:.3f}, train = {:.3f},     LOSS: val = {:.3f}, train = {:.3f}'\\\n              .format(epoch_max_acc, \n                      accuracies[experiment_id]['val'][epoch_max_acc], \n                      accuracies[experiment_id]['train'][epoch_max_acc],\n                      losses[experiment_id]['val'][epoch_max_acc],\n                      losses[experiment_id]['train'][epoch_max_acc]))\n        epoch_min_loss = np.array(losses[experiment_id]['val']).argmin()\n        print('Min val loss on:     Epoch = {:>3},     ACCURACY: val = {:.3f}, train = {:.3f},     LOSS: val = {:.3f}, train = {:.3f}'\\\n              .format(epoch_min_loss, \n                      accuracies[experiment_id]['val'][epoch_min_loss], \n                      accuracies[experiment_id]['train'][epoch_min_loss],\n                      losses[experiment_id]['val'][epoch_min_loss],\n                      losses[experiment_id]['train'][epoch_min_loss]))\n    \n    for experiment_id in accuracies.keys():\n        plt.plot(accuracies[experiment_id]['val'], label=experiment_id + ' val')\n    plt.legend()\n    plt.title('Validation Accuracy (Val only)')\n    plt.show()\n\n    for experiment_id in accuracies.keys():\n        plt.plot(accuracies[experiment_id]['val'], label=experiment_id + ' val')\n        plt.plot(accuracies[experiment_id]['train'], label=experiment_id + ' train')\n    plt.legend()\n    plt.title('Validation Accuracy (Val/Train)');\n    plt.show()\n\n    for experiment_id in losses.keys():\n        plt.plot(losses[experiment_id]['val'], label=experiment_id  + ' val')\n    plt.legend()\n    plt.title('Validation Loss (Val only)');\n    plt.show()\n\n    for experiment_id in losses.keys():\n        plt.plot(losses[experiment_id]['val'], label=experiment_id  + ' val')\n        plt.plot(losses[experiment_id]['train'], label=experiment_id  + ' train')\n    plt.legend()\n    plt.title('Validation Loss (Val/Train)');\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:54:32.545417Z","iopub.execute_input":"2021-12-23T13:54:32.545699Z","iopub.status.idle":"2021-12-23T13:54:32.564575Z","shell.execute_reply.started":"2021-12-23T13:54:32.545662Z","shell.execute_reply":"2021-12-23T13:54:32.563611Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def train_model(model, loss, optimizer, scheduler, num_epochs, with_validation = True):\n    \n    accuracy_history = {}\n    loss_history = {}\n    accuracy_history['train'] = []\n    loss_history['train'] = []\n    accuracy_history['val'] = []\n    loss_history['val'] = []\n    \n    for epoch in range(num_epochs):\n        \n        #####train phase######\n        train_accuracy_epoch = [] #for statistics\n        train_loss_epoch = [] #for statistics\n        \n        model.train()  # Set model to training mode\n        scheduler.step() #no grad step, only change lr of optimizer\n\n        for inputs, labels in train_dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                preds = model(inputs) #аналогично вызову метода model.forward(inputs)\n                loss_value = loss(preds, labels)\n                \n                loss_value.backward()\n                optimizer.step()\n                \n                train_accuracy_epoch.append(float((preds.argmax(dim=1) == labels.data).float().mean().data))\n                #.item() Returns the value of this tensor as a standard Python number. This only works for tensors with one element.\n                train_loss_epoch.append(loss_value.item())\n                \n        #####validation phase######\n        val_accuracy_epoch = []\n        val_loss_epoch = []\n        \n        model.eval()   #Set model to evaluate mode\n        \n        for inputs, labels in val_dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n        \n                with torch.no_grad(): #не храним градиенты для теста, многоркатно сокращает потребление памяти                  \n                    preds = model(inputs) #аналогично вызову метода model.forward(inputs)\n                    loss_value = loss(preds, labels)\n                \n                    val_accuracy_epoch.append(float((preds.argmax(dim=1) == labels.data).float().mean().data))\n                    val_loss_epoch.append(loss_value.item())\n                    \n        ########ending of epoch#########\n        accuracy_history['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n        loss_history['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n        accuracy_history['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n        loss_history['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))    \n        \n        print('Epoch = {:>3}/{},     ACCURACY: val = {:.3f}, train = {:.3f},     LOSS: val = {:.3f}, train = {:.3f}'\\\n                  .format(epoch,\n                          num_epochs - 1,\n                          accuracy_history['val'][-1], \n                          accuracy_history['train'][-1],\n                          loss_history['val'][-1],\n                          loss_history['train'][-1]), \n                  flush=True)\n\n    return model, accuracy_history, loss_history","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:42:29.316902Z","iopub.execute_input":"2021-12-23T13:42:29.317364Z","iopub.status.idle":"2021-12-23T13:42:29.336249Z","shell.execute_reply.started":"2021-12-23T13:42:29.317309Z","shell.execute_reply":"2021-12-23T13:42:29.335528Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"accuracies = {}\nlosses = {}","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:20.247800Z","iopub.execute_input":"2021-12-23T13:41:20.248455Z","iopub.status.idle":"2021-12-23T13:41:20.252690Z","shell.execute_reply.started":"2021-12-23T13:41:20.248099Z","shell.execute_reply":"2021-12-23T13:41:20.251914Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = models.resnet18(pretrained=True) #загружаем предтренированную на imagenet1000 модель\n\n# Disable grad for all conv layers (скорей всего морозим градиенты не только conv)\nfor param in model.parameters():\n    param.requires_grad = False\n    \n#заменяем последний полносвязный слой, содержащий 1000 нейронов (для 1000 классов imagenet1000)\n#на полносвязный слой из 2-х нейронов, для наших 2-х классов (кстати странно что не используется \n#классическая бинарная конфигурация, с 1-м выходом и BCE, нужно проверить, даст ли замена на \n#классику более лучший результат)\n#последний слой хранится в параметре .fc экземпляра класса, до замены он был = \n#Linear(in_features=512, out_features=1000, bias=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n# Единственное назначение .lr_scheduler.StepLR, это изменение LR, оптимизатора, путем его домножения \n# на gamma, каждые step_size эпох\n# Важно понимать, что scheduler считает эпохи путем явного выхова scheduler.step(),\n# а так же, что его .step() не делает ничего более кроме изменения LR каждые step_size эпох т.е. \n# scheduler.step() не отменяет необходимости вызова optimizer.step() для осуществления град. шага\n\n# gamma - гиперпараметр, соответсвенно имеет смысл им поиграть на практике и найти оптимальный\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:41:28.635673Z","iopub.execute_input":"2021-12-23T13:41:28.636294Z","iopub.status.idle":"2021-12-23T13:41:30.518319Z","shell.execute_reply.started":"2021-12-23T13:41:28.636227Z","shell.execute_reply":"2021-12-23T13:41:30.517510Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"net_name = 'resnet18'\nmodel, accuracies[net_name], losses[net_name] = train_model(model, loss, optimizer, scheduler, num_epochs=100)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:42:33.759206Z","iopub.execute_input":"2021-12-23T13:42:33.759662Z","iopub.status.idle":"2021-12-23T13:47:07.651607Z","shell.execute_reply.started":"2021-12-23T13:42:33.759621Z","shell.execute_reply":"2021-12-23T13:47:07.650528Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"show_experiments_plots(accuracies, losses)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:54:50.421592Z","iopub.execute_input":"2021-12-23T13:54:50.421901Z","iopub.status.idle":"2021-12-23T13:54:51.340306Z","shell.execute_reply.started":"2021-12-23T13:54:50.421853Z","shell.execute_reply":"2021-12-23T13:54:51.339468Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_dir = 'test'\n#создаем в test поддиректорию не существующего класса unknown, и копируем туда test изображения\n#т.к. .ImageFolder требует, что бы в целевой директории обязательно были поддиректории=классы\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:56:01.662485Z","iopub.execute_input":"2021-12-23T13:56:01.663036Z","iopub.status.idle":"2021-12-23T13:56:01.834231Z","shell.execute_reply.started":"2021-12-23T13:56:01.662970Z","shell.execute_reply":"2021-12-23T13:56:01.833291Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#создаем класс ImageFolderWithPaths, на основе ImageFolder, с единственным изменением метода .__getitem__\n#что бы кортеж элементов, помимо изображения и класса, содержал еще 3-й элемент = полный путь к изображению\n#это необходимо для формирования submit csv, где первый столбец = индекcу фото (имя файла без расширения),\n#второй столбец = отнесению к классу\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,)) #сумма кортежей = concat из всех элементов\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('/kaggle/working/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:56:02.106258Z","iopub.execute_input":"2021-12-23T13:56:02.106635Z","iopub.status.idle":"2021-12-23T13:56:02.119492Z","shell.execute_reply.started":"2021-12-23T13:56:02.106581Z","shell.execute_reply":"2021-12-23T13:56:02.118770Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model.eval() #не забываем перевести модель в eval режим\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths) #непонятно почему здесь extend, а не append\n    \ntest_predictions = np.concatenate(test_predictions) #преобразуем list в np.array","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:56:03.640999Z","iopub.execute_input":"2021-12-23T13:56:03.641492Z","iopub.status.idle":"2021-12-23T13:56:40.731110Z","shell.execute_reply.started":"2021-12-23T13:56:03.641434Z","shell.execute_reply":"2021-12-23T13:56:40.729316Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T13:58:02.365906Z","iopub.execute_input":"2021-12-23T13:58:02.366240Z","iopub.status.idle":"2021-12-23T13:58:05.041711Z","shell.execute_reply.started":"2021-12-23T13:58:02.366191Z","shell.execute_reply":"2021-12-23T13:58:05.040213Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:36.112373Z","iopub.execute_input":"2021-12-17T07:27:36.112728Z","iopub.status.idle":"2021-12-17T07:27:36.120184Z","shell.execute_reply.started":"2021-12-17T07:27:36.112667Z","shell.execute_reply":"2021-12-17T07:27:36.119021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned') #threshold (отсечка) для отнесения к классу \nsubmission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:41.668433Z","iopub.execute_input":"2021-12-17T07:27:41.668786Z","iopub.status.idle":"2021-12-17T07:27:41.715008Z","shell.execute_reply.started":"2021-12-17T07:27:41.668711Z","shell.execute_reply":"2021-12-17T07:27:41.714268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission_{}.csv'.format(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:28:54.203059Z","iopub.execute_input":"2021-12-17T07:28:54.203449Z","iopub.status.idle":"2021-12-17T07:28:54.300812Z","shell.execute_reply.started":"2021-12-17T07:28:54.203387Z","shell.execute_reply":"2021-12-17T07:28:54.300043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf train val test plates","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:05.616624Z","iopub.execute_input":"2021-12-21T13:12:05.616958Z","iopub.status.idle":"2021-12-21T13:12:06.474065Z","shell.execute_reply.started":"2021-12-21T13:12:05.616905Z","shell.execute_reply":"2021-12-21T13:12:06.473036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:11.67344Z","iopub.execute_input":"2021-12-21T13:12:11.673735Z","iopub.status.idle":"2021-12-21T13:12:12.465077Z","shell.execute_reply.started":"2021-12-21T13:12:11.673697Z","shell.execute_reply":"2021-12-21T13:12:12.464044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}