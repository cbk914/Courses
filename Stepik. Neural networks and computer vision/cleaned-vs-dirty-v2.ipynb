{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('../input/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-22T12:38:05.290634Z","iopub.execute_input":"2021-12-22T12:38:05.291077Z","iopub.status.idle":"2021-12-22T12:38:07.242089Z","shell.execute_reply.started":"2021-12-22T12:38:05.290997Z","shell.execute_reply":"2021-12-22T12:38:07.241069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/working/plates/'\nprint(os.listdir(data_root))","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:07.243873Z","iopub.execute_input":"2021-12-22T12:38:07.244116Z","iopub.status.idle":"2021-12-22T12:38:07.249178Z","shell.execute_reply.started":"2021-12-22T12:38:07.244073Z","shell.execute_reply":"2021-12-22T12:38:07.248234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove all files .DS_Store\nfor parent, dirnames, filenames in os.walk(data_root):\n    for fn in filenames:\n        if fn.lower().endswith('.ds_store'):\n            os.remove(os.path.join(parent, fn))","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:07.250816Z","iopub.execute_input":"2021-12-22T12:38:07.251091Z","iopub.status.idle":"2021-12-22T12:38:07.262869Z","shell.execute_reply.started":"2021-12-22T12:38:07.251044Z","shell.execute_reply":"2021-12-22T12:38:07.261993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The shutil module offers a number of high-level operations on files and collections of files. \n# In particular, functions are provided which support file copying and removal.\nimport shutil\nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\n#create dirs ./train/cleaned, /train/dirty, ./val/cleaned, /val/dirty,\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\n#fill directories train (5/6)=83% and val (1/6)=17%, \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-12-22T12:38:07.2643Z","iopub.execute_input":"2021-12-22T12:38:07.264621Z","iopub.status.idle":"2021-12-22T12:38:07.29214Z","shell.execute_reply.started":"2021-12-22T12:38:07.264526Z","shell.execute_reply":"2021-12-22T12:38:07.291453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\nfrom torchvision import transforms, models","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:07.294368Z","iopub.execute_input":"2021-12-22T12:38:07.294649Z","iopub.status.idle":"2021-12-22T12:38:08.254368Z","shell.execute_reply.started":"2021-12-22T12:38:07.294597Z","shell.execute_reply":"2021-12-22T12:38:08.253562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms are common image transformations. They can be chained together using Compose. \n# Most transform classes have a function equivalent: functional transforms give fine-grained \n# control over the transformations. This is useful if you have to build a more complex transformation \n# pipeline (e.g. in the case of segmentation tasks).\n\n# ILLUSTRATION OF TRANSFORMS: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n\ntrain_transforms = transforms.Compose([\n    #!!очень спорное применение RandomResizedCrop для нашего случая, \n    #т.к. по итогу абсолютно получается рандомно вырезанный, сжатый/растянутый кусок исходного изображения\n    transforms.RandomResizedCrop(224), \n    transforms.RandomHorizontalFlip(), #с вероятностью 50% отражаем по горизонтали\n    transforms.ToTensor(),\n    #нормализуем в соостветсвии с тем, как, были предобработаны изображения imagenet1000, при обучении resnet\n    # mean = [0.485, 0.456, 0.406], эти значения вычитаются из RGB каналов изображения (т.е. нормализуем смещение)\n    # std = [0.229, 0.224, 0.225], на эти значения делим RGB каналы изображения \n    #      (т.е. нормализцем среднеквадратичное (стандартное) отклонение, std^2 = var)\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), #важно, не квадратные изображения будут сжиматься/растягиваться!\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n#получаем объект типа .ImageFolder, для дальнейшего использования через torch.utils.data.DataLoader\n#по сути связывает директорию с изображениями, разбитыми ко поддиректориям=классам, с трансформациями, \n#которые будут производиться при дальнейшей загрузке изображений в память (в том числе в композиции \n#трансформаций выше сразу зашито преобразование в тензор)\n#Важные особенности:\n# - указанная директория обязательно должна содержать поддиректории соответсвующие названиям классов, \n#   уже в свою очередь в которых должны находиться изображения\n# - по итогу, мы получаем объект типа ImageFolder, который на первый взгляд является последовательностью \n#   (sequential), длины = кол-ву изображений во всех поддиректориях, но по факту, является только ссылками\n#   на изображения, т.е. в память сразу они не загружаются, а только в момент обращения к конкретному\n#   объекту; простая проверка, если удалить изображение(-я) с диска, заранее созданный объект ImageFolder\n#   уже не сможет предоставить доступ к соответсвующему удаленному изображению объекту (с исключением о \n#   невозможности найти файл)\n# - важно понимать!, что каждый раз при обращении к конкретному элементу (например с индексом 0), весь \n#   процесс загрузки с диска и применение трансформаций будет запускаться заново, как следствие, если мы\n#   имеем рандомные трансформации, т.е. каждый раз при обращении к одному и тому же объекту мы будем \n#   получать разные результаты\n# - для нашего случая трансформаций (где присутсвует .ToTensor()), мы получим последовательность кортежей, \n#   из 2-х подэлементов, 0-й элемент = тензору соответсвующему изображению, 1-й элемент целое число, \n#   отнесение к классу (насколько я понял, классы назначаются в порядке алфавитной сортировки поддиректорий, \n#   т.е. для нашего случая cleaned = 0, dirty = 1)\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\nlen(train_dataset), len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:08.25587Z","iopub.execute_input":"2021-12-22T12:38:08.256115Z","iopub.status.idle":"2021-12-22T12:38:08.271955Z","shell.execute_reply.started":"2021-12-22T12:38:08.256073Z","shell.execute_reply":"2021-12-22T12:38:08.271088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# На основе .ImageFolder генерируем подобие итератора/последовательности псевдо-батчей указанного размера \n# (в зависимости от shuffle, перемешиваем их или нет), псевдо-, потому что данные не загружаются сразу, \n# а только в момет обращения к ним, для этого указывается количество воркеров (для многопоточности).\n# Важные примечания:\n# - с учетом нашей реализации .ImageFolder, где происходят рандомные трансформации, каждый раз получем\n#   рандомно измененные изображения, даже для одинаковых батчей\n# - изображения в каждом батче, так же перемешиваются каждый раз при вызове запросе батча\n# - это все же, как понял это некоторый микс итератора/последовательности, т.к. например, у объекта есть \n#   размер len(dataloader), но при этом обратиться к индексу нельзя, просто вызвать next(dataloader)\n#   нельзя, зато можно вызвать предварительно приведя к классическому итератору через next(iter(dataloader))\n# - при получении следующего батча (пример выше), возвращает 2 объекта/датасета с фичами (X) и labels (y)\n#   (для нашего случая возвращает 2 тензора, каждый батч фичей будет torch.Size([8, 3, 224, 224]), батч \n#   лейблов torch.Size([8]))\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:08.273502Z","iopub.execute_input":"2021-12-22T12:38:08.273775Z","iopub.status.idle":"2021-12-22T12:38:08.282991Z","shell.execute_reply.started":"2021-12-22T12:38:08.273729Z","shell.execute_reply":"2021-12-22T12:38:08.282316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:08.284148Z","iopub.execute_input":"2021-12-22T12:38:08.28449Z","iopub.status.idle":"2021-12-22T12:38:08.30137Z","shell.execute_reply.started":"2021-12-22T12:38:08.284433Z","shell.execute_reply":"2021-12-22T12:38:08.300586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_batch, y_batch = next(iter(train_dataloader))\n# mean = np.array([0.485, 0.456, 0.406])\n# std = np.array([0.229, 0.224, 0.225])\n# #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n# #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n# #размерами изображения (на 2 месте)\n# #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n# #противоположную той, которую произвели в трансформере во время загрузки изображения\n# plt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean); ","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:08.303905Z","iopub.execute_input":"2021-12-22T12:38:08.304345Z","iopub.status.idle":"2021-12-22T12:38:08.312658Z","shell.execute_reply.started":"2021-12-22T12:38:08.304269Z","shell.execute_reply":"2021-12-22T12:38:08.311836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_input(input_tensor, title='', \n               mean = np.array([0.485, 0.456, 0.406]), \n               std = np.array([0.229, 0.224, 0.225])):\n    '''Функция реализующая вывод изображений с заголовком = классу'''\n    \n    #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n    #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n    #размерами изображения (на 2 месте)\n    #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n    #противоположную той, которую произвели в трансформере во время загрузки изображения\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1)) #.clip загоняет все значения в заданный интервал, т.е. [0, 1]\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:08.314047Z","iopub.execute_input":"2021-12-22T12:38:08.314342Z","iopub.status.idle":"2021-12-22T12:38:10.902173Z","shell.execute_reply.started":"2021-12-22T12:38:08.314301Z","shell.execute_reply":"2021-12-22T12:38:10.901071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, loss, optimizer, scheduler, num_epochs, with_validation = True):\n    \n    val_accuracy_history = []\n    val_loss_history = []\n    train_accuracy_history = []\n    train_loss_history = []\n    \n    for epoch in range(num_epochs):\n        \n        #####train phase######\n        train_accuracy_epoch = [] #for statistics\n        train_loss_epoch = [] #for statistics\n        \n        model.train()  # Set model to training mode\n        scheduler.step() #no grad step, only change lr of optimizer\n\n        for inputs, labels in train_dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                preds = model(inputs) #аналогично вызову метода model.forward(inputs)\n                loss_value = loss(preds, labels)\n                \n                loss_value.backward()\n                optimizer.step()\n                \n                train_accuracy_epoch.append(float((preds.argmax(dim=1) == labels.data).float().mean().data))\n                #.item() Returns the value of this tensor as a standard Python number. This only works for tensors with one element.\n                train_loss_epoch.append(loss_value.item())\n                \n        #####validation phase######\n        val_accuracy_epoch = []\n        val_loss_epoch = []\n        \n        model.eval()   #Set model to evaluate mode\n        \n        for inputs, labels in val_dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n        \n                with torch.no_grad(): #не храним градиенты для теста, многоркатно сокращает потребление памяти                  \n                    preds = model(inputs) #аналогично вызову метода model.forward(inputs)\n                    loss_value = loss(preds, labels)\n                \n                    val_accuracy_epoch.append(float((preds.argmax(dim=1) == labels.data).float().mean().data))\n                    val_loss_epoch.append(loss_value.item())\n                    \n        ########ending of epoch#########\n        val_accuracy_history.append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n        val_loss_history.append(sum(val_loss_epoch) / len(val_loss_epoch))\n        train_accuracy_history.append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n        train_loss_history.append(sum(train_loss_epoch) / len(train_loss_epoch))    \n        \n        print('Epoch = {:>3}/{:>3},     ACCURACY: test = {:.3f}, train = {:.3f},     LOSS: test = {:.3f}, train = {:.3f}'\\\n                  .format(epoch,\n                          num_epochs - 1,\n                          test_accuracy_history[-1], \n                          train_accuracy_history[-1],\n                          test_loss_history[-1],\n                          train_loss_history[-1]), \n                  flush=True)\n\n    return model, val_accuracy_history, train_accuracy_history, val_loss_history, train_loss_history","metadata":{"execution":{"iopub.status.busy":"2021-12-22T12:38:10.904505Z","iopub.execute_input":"2021-12-22T12:38:10.905504Z","iopub.status.idle":"2021-12-22T12:38:10.936911Z","shell.execute_reply.started":"2021-12-22T12:38:10.905416Z","shell.execute_reply":"2021-12-22T12:38:10.935508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet18(pretrained=True) #загружаем предтренированную на imagenet1000 модель\n\n# Disable grad for all conv layers (скорей всего морозим градиенты не только conv)\nfor param in model.parameters():\n    param.requires_grad = False\n    \n#заменяем последний полносвязный слой, содержащий 1000 нейронов (для 1000 классов imagenet1000)\n#на полносвязный слой из 2-х нейронов, для наших 2-х классов (кстати странно что не используется \n#классическая бинарная конфигурация, с 1-м выходом и BCE, нужно проверить, даст ли замена на \n#классику более лучший результат)\n#последний слой хранится в параметре .fc экземпляра класса, до замены он был = \n#Linear(in_features=512, out_features=1000, bias=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n# Единственное назначение .lr_scheduler.StepLR, это изменение LR, оптимизатора, путем его домножения \n# на gamma, каждые step_size эпох\n# Важно понимать, что scheduler считает эпохи путем явного выхова scheduler.step(),\n# а так же, что его .step() не делает ничего более кроме изменения LR каждые step_size эпох т.е. \n# scheduler.step() не отменяет необходимости вызова optimizer.step() для осуществления град. шага\n\n# gamma - гиперпараметр, соответсвенно имеет смысл им поиграть на практике и найти оптимальный\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:59:28.684305Z","iopub.execute_input":"2021-12-21T10:59:28.684871Z","iopub.status.idle":"2021-12-21T10:59:29.559451Z","shell.execute_reply.started":"2021-12-21T10:59:28.684819Z","shell.execute_reply":"2021-12-21T10:59:29.558579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, val_accuracy_history, train_accuracy_history, val_loss_history, train_loss_history = \\\ntrain_model(model, loss, optimizer, scheduler, num_epochs=100)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:59:37.511083Z","iopub.execute_input":"2021-12-21T10:59:37.511742Z","iopub.status.idle":"2021-12-21T11:00:00.579528Z","shell.execute_reply.started":"2021-12-21T10:59:37.511669Z","shell.execute_reply":"2021-12-21T11:00:00.57799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir = 'test'\n#создаем в test поддиректорию не существующего класса unknown, и копируем туда test изображения\n#т.к. .ImageFolder требует, что бы в целевой директории обязательно были поддиректории=классы\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T12:31:02.361066Z","iopub.execute_input":"2021-12-21T12:31:02.361378Z","iopub.status.idle":"2021-12-21T12:31:02.582164Z","shell.execute_reply.started":"2021-12-21T12:31:02.361329Z","shell.execute_reply":"2021-12-21T12:31:02.581141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#создаем класс ImageFolderWithPaths, на основе ImageFolder, с единственным изменением метода .__getitem__\n#что бы кортеж элементов, помимо изображения и класса, содержал еще 3-й элемент = полный путь к изображению\n#это необходимо для формирования submit csv, где первый столбец = индекcу фото (имя файла без расширения),\n#второй столбец = отнесению к классу\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,)) #сумма кортежей = concat из всех элементов\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('/kaggle/working/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T12:31:02.909778Z","iopub.execute_input":"2021-12-21T12:31:02.910082Z","iopub.status.idle":"2021-12-21T12:31:02.919591Z","shell.execute_reply.started":"2021-12-21T12:31:02.910033Z","shell.execute_reply":"2021-12-21T12:31:02.918856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval() #не забываем перевести модель в eval режим\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths) #непонятно почему здесь extend, а не append\n    \ntest_predictions = np.concatenate(test_predictions) #преобразуем list в np.array","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:06.417287Z","iopub.execute_input":"2021-12-17T07:27:06.417972Z","iopub.status.idle":"2021-12-17T07:27:10.835516Z","shell.execute_reply.started":"2021-12-17T07:27:06.4179Z","shell.execute_reply":"2021-12-17T07:27:10.833562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:16.287357Z","iopub.execute_input":"2021-12-17T07:27:16.287745Z","iopub.status.idle":"2021-12-17T07:27:19.400534Z","shell.execute_reply.started":"2021-12-17T07:27:16.287681Z","shell.execute_reply":"2021-12-17T07:27:19.399489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:36.112373Z","iopub.execute_input":"2021-12-17T07:27:36.112728Z","iopub.status.idle":"2021-12-17T07:27:36.120184Z","shell.execute_reply.started":"2021-12-17T07:27:36.112667Z","shell.execute_reply":"2021-12-17T07:27:36.119021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned') #threshold (отсечка) для отнесения к классу \nsubmission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:41.668433Z","iopub.execute_input":"2021-12-17T07:27:41.668786Z","iopub.status.idle":"2021-12-17T07:27:41.715008Z","shell.execute_reply.started":"2021-12-17T07:27:41.668711Z","shell.execute_reply":"2021-12-17T07:27:41.714268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission_baseline_without_changes.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:28:54.203059Z","iopub.execute_input":"2021-12-17T07:28:54.203449Z","iopub.status.idle":"2021-12-17T07:28:54.300812Z","shell.execute_reply.started":"2021-12-17T07:28:54.203387Z","shell.execute_reply":"2021-12-17T07:28:54.300043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf train val test plates","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:05.616624Z","iopub.execute_input":"2021-12-21T13:12:05.616958Z","iopub.status.idle":"2021-12-21T13:12:06.474065Z","shell.execute_reply.started":"2021-12-21T13:12:05.616905Z","shell.execute_reply":"2021-12-21T13:12:06.473036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:11.67344Z","iopub.execute_input":"2021-12-21T13:12:11.673735Z","iopub.status.idle":"2021-12-21T13:12:12.465077Z","shell.execute_reply.started":"2021-12-21T13:12:11.673697Z","shell.execute_reply":"2021-12-21T13:12:12.464044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}