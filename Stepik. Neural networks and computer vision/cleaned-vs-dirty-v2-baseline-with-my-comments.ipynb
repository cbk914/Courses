{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport zipfile\nwith zipfile.ZipFile('../input/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('/kaggle/working/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"/kaggle/working/\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-21T10:53:36.245754Z","iopub.execute_input":"2021-12-21T10:53:36.246082Z","iopub.status.idle":"2021-12-21T10:53:38.032695Z","shell.execute_reply.started":"2021-12-21T10:53:36.246042Z","shell.execute_reply":"2021-12-21T10:53:38.031883Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_root = '/kaggle/working/plates/'\nprint(os.listdir(data_root))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:38.034518Z","iopub.execute_input":"2021-12-21T10:53:38.034803Z","iopub.status.idle":"2021-12-21T10:53:38.040148Z","shell.execute_reply.started":"2021-12-21T10:53:38.034756Z","shell.execute_reply":"2021-12-21T10:53:38.039132Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# The shutil module offers a number of high-level operations on files and collections of files. \n# In particular, functions are provided which support file copying and removal.\nimport shutil\nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\n#create dirs ./train/cleaned, /train/dirty, ./val/cleaned, /val/dirty,\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\n#fill directories train (5/6)=83% and val (1/6)=17%, \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-12-21T10:53:44.486652Z","iopub.execute_input":"2021-12-21T10:53:44.486963Z","iopub.status.idle":"2021-12-21T10:53:44.514206Z","shell.execute_reply.started":"2021-12-21T10:53:44.486912Z","shell.execute_reply":"2021-12-21T10:53:44.513303Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\nfrom torchvision import transforms, models","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:44.516173Z","iopub.execute_input":"2021-12-21T10:53:44.516731Z","iopub.status.idle":"2021-12-21T10:53:45.342081Z","shell.execute_reply.started":"2021-12-21T10:53:44.516669Z","shell.execute_reply":"2021-12-21T10:53:45.341344Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Transforms are common image transformations. They can be chained together using Compose. \n# Most transform classes have a function equivalent: functional transforms give fine-grained \n# control over the transformations. This is useful if you have to build a more complex transformation \n# pipeline (e.g. in the case of segmentation tasks).\n\n# ILLUSTRATION OF TRANSFORMS: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n\ntrain_transforms = transforms.Compose([\n    #!!очень спорное применение RandomResizedCrop для нашего случая, \n    #т.к. по итогу абсолютно получается рандомно вырезанный, сжатый/растянутый кусок исходного изображения\n    transforms.RandomResizedCrop(224), \n    transforms.RandomHorizontalFlip(), #с вероятностью 50% отражаем по горизонтали\n    transforms.ToTensor(),\n    #нормализуем в соостветсвии с тем, как, были предобработаны изображения imagenet1000, при обучении resnet\n    # mean = [0.485, 0.456, 0.406], эти значения вычитаются из RGB каналов изображения (т.е. нормализуем смещение)\n    # std = [0.229, 0.224, 0.225], на эти значения делим RGB каналы изображения \n    #      (т.е. нормализцем среднеквадратичное (стандартное) отклонение, std^2 = var)\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), #важно, не квадратные изображения будут сжиматься/растягиваться!\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n#получаем объект типа .ImageFolder, для дальнейшего использования через torch.utils.data.DataLoader\n#по сути связывает директорию с изображениями, разбитыми ко поддиректориям=классам, с трансформациями, \n#которые будут производиться при дальнейшей загрузке изображений в память (в том числе в композиции \n#трансформаций выше сразу зашито преобразование в тензор)\n#Важные особенности:\n# - указанная директория обязательно должна содержать поддиректории соответсвующие названиям классов, \n#   уже в свою очередь в которых должны находиться изображения\n# - по итогу, мы получаем объект типа ImageFolder, который на первый взгляд является последовательностью \n#   (sequential), длины = кол-ву изображений во всех поддиректориях, но по факту, является только ссылками\n#   на изображения, т.е. в память сразу они не загружаются, а только в момент обращения к конкретному\n#   объекту; простая проверка, если удалить изображение(-я) с диска, заранее созданный объект ImageFolder\n#   уже не сможет предоставить доступ к соответсвующему удаленному изображению объекту (с исключением о \n#   невозможности найти файл)\n# - важно понимать!, что каждый раз при обращении к конкретному элементу (например с индексом 0), весь \n#   процесс загрузки с диска и применение трансформаций будет запускаться заново, как следствие, если мы\n#   имеем рандомные трансформации, т.е. каждый раз при обращении к одному и тому же объекту мы будем \n#   получать разные результаты\n# - для нашего случая трансформаций (где присутсвует .ToTensor()), мы получим последовательность кортежей, \n#   из 2-х подэлементов, 0-й элемент = тензору соответсвующему изображению, 1-й элемент целое число, \n#   отнесение к классу (насколько я понял, классы назначаются в порядке алфавитной сортировки поддиректорий, \n#   т.е. для нашего случая cleaned = 0, dirty = 1)\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\nlen(train_dataset), len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:45.343534Z","iopub.execute_input":"2021-12-21T10:53:45.344064Z","iopub.status.idle":"2021-12-21T10:53:45.359474Z","shell.execute_reply.started":"2021-12-21T10:53:45.344006Z","shell.execute_reply":"2021-12-21T10:53:45.358376Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# На основе .ImageFolder генерируем подобие итератора/последовательности псевдо-батчей указанного размера \n# (в зависимости от shuffle, перемешиваем их или нет), псевдо-, потому что данные не загружаются сразу, \n# а только в момет обращения к ним, для этого указывается количество воркеров (для многопоточности).\n# Важные примечания:\n# - с учетом нашей реализации .ImageFolder, где происходят рандомные трансформации, каждый раз получем\n#   рандомно измененные изображения, даже для одинаковых батчей\n# - изображения в каждом батче, так же перемешиваются каждый раз при вызове запросе батча\n# - это все же, как понял это некоторый микс итератора/последовательности, т.к. например, у объекта есть \n#   размер len(dataloader), но при этом обратиться к индексу нельзя, просто вызвать next(dataloader)\n#   нельзя, зато можно вызвать предварительно приведя к классическому итератору через next(iter(dataloader))\n# - при получении следующего батча (пример выше), возвращает 2 объекта/датасета с фичами (X) и labels (y)\n#   (для нашего случая возвращает 2 тензора, каждый батч фичей будет torch.Size([8, 3, 224, 224]), батч \n#   лейблов torch.Size([8]))\n\nbatch_size = 8\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:45.360545Z","iopub.execute_input":"2021-12-21T10:53:45.360892Z","iopub.status.idle":"2021-12-21T10:53:45.370649Z","shell.execute_reply.started":"2021-12-21T10:53:45.360853Z","shell.execute_reply":"2021-12-21T10:53:45.369741Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:45.372141Z","iopub.execute_input":"2021-12-21T10:53:45.372532Z","iopub.status.idle":"2021-12-21T10:53:45.387044Z","shell.execute_reply.started":"2021-12-21T10:53:45.372475Z","shell.execute_reply":"2021-12-21T10:53:45.385943Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# X_batch, y_batch = next(iter(train_dataloader))\n# mean = np.array([0.485, 0.456, 0.406])\n# std = np.array([0.229, 0.224, 0.225])\n# #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n# #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n# #размерами изображения (на 2 месте)\n# #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n# #противоположную той, которую произвели в трансформере во время загрузки изображения\n# plt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean); ","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:53:45.388684Z","iopub.execute_input":"2021-12-21T10:53:45.389194Z","iopub.status.idle":"2021-12-21T10:53:45.396484Z","shell.execute_reply.started":"2021-12-21T10:53:45.389133Z","shell.execute_reply":"2021-12-21T10:53:45.395473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def show_input(input_tensor, title='', \n               mean = np.array([0.485, 0.456, 0.406]), \n               std = np.array([0.229, 0.224, 0.225])):\n    '''Функция реализующая вывод изображений с заголовком = классу'''\n    \n    #permute(1, 2, 0) необходим, т.к. для нескольких каналов, plt принимает тензор, \n    #где измерение (dim) каналов на последнем месте, а в торче каналы стоят перед \n    #размерами изображения (на 2 месте)\n    #так же производим денормализацию (там делили вычитали, здесь умножаем складываем), \n    #противоположную той, которую произвели в трансформере во время загрузки изображения\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1)) #.clip загоняет все значения в заданный интервал, т.е. [0, 1]\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:56:14.597810Z","iopub.execute_input":"2021-12-21T10:56:14.598183Z","iopub.status.idle":"2021-12-21T10:56:17.125293Z","shell.execute_reply.started":"2021-12-21T10:56:14.598117Z","shell.execute_reply":"2021-12-21T10:56:17.123916Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step() #no grad step, only change lr of optimizer\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                # в зависимости от train/val задаем, будут ли тензоры запоминать \n                # все производимые на них операции\n                # для train это необходимое условие возможности сделать градиентный шаг\n                # для val/test это только траты памяти (ОЗУ/GPU) и вычислительных ресурсов\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs) #аналогично вызову метода model.forward(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss / len(dataloader)\n            epoch_acc = running_acc / len(dataloader)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:59:27.167132Z","iopub.execute_input":"2021-12-21T10:59:27.167459Z","iopub.status.idle":"2021-12-21T10:59:27.178355Z","shell.execute_reply.started":"2021-12-21T10:59:27.167409Z","shell.execute_reply":"2021-12-21T10:59:27.177435Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = models.resnet18(pretrained=True) #загружаем предтренированную на imagenet1000 модель\n\n# Disable grad for all conv layers (скорей всего морозим градиенты не только conv)\nfor param in model.parameters():\n    param.requires_grad = False\n    \n#заменяем последний полносвязный слой, содержащий 1000 нейронов (для 1000 классов imagenet1000)\n#на полносвязный слой из 2-х нейронов, для наших 2-х классов (кстати странно что не используется \n#классическая бинарная конфигурация, с 1-м выходом и BCE, нужно проверить, даст ли замена на \n#классику более лучший результат)\n#последний слой хранится в параметре .fc экземпляра класса, до замены он был = \n#Linear(in_features=512, out_features=1000, bias=True)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n# Единственное назначение .lr_scheduler.StepLR, это изменение LR, оптимизатора, путем его домножения \n# на gamma, каждые step_size эпох\n# Важно понимать, что scheduler считает эпохи путем явного выхова scheduler.step(),\n# а так же, что его .step() не делает ничего более кроме изменения LR каждые step_size эпох т.е. \n# scheduler.step() не отменяет необходимости вызова optimizer.step() для осуществления град. шага\n\n# gamma - гиперпараметр, соответсвенно имеет смысл им поиграть на практике и найти оптимальный\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:59:28.684305Z","iopub.execute_input":"2021-12-21T10:59:28.684871Z","iopub.status.idle":"2021-12-21T10:59:29.559451Z","shell.execute_reply.started":"2021-12-21T10:59:28.684819Z","shell.execute_reply":"2021-12-21T10:59:29.558579Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_model(model, loss, optimizer, scheduler, num_epochs=100);","metadata":{"execution":{"iopub.status.busy":"2021-12-21T10:59:37.511083Z","iopub.execute_input":"2021-12-21T10:59:37.511742Z","iopub.status.idle":"2021-12-21T11:00:00.579528Z","shell.execute_reply.started":"2021-12-21T10:59:37.511669Z","shell.execute_reply":"2021-12-21T11:00:00.577990Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_dir = 'test'\n#создаем в test поддиректорию не существующего класса unknown, и копируем туда test изображения\n#т.к. .ImageFolder требует, что бы в целевой директории обязательно были поддиректории=классы\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T12:31:02.361066Z","iopub.execute_input":"2021-12-21T12:31:02.361378Z","iopub.status.idle":"2021-12-21T12:31:02.582164Z","shell.execute_reply.started":"2021-12-21T12:31:02.361329Z","shell.execute_reply":"2021-12-21T12:31:02.581141Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"ttt = torchvision.datasets.ImageFolder('/kaggle/working/test', val_transforms)\nttt[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-21T12:42:43.039838Z","iopub.execute_input":"2021-12-21T12:42:43.040454Z","iopub.status.idle":"2021-12-21T12:42:43.075039Z","shell.execute_reply.started":"2021-12-21T12:42:43.040402Z","shell.execute_reply":"2021-12-21T12:42:43.074063Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#создаем класс ImageFolderWithPaths, на основе ImageFolder, с единственным изменением метода .__getitem__\n#что бы кортеж элементов, помимо изображения и класса, содержал еще 3-й элемент = полный путь к изображению\n#это необходимо для формирования submit csv, где первый столбец = индекcу фото (имя файла без расширения),\n#второй столбец = отнесению к классу\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,)) #сумма кортежей = concat из всех элементов\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('/kaggle/working/test', val_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T12:31:02.909778Z","iopub.execute_input":"2021-12-21T12:31:02.910082Z","iopub.status.idle":"2021-12-21T12:31:02.919591Z","shell.execute_reply.started":"2021-12-21T12:31:02.910033Z","shell.execute_reply":"2021-12-21T12:31:02.918856Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model.eval() #не забываем перевести модель в eval режим\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths) #непонятно почему здесь extend, а не append\n    \ntest_predictions = np.concatenate(test_predictions) #преобразуем list в np.array","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:06.417287Z","iopub.execute_input":"2021-12-17T07:27:06.417972Z","iopub.status.idle":"2021-12-17T07:27:10.835516Z","shell.execute_reply.started":"2021-12-17T07:27:06.4179Z","shell.execute_reply":"2021-12-17T07:27:10.833562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:16.287357Z","iopub.execute_input":"2021-12-17T07:27:16.287745Z","iopub.status.idle":"2021-12-17T07:27:19.400534Z","shell.execute_reply.started":"2021-12-17T07:27:16.287681Z","shell.execute_reply":"2021-12-17T07:27:19.399489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:36.112373Z","iopub.execute_input":"2021-12-17T07:27:36.112728Z","iopub.status.idle":"2021-12-17T07:27:36.120184Z","shell.execute_reply.started":"2021-12-17T07:27:36.112667Z","shell.execute_reply":"2021-12-17T07:27:36.119021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned') #threshold (отсечка) для отнесения к классу \nsubmission_df['id'] = submission_df['id'].str.replace('/kaggle/working/test/unknown/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:27:41.668433Z","iopub.execute_input":"2021-12-17T07:27:41.668786Z","iopub.status.idle":"2021-12-17T07:27:41.715008Z","shell.execute_reply.started":"2021-12-17T07:27:41.668711Z","shell.execute_reply":"2021-12-17T07:27:41.714268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission_baseline_without_changes.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T07:28:54.203059Z","iopub.execute_input":"2021-12-17T07:28:54.203449Z","iopub.status.idle":"2021-12-17T07:28:54.300812Z","shell.execute_reply.started":"2021-12-17T07:28:54.203387Z","shell.execute_reply":"2021-12-17T07:28:54.300043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf train val test plates","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:05.616624Z","iopub.execute_input":"2021-12-21T13:12:05.616958Z","iopub.status.idle":"2021-12-21T13:12:06.474065Z","shell.execute_reply.started":"2021-12-21T13:12:05.616905Z","shell.execute_reply":"2021-12-21T13:12:06.473036Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-12-21T13:12:11.673440Z","iopub.execute_input":"2021-12-21T13:12:11.673735Z","iopub.status.idle":"2021-12-21T13:12:12.465077Z","shell.execute_reply.started":"2021-12-21T13:12:11.673697Z","shell.execute_reply":"2021-12-21T13:12:12.464044Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}