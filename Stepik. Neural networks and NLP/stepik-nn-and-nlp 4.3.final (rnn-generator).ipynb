{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Генерация текста с помощью RNN\n","metadata":{}},{"cell_type":"markdown","source":"(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle,\n# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n\n# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n# import sys; sys.path.append('./stepik-dl-nlp')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:48:40.252870Z","iopub.execute_input":"2022-03-02T13:48:40.253576Z","iopub.status.idle":"2022-03-02T13:48:40.258918Z","shell.execute_reply.started":"2022-03-02T13:48:40.253527Z","shell.execute_reply":"2022-03-02T13:48:40.257968Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:20:34.854793Z","start_time":"2019-11-05T18:20:34.372865Z"},"execution":{"iopub.status.busy":"2022-03-02T13:48:40.260658Z","iopub.execute_input":"2022-03-02T13:48:40.261272Z","iopub.status.idle":"2022-03-02T13:48:40.274222Z","shell.execute_reply.started":"2022-03-02T13:48:40.261228Z","shell.execute_reply":"2022-03-02T13:48:40.272835Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Данные\nДатасет содержит ~9k имен, все написаны латиницей.","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n!mkdir ./stepik-dl-nlp\n!mkdir ./stepik-dl-nlp/datasets\n!curl \"https://raw.githubusercontent.com/Samsung-IT-Academy/stepik-dl-nlp/master/datasets/russian_names.txt\" -o ./stepik-dl-nlp/datasets/russian_names.txt\nwith open('./stepik-dl-nlp/datasets/russian_names.txt') as input_file:\n    names = input_file.read()[:-1].split('\\n')\n    names = [' ' + line for line in names]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.509714Z","start_time":"2019-11-05T18:21:03.491489Z"},"execution":{"iopub.status.busy":"2022-03-02T13:57:20.624911Z","iopub.execute_input":"2022-03-02T13:57:20.625272Z","iopub.status.idle":"2022-03-02T13:57:22.129975Z","shell.execute_reply.started":"2022-03-02T13:57:20.625238Z","shell.execute_reply":"2022-03-02T13:57:22.128793Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"names[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.946758Z","start_time":"2019-11-05T18:21:03.938432Z"},"execution":{"iopub.status.busy":"2022-03-02T13:58:53.384111Z","iopub.execute_input":"2022-03-02T13:58:53.385587Z","iopub.status.idle":"2022-03-02T13:58:53.396169Z","shell.execute_reply.started":"2022-03-02T13:58:53.385513Z","shell.execute_reply":"2022-03-02T13:58:53.394694Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на распределение длин имен:","metadata":{}},{"cell_type":"code","source":"plt.title('Name length distribution')\nplt.hist(list(map(len, names)), bins=25);","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:05.420060Z","start_time":"2019-11-05T18:21:05.179513Z"},"execution":{"iopub.status.busy":"2022-03-02T13:59:08.996192Z","iopub.execute_input":"2022-03-02T13:59:08.996571Z","iopub.status.idle":"2022-03-02T13:59:09.328415Z","shell.execute_reply.started":"2022-03-02T13:59:08.996534Z","shell.execute_reply":"2022-03-02T13:59:09.327676Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Препроцессинг","metadata":{}},{"cell_type":"code","source":"#all unique characters go here\ntokens = list(set(''.join(names)))\n\nnum_tokens = len(tokens)\nprint ('num_tokens = ', num_tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.335188Z","start_time":"2019-11-05T18:21:07.320148Z"},"execution":{"iopub.status.busy":"2022-03-02T13:59:34.088849Z","iopub.execute_input":"2022-03-02T13:59:34.089579Z","iopub.status.idle":"2022-03-02T13:59:34.101782Z","shell.execute_reply.started":"2022-03-02T13:59:34.089524Z","shell.execute_reply":"2022-03-02T13:59:34.100309Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Символы -> id\n\nСоздадим словарь < символ > -> < id >","metadata":{}},{"cell_type":"code","source":"token_to_id = {token: idx for idx, token in enumerate(tokens)}","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.674548Z","start_time":"2019-11-05T18:21:07.671129Z"},"execution":{"iopub.status.busy":"2022-03-02T14:00:13.920717Z","iopub.execute_input":"2022-03-02T14:00:13.921135Z","iopub.status.idle":"2022-03-02T14:00:13.927422Z","shell.execute_reply.started":"2022-03-02T14:00:13.921096Z","shell.execute_reply":"2022-03-02T14:00:13.925821Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n\nfor i in range(num_tokens):\n    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n\nprint(\"Seems alright!\")","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.838814Z","start_time":"2019-11-05T18:21:07.833611Z"},"execution":{"iopub.status.busy":"2022-03-02T14:01:22.061811Z","iopub.execute_input":"2022-03-02T14:01:22.062224Z","iopub.status.idle":"2022-03-02T14:01:22.069477Z","shell.execute_reply.started":"2022-03-02T14:01:22.062189Z","shell.execute_reply":"2022-03-02T14:01:22.068613Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n    max_len = max_len or max(map(len, data))\n    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n    for i in range(len(data)):\n        line_ix = [token_to_id[c] for c in data[i]]\n        data_ix[i, :len(line_ix)] = line_ix\n        \n    if not batch_first: # convert [batch, time] into [time, batch]\n        data_ix = np.transpose(data_ix)\n\n    return data_ix","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.988093Z","start_time":"2019-11-05T18:21:07.977722Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Example: cast 4 names to matrices, pad with zeros\nprint('\\n'.join(names[::2000]))\nprint(to_matrix(names[::2000], token_to_id))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:08.136936Z","start_time":"2019-11-05T18:21:08.131609Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Рекуррентные нейронные сети\n\n<img src=\"img/rnn.png\" width=480>","metadata":{}},{"cell_type":"code","source":"import torch, torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.739438Z","start_time":"2019-11-05T18:21:09.661222Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharRNNCell(nn.Module):\n    \"\"\"\n    Implement the scheme above as torch module\n    \"\"\"\n    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n        super(self.__class__,self).__init__()\n        self.num_units = rnn_num_units\n        \n        self.embedding = nn.Embedding(num_tokens, embedding_size)\n        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward(self, x, h_prev):\n        \"\"\"\n        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n        We'll call it repeatedly to produce the whole sequence.\n        \n        :param x: batch of character ids, variable containing vector of int64\n        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n        \"\"\"\n        # get vector embedding of x\n        x_emb = self.embedding(x)\n        \n        # compute next hidden state using self.rnn_update\n        x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n        h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n        \n        h_next = F.tanh(h_next)\n        \n        assert h_next.size() == h_prev.size()\n        \n        #compute logits for next character probs\n        logits = self.rnn_to_logits(h_next)\n        \n        return h_next, F.log_softmax(logits, -1)\n    \n    def initial_state(self, batch_size):\n        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n        return Variable(torch.zeros(batch_size, self.num_units))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.751862Z","start_time":"2019-11-05T18:21:10.741772Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_rnn = CharRNNCell()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.071002Z","start_time":"2019-11-05T18:21:11.052377Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети, RNN loop","metadata":{}},{"cell_type":"code","source":"def rnn_loop(rnn, batch_index):\n    \"\"\"\n    Computes log P(next_character) for all time-steps in names_ix\n    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n    \"\"\"\n    batch_size, max_length = batch_index.size()\n    hid_state = rnn.initial_state(batch_size)\n    logprobs = []\n\n    for x_t in batch_index.transpose(0,1):\n        hid_state, logp_next = rnn(x_t, hid_state)  \n        logprobs.append(logp_next)\n        \n    return torch.stack(logprobs, dim=1)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.521078Z","start_time":"2019-11-05T18:21:11.510175Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom random import sample\n\nchar_rnn = CharRNNCell()\nopt = torch.optim.Adam(char_rnn.parameters())\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:12.120106Z","start_time":"2019-11-05T18:21:12.109585Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\nfor i in range(1000):\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = rnn_loop(char_rnn, batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))###YOUR CODE\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    # visualizing training process\n    history.append(loss.data.numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history,label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.521061Z","start_time":"2019-11-05T18:21:12.302892Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN: генерация имен","metadata":{}},{"cell_type":"code","source":"def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    hid_state = char_rnn.initial_state(batch_size=1)\n    \n    #feed the seed phrase, if any\n    for i in range(len(seed_phrase) - 1):\n        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n    \n    #start generating\n    for _ in range(max_length - len(seed_phrase)):\n        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n        \n        # sample next token and push it back into x_sequence\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n        \n    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.540765Z","start_time":"2019-11-05T18:21:23.524503Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(10):\n    print(generate_sample(char_rnn))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.625562Z","start_time":"2019-11-05T18:21:23.544968Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(10):\n    print(generate_sample(char_rnn, seed_phrase=' Ar'))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.702249Z","start_time":"2019-11-05T18:21:23.629226Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Более простое решение\n\n* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n\nКроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n\nПерепишем наш пример с генерацией имен с помощью средств PyTorch.","metadata":{}},{"cell_type":"code","source":"class CharRNNLoop(nn.Module):\n    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n        super(self.__class__, self).__init__()\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward(self, x):\n        assert isinstance(x, Variable) and isinstance(x.data, torch.LongTensor)\n        h_seq, _ = self.rnn(self.emb(x))\n        next_logits = self.hid_to_logits(h_seq)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp\n    \nmodel = CharRNNLoop()\nopt = torch.optim.Adam(model.parameters())\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.713285Z","start_time":"2019-11-05T18:21:23.704755Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the model applies over the whole sequence\nbatch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\nbatch_ix = Variable(torch.LongTensor(batch_ix))\n\nlogp_seq = model(batch_ix)\n\n# compute loss\nloss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\nfor i in range(1000):\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = model(batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    history.append(loss.data.numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history, label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in range(10):\n    print(generate_sample(char_rnn))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.526436Z","start_time":"2019-11-05T18:21:31.469965Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Домашнее задание: мотивационные лозунги","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/author_quotes.txt') as input_file:\n    quotes = input_file.read()[:-1].split('\\n')\n    quotes = [' ' + line for line in quotes]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.570320Z","start_time":"2019-11-05T18:21:31.528673Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.575286Z","start_time":"2019-11-05T18:21:31.571798Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = list(set(''.join(quotes)))\ntoken_to_id = {token: idx for idx, token in enumerate(tokens)}\nnum_tokens = len(tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.653673Z","start_time":"2019-11-05T18:21:31.578424Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# your code here","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Что еще можно генерировать?\nС помощью кода из этого семинара можно генерировать не только имена, но и:\n\n* Повести/романы/поэзию/песни любимого автора\n* Новостные заголовки\n* Программный код\n* Молекулы в формате [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n* Музыку\n* Названия мебели из ИКЕА\n* Мотивационные лозунги\n* etc.\n\n__Удачи!__","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}