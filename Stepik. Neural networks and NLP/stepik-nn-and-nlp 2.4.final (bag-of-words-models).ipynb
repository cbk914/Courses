{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-25T13:09:00.258266Z","iopub.execute_input":"2022-01-25T13:09:00.258776Z","iopub.status.idle":"2022-01-25T13:09:00.264037Z","shell.execute_reply.started":"2022-01-25T13:09:00.258739Z","shell.execute_reply":"2022-01-25T13:09:00.263325Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n        \n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    experiment_name = 'NoName'):\n    \"\"\"\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.3f}, train = {:.3f}    LOSS: val = {:.3f}, train = {:.3f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.3f}, train = {:.3f}, LOSS: val = {:.3f}, train = {:.3f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['best_epoch'] = best_epoch_i\n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:15:40.554095Z","iopub.execute_input":"2022-01-25T13:15:40.554358Z","iopub.status.idle":"2022-01-25T13:15:40.616514Z","shell.execute_reply.started":"2022-01-25T13:15:40.554328Z","shell.execute_reply":"2022-01-25T13:15:40.615619Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), best_models = None, \n                           train_dataset = None, test_dataset = None, show_plots = True):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n        print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n              .format(epoch_max_acc, \n                      histories[experiment_id]['acc']['val'][epoch_max_acc], \n                      histories[experiment_id]['acc']['train'][epoch_max_acc],\n                      histories[experiment_id]['loss']['val'][epoch_max_acc],\n                      histories[experiment_id]['loss']['train'][epoch_max_acc]))\n        epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n        print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n              .format(epoch_min_loss, \n                      histories[experiment_id]['acc']['val'][epoch_min_loss], \n                      histories[experiment_id]['acc']['train'][epoch_min_loss],\n                      histories[experiment_id]['loss']['val'][epoch_min_loss],\n                      histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if best_models is not None:\n            train_pred = predict_with_model(best_models[experiment_id], train_dataset, return_labels=True)\n            train_loss = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n            train_acc = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n\n            test_pred = predict_with_model(best_models[experiment_id], test_dataset, return_labels=True)\n            test_loss = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n            test_acc = accuracy_score(test_pred[1], test_pred[0].argmax(-1))\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.3f}, train = {:.3f}, LOSS: test = {:.3f}, train = {:.3f}\"\\\n                  .format(histories[experiment_id]['best_epoch'], \n                          test_acc,\n                          train_acc,\n                          test_loss,\n                          train_loss))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:09.567926Z","iopub.execute_input":"2022-01-25T13:09:09.568334Z","iopub.status.idle":"2022-01-25T13:09:09.586931Z","shell.execute_reply.started":"2022-01-25T13:09:09.568298Z","shell.execute_reply":"2022-01-25T13:09:09.586255Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка датасета","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import accuracy_score\n\ntrain_source = fetch_20newsgroups(subset='train')\ntest_source = fetch_20newsgroups(subset='test')\n\nprint('Количество обучающих текстов', len(train_source['data']))\nprint('Количество тестовых текстов', len(test_source['data']))\nprint()\nprint(train_source['data'][0].strip())\n\nprint()\nprint('Метка', train_source['target'][0])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:11.471508Z","iopub.execute_input":"2022-01-25T13:09:11.471749Z","iopub.status.idle":"2022-01-25T13:09:22.418098Z","shell.execute_reply.started":"2022-01-25T13:09:11.471721Z","shell.execute_reply":"2022-01-25T13:09:22.417361Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_source.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_source['target_names']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Реализация на scikit-learn","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_DF = 0.8\nMIN_COUNT = 5\n\nsklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n                                                      max_df=MAX_DF,\n                                                      min_df=MIN_COUNT)),\n                             ('cls', LogisticRegression())))\nsklearn_pipeline.fit(train_source['data'], train_source['target']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Оценка качества\nsklearn_train_pred = sklearn_pipeline.predict_proba(train_source['data'])\nsklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n                                                 torch.from_numpy(train_source['target']))\nprint('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\nprint('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\nprint()\n\nsklearn_test_pred = sklearn_pipeline.predict_proba(test_source['data'])\nsklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n                                                torch.from_numpy(test_source['target']))\nprint('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\nprint('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Среднее значение функции потерь на обучении 2.495478891861736\nДоля верных ответов 0.9716280714159449\n\nСреднее значение функции потерь на валидации 2.6539022582370295\nДоля верных ответов 0.8190387679235263","metadata":{}},{"cell_type":"markdown","source":"## Реализация из семинара, которую необходимо улучшить до уровня scikit-learn или выше","metadata":{"execution":{"iopub.status.busy":"2022-01-24T12:13:49.29619Z","iopub.execute_input":"2022-01-24T12:13:49.29653Z","iopub.status.idle":"2022-01-24T12:13:49.302051Z","shell.execute_reply.started":"2022-01-24T12:13:49.296497Z","shell.execute_reply":"2022-01-24T12:13:49.3007Z"}}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import accuracy_score\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport collections\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nhistories = {}\nbest_models = {}","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:22.419710Z","iopub.execute_input":"2022-01-25T13:09:22.420148Z","iopub.status.idle":"2022-01-25T13:09:22.429252Z","shell.execute_reply.started":"2022-01-25T13:09:22.420110Z","shell.execute_reply":"2022-01-25T13:09:22.427769Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Подготовка признаков","metadata":{}},{"cell_type":"code","source":"train_tokenized = tokenize_corpus(train_source['data'])\ntest_tokenized = tokenize_corpus(test_source['data'])\n\nprint(' '.join(train_tokenized[0]))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:22.544302Z","iopub.execute_input":"2022-01-25T13:09:22.544516Z","iopub.status.idle":"2022-01-25T13:09:25.883813Z","shell.execute_reply.started":"2022-01-25T13:09:22.544490Z","shell.execute_reply":"2022-01-25T13:09:25.883060Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"MAX_DF = 0.8\nMIN_COUNT = 5\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:25.885303Z","iopub.execute_input":"2022-01-25T13:09:25.886172Z","iopub.status.idle":"2022-01-25T13:09:26.394671Z","shell.execute_reply.started":"2022-01-25T13:09:25.886132Z","shell.execute_reply":"2022-01-25T13:09:26.393938Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"word_doc_freq[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:26.396076Z","iopub.execute_input":"2022-01-25T13:09:26.396524Z","iopub.status.idle":"2022-01-25T13:09:26.405934Z","shell.execute_reply.started":"2022-01-25T13:09:26.396486Z","shell.execute_reply":"2022-01-25T13:09:26.405029Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"word_doc_freq[-20:]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:26.408316Z","iopub.execute_input":"2022-01-25T13:09:26.408605Z","iopub.status.idle":"2022-01-25T13:09:26.416597Z","shell.execute_reply.started":"2022-01-25T13:09:26.408572Z","shell.execute_reply":"2022-01-25T13:09:26.415629Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.hist(word_doc_freq, bins=20)\nplt.title('Распределение относительных частот слов')\nplt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VECTORIZATION_MODE = 'tfidf'\ntrain_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\ntest_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_vectors.data, bins=20)\nplt.title('Распределение весов признаков')\nplt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Распределение классов","metadata":{}},{"cell_type":"code","source":"UNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:09:33.354956Z","iopub.execute_input":"2022-01-25T13:09:33.355587Z","iopub.status.idle":"2022-01-25T13:09:33.367658Z","shell.execute_reply.started":"2022-01-25T13:09:33.355549Z","shell.execute_reply":"2022-01-25T13:09:33.366938Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_source['target'], bins=np.arange(0, 21))\nplt.title('Распределение меток в обучающей выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(test_source['target'], bins=np.arange(0, 21))\nplt.title('Распределение меток в тестовой выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Обучение модели на PyTorch","metadata":{}},{"cell_type":"code","source":"model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\nhistory, best_epoch_i, best_model = train_eval_loop(model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=1e-1,\n                                            epoch_n=200,\n                                            batch_size=32,\n                                            l2_reg_alpha=0,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Оценка качества","metadata":{}},{"cell_type":"code","source":"train_pred = predict_with_model(best_model, train_dataset)\n\ntrain_loss = F.cross_entropy(torch.from_numpy(train_pred),\n                             torch.from_numpy(train_source['target']).long())\n\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\nprint()\n\n\n\ntest_pred = predict_with_model(best_model, test_dataset)\n\ntest_loss = F.cross_entropy(torch.from_numpy(test_pred),\n                            torch.from_numpy(test_source['target']).long())\n\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Домашнее задание 2.4.Final","metadata":{}},{"cell_type":"raw","source":"В качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы попробовать улучшить качество на отложенной выборке. Что можно попробовать сделать:\n\n    - изменить способ взвешивания признаков\n    \n    - реализовать взвешивание признаков с помощью точечной взаимной информации (PMI)\n    \n    - изменить способ стандартизации данных (см. начиная с 4:25 на шаге 6), например, запоминая сдвиг и масштаб с обучающей выборки и применяя эти параметры для стандартизации тестовой выборки; и/или стандартизируя каждый столбец по отдельности\n    \n    - добавить регуляризацию\n        --- применение регуляризации на исходной модели (без корректировки других параметров), практически всегда приводит к ухудшению показателей, во многих случаях, при значениях <= 1e-3 модель вообще перестает обучаться. Пробовал диапазон от 1e-2 до 1e-7\n        \n    - извлекать признаки не через токены, а через N-граммы\n    \n    - добавить стемминг или простую лемматизацию\n    \n    - изменить архитектуру нейросети, например, сделав два слоя вместо одного\n    \n    - добавить дропаут\n    \n    - проанализировать, как сильно падает качество классификации с уменьшением размера словаря (для фильтрации словаря можно использовать разные эвристики, например, тот же PMI)\n\nТакже мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы улучшить качество классификации.\n\nОпишите то, что у Вас получилось, в ответе к этому шагу.\n\nБалл за этот шаг зачитывается автоматически, вне зависимости от текста, который Вы впишете в поле ответа :). Но то, насколько этот семинар и это задание будет полезным для Вас, вполне зависит - так что мы предлагаем Вам поисследовать возможности линейных моделей и подробно описать свой опыт. К тому же, после нажатия кнопки \"Отправить\" Вы получите доступ к ответам других участников и сможете обменяться своими находками.\n\nУспехов! :)","metadata":{}},{"cell_type":"markdown","source":"**Варианты с регуляризацией и разным lr**","metadata":{}},{"cell_type":"raw","source":"Итоги\nизменения не дали какого-либо положительного эффекта (либо тот же уровень, либо хуже)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:48:30.349819Z","iopub.execute_input":"2022-01-24T13:48:30.350512Z","iopub.status.idle":"2022-01-24T13:48:30.353652Z","shell.execute_reply.started":"2022-01-24T13:48:30.350479Z","shell.execute_reply":"2022-01-24T13:48:30.352819Z"}}},{"cell_type":"code","source":"model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\nhistory, best_epoch_i, best_model = train_eval_loop(model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=1e-1,\n                                            epoch_n=200,\n                                            batch_size=32,\n                                            l2_reg_alpha=0,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Оценка качества","metadata":{}},{"cell_type":"code","source":"train_pred = predict_with_model(best_model, train_dataset)\n\ntrain_loss = F.cross_entropy(torch.from_numpy(train_pred),\n                             torch.from_numpy(train_source['target']).long())\n\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\nprint()\n\n\n\ntest_pred = predict_with_model(best_model, test_dataset)\n\ntest_loss = F.cross_entropy(torch.from_numpy(test_pred),\n                            torch.from_numpy(test_source['target']).long())\n\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Варианты с разными подходами по взвешиванию**","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:48:17.580289Z","iopub.execute_input":"2022-01-24T13:48:17.580565Z","iopub.status.idle":"2022-01-24T13:48:17.585965Z","shell.execute_reply.started":"2022-01-24T13:48:17.580536Z","shell.execute_reply":"2022-01-24T13:48:17.585163Z"}}},{"cell_type":"raw","source":"tf по рекомендации Эдуард Смирнов\n#Эдуард Смирнов\n#sklearn для idf используют формулу idf(t) = log [ n / df(t) ] + 1\n#Здесь описано https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\n\nИТОГИ:\n\ntf-lidf_v2: BEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.827, train = 0.998, LOSS: test = 0.755, train = 0.051\ntf-lidf:    BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.825, train = 0.999, LOSS: test = 0.747, train = 0.020\ntf:         BEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.820, train = 0.999, LOSS: test = 0.696, train = 0.028\nltf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.817, train = 0.999, LOSS: test = 0.666, train = 0.014\ntf-idf:     BEST MODEL CALC:   Epoch =  13,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.007, train = 0.023\nltf-idf:    BEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.027, train = 0.034\nidf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.756, train = 0.999, LOSS: test = 0.988, train = 0.015\nbin:        BEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.743, train = 0.999, LOSS: test = 4.418, train = 0.007\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"VECTORIZATION_MODE = 'tflidf'\ntrain_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\ntest_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:15:54.640977Z","iopub.execute_input":"2022-01-25T13:15:54.641236Z","iopub.status.idle":"2022-01-25T13:17:44.430986Z","shell.execute_reply.started":"2022-01-25T13:15:54.641207Z","shell.execute_reply":"2022-01-25T13:17:44.430242Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:44.432815Z","iopub.execute_input":"2022-01-25T13:17:44.433077Z","iopub.status.idle":"2022-01-25T13:17:44.437302Z","shell.execute_reply.started":"2022-01-25T13:17:44.433041Z","shell.execute_reply":"2022-01-25T13:17:44.436405Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\nexperiment_name = 'NoName'\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=1e-1,\n                                            epoch_n=200,\n                                            batch_size=32,\n                                            l2_reg_alpha=0,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            experiment_name = experiment_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T13:17:44.438762Z","iopub.execute_input":"2022-01-25T13:17:44.439347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_experiments_stats(histories, best_models = best_models, \n                       train_dataset = train_dataset, \n                       test_dataset = test_dataset,  show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VECTORIZATION_MODE = 'tflidf_v2'\ntrain_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\ntest_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\nexperiment_name = 'NoName'\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=1e-1,\n                                            epoch_n=200,\n                                            batch_size=32,\n                                            l2_reg_alpha=0,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_experiments_stats(histories, best_models = best_models, \n                       train_dataset = train_dataset, \n                       test_dataset = test_dataset,  show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}