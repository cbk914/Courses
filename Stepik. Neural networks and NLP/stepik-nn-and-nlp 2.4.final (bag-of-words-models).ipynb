{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef tokenize_corpus_verbose(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    tokenize_texts = []\n    for i, text in enumerate(texts):\n        tokenize_texts.append(tokenizer(text, **tokenizer_kwargs))\n        if i % 1000 == 0:\n            print('Complete:', i)\n    return tokenize_texts\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf', 'tfpmi'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n        \n    elif mode == 'tfpmi': # TFPMI=TF⋅PMI\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(word2freq)  # домножить каждую строку на word2freq (это массив PMI Scores)\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    test_dataset = None,\n                    experiment_name = 'NoName'):\n    \"\"\"\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.3f}, train = {:.3f}    LOSS: val = {:.3f}, train = {:.3f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.3f}, train = {:.3f}, LOSS: val = {:.3f}, train = {:.3f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['BEST'] = {}\n    history['BEST']['epoch'] = best_epoch_i\n    history['BEST']['dict_size'] = batch_x.shape[-1]\n    \n    \n    #calculate and save final metrics best_model on train/val/test datasets\n    if test_dataset is not None:\n        history['BEST']['acc'] = {}\n        history['BEST']['loss'] = {}\n        \n        #save validation metrics (no calculate again)\n        history['BEST']['acc']['val'] = history['acc']['val'][best_epoch_i]\n        history['BEST']['loss']['val'] = history['loss']['val'][best_epoch_i]\n        \n        #calculate and save train metrics\n        train_pred = predict_with_model(best_model, train_dataset, return_labels=True)\n        history['BEST']['loss']['train'] = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n        history['BEST']['acc']['train'] = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n        \n        #calculate and save test metrics\n        test_pred = predict_with_model(best_model, test_dataset, return_labels=True)\n        history['BEST']['loss']['test'] = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n        history['BEST']['acc']['test'] = accuracy_score(test_pred[1], test_pred[0].argmax(-1))    \n    \n    \n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T12:45:09.888761Z","iopub.execute_input":"2022-01-28T12:45:09.88937Z","iopub.status.idle":"2022-01-28T12:45:09.977683Z","shell.execute_reply.started":"2022-01-28T12:45:09.88932Z","shell.execute_reply":"2022-01-28T12:45:09.976762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport spacy\nspacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\ndef tokenize_text_spacy_lemmatize(txt, spacy_nlp, min_token_size=1):\n    doc = spacy_nlp(txt)\n    return [token.lemma_ for token in doc if len(token) >= min_token_size]\n\ndef tokenize_corpus_convert(tokenized_corpus, converter, addition = False):\n    '''\n    Convert each token in tokenized_corpus by converter\n    \n    Sample (PorterStemmer):\n    import nltk\n    ps = nltk.stemmer.PorterStemmer()\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=ps.stem)\n    \n    Sample (SnowballStemmer):\n    import nltk\n    sno = nltk.stem.SnowballStemmer('english')\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=sno.stem)\n    \n    Sample (WordNetLemmatizer):\n    import nltk\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    tokenized_lemmas_corpus = tokenize_corpus_convert(tokenized_corpus, converter=lemma.lemmatize)\n    '''\n    output = []\n    if not addition: #возвращаем только преобразованные токены\n        for doc in tokenized_corpus:\n            output.append([converter(token) for token in doc])\n    else: #возвращаем списк из исходных токенов, дополненных списком преобразованных\n        for doc in tokenized_corpus:\n            output.append(doc + [converter(token) for token in doc])        \n    return output\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), show_plots = True, only_BEST_MODEL_CALC = False):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        \n        if not only_BEST_MODEL_CALC:\n            epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n            print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_max_acc, \n                          histories[experiment_id]['acc']['val'][epoch_max_acc], \n                          histories[experiment_id]['acc']['train'][epoch_max_acc],\n                          histories[experiment_id]['loss']['val'][epoch_max_acc],\n                          histories[experiment_id]['loss']['train'][epoch_max_acc]))\n            epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n            print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_min_loss, \n                          histories[experiment_id]['acc']['val'][epoch_min_loss], \n                          histories[experiment_id]['acc']['train'][epoch_min_loss],\n                          histories[experiment_id]['loss']['val'][epoch_min_loss],\n                          histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if 'acc' in histories[experiment_id]['BEST']:\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.3f}, train = {:.3f},   LOSS: test = {:.3f}, train = {:.3f}  DICT SIZE = {}\"\\\n                  .format(histories[experiment_id]['BEST']['epoch'], \n                          histories[experiment_id]['BEST']['acc']['test'],\n                          histories[experiment_id]['BEST']['acc']['train'],\n                          histories[experiment_id]['BEST']['loss']['test'],\n                          histories[experiment_id]['BEST']['loss']['train'],\n                          histories[experiment_id]['BEST']['dict_size']))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n        \n\ndef build_vocabulary_pmi(tokenized_texts, texts_label_indexes, max_size=1000000, \n                         invers_quantile_threshold = 1.0, min_count = 5, \n                         pmi_type = 'max', no_negative = False, \n                         p_w_l_denominator = 'size(l)', pad_word=None):\n    '''\n    texts_label_indexes: содержит индексы (не названия) меток, соответсвующие документам из tokenized_texts\n    '''\n    \n    assert pmi_type in {'max', 'mean'}\n    assert p_w_l_denominator in {'size(l)', 'size(c)'}\n    \n    #формируем массив маржинальных вероятностей встретить документ класса label\n    # p(l) = Size(l)/Size(c)\n    _, counts_labels = np.unique(texts_label_indexes, return_counts=True)\n    p_labels = counts_labels / len(texts_label_indexes)\n    \n    tokens_list = list(set([item for sublist in tokenized_texts for item in sublist]))\n    \n    tokens_dict_indexes = {k: v for v, k in enumerate(tokens_list)}\n    \n    #для подсчета количества документов, в которых встречается токен\n    token_counts_corpus = np.zeros(len(tokens_list))\n    #для подсчета количества документов класса label, в которых встречается токен\n    token_counts_labels = np.zeros((len(tokens_list), len(p_labels)))\n    \n    # посчитываем количество документов (общее и распеределенное по меткам), \n    # в которых употребляется каждое слово\n    for doc_i, doc in enumerate(tokenized_texts):\n        unique_doc_tokens = set(doc)\n        for token in unique_doc_tokens:\n            token_counts_corpus[tokens_dict_indexes[token]] += 1\n            token_counts_labels[tokens_dict_indexes[token]][texts_label_indexes[doc_i]] += 1\n    \n    #оставляем только слова повторяющиеся >= min_count\n    mask = token_counts_corpus >= min_count\n    token_counts_corpus = token_counts_corpus[mask]\n    token_counts_labels = token_counts_labels[mask]\n    tokens_list = list(np.array(tokens_list)[mask])\n    \n    #массив маржинальных вероятностей употребления слов в корпусе\n    # p(w) = DocCount(w,c)/Size(c)\n    p_words = token_counts_corpus / len(tokenized_texts)\n    \n    if p_w_l_denominator == 'size(l)':\n        #массив вероятностей встретить слово в документе класса label\n        # p(w,l) = DocCount(w,l)/Size(l)\n        p_words_labels = token_counts_labels / counts_labels\n    elif p_w_l_denominator == 'size(c)':\n        #массив совместных вероятностей w и l во всем корпусе\n        # p(w,l) = DocCount(w,l)/len(tokenized_texts)\n        p_words_labels = token_counts_labels / counts_labels\n    \n    #Pointwise mutal information words in labels\n    # pmi(l,w) = log(p(w,l)/(p(w)*p(l)))\n    pmi_words_labels = p_words_labels / p_labels\n    pmi_words_labels = (pmi_words_labels.T / p_words).T\n    eps = 1e-10\n    pmi_words_labels = np.log(pmi_words_labels + eps)\n    pmi_words_labels[pmi_words_labels < 0] = 0 #replace negative by 0\n\n    if not no_negative:\n        #массив маржинальных вероятностей НЕ встретить слово в корпусе\n        # p(~w) = DocCount(~w,c)/Size(c)\n        p_words_negative = 1.0 - p_words\n\n        #массив вероятностей НЕ встретить слово в документе класса label\n        # p(~w,l) = DocCount(~w,l)/Size(l)\n        p_words_labels_negative = 1.0 - p_words_labels\n\n        #Pointwise mutal information NO words in labels\n        # pmi(l,~w) = log(p(~w,l)/(p(~w)*p(l)))\n        pmi_words_labels_negative = p_words_labels_negative / p_labels\n        pmi_words_labels_negative = (pmi_words_labels_negative.T / p_words_negative).T\n        pmi_words_labels_negative = np.log(pmi_words_labels_negative + eps)\n        pmi_words_labels_negative[pmi_words_labels_negative < 0] = 0 #replace negative by 0 \n        if pmi_type == 'max':\n            pmi_word_scores = pmi_words_labels.max(axis=1) + p_words_labels_negative.max(axis=1)\n        elif pmi_type == 'mean':\n            pmi_word_scores = pmi_words_labels.mean(axis=1) + p_words_labels_negative.mean(axis=1)\n    else:\n        if pmi_type == 'max':\n            pmi_word_scores = pmi_words_labels.max(axis=1)\n        elif pmi_type == 'mean':\n            pmi_word_scores = pmi_words_labels.mean(axis=1)\n    \n    if invers_quantile_threshold < 1.0:\n        invers_quantile = np.quantile(pmi_word_scores, 1.0 - invers_quantile_threshold)\n        mask = pmi_word_scores > invers_quantile\n        pmi_word_scores = pmi_word_scores[mask]\n        tokens_list = list(np.array(tokens_list)[mask])\n    \n    pmi_word_dict = {k: v for k, v in zip(tokens_list, list(pmi_word_scores))}\n    \n    #отсортировать слова по убыванию pmi scores\n    sorted_word_counts = sorted(pmi_word_dict.items(),\n                                 reverse=True,\n                                 key=lambda pair: pair[1])\n    \n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size с самым большим pmi score\n    if len(sorted_word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n        \n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # формируем итоговый массив pmi score\n    word2pmi_score = np.array([score for _, score in sorted_word_counts], dtype='float32')\n\n    return word2id, word2pmi_score\n\n        \n#https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/4529901\nimport pickle\ndef save_object(obj, filename):\n    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\ndef load_object(filename):\n    with open(filename, 'rb') as inp:\n        return pickle.load(inp)\n\n# sample usage\n#company1 = [1,2,3,4,5]\n#save_object(company1, '/kaggle/working/company1.pkl')\n#del company\n#company1 = load_object(filename)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T12:21:05.88175Z","iopub.execute_input":"2022-01-28T12:21:05.881973Z","iopub.status.idle":"2022-01-28T12:21:14.194893Z","shell.execute_reply.started":"2022-01-28T12:21:05.881949Z","shell.execute_reply":"2022-01-28T12:21:14.194057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка датасета","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import accuracy_score\n\ntrain_source = fetch_20newsgroups(subset='train')\ntest_source = fetch_20newsgroups(subset='test')\n\nprint('Количество обучающих текстов', len(train_source['data']))\nprint('Количество тестовых текстов', len(test_source['data']))\nprint()\nprint(train_source['data'][0].strip())\n\nprint()\nprint('Метка', train_source['target'][0])","metadata":{"execution":{"iopub.status.busy":"2022-01-28T12:21:14.196432Z","iopub.execute_input":"2022-01-28T12:21:14.196649Z","iopub.status.idle":"2022-01-28T12:21:26.348326Z","shell.execute_reply.started":"2022-01-28T12:21:14.196623Z","shell.execute_reply":"2022-01-28T12:21:26.347382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_source.keys()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.077889Z","iopub.execute_input":"2022-01-27T12:46:00.078117Z","iopub.status.idle":"2022-01-27T12:46:00.08881Z","shell.execute_reply.started":"2022-01-27T12:46:00.07809Z","shell.execute_reply":"2022-01-27T12:46:00.087879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_source['target_names']","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.091425Z","iopub.execute_input":"2022-01-27T12:46:00.091724Z","iopub.status.idle":"2022-01-27T12:46:00.109465Z","shell.execute_reply.started":"2022-01-27T12:46:00.091691Z","shell.execute_reply":"2022-01-27T12:46:00.108772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Реализация на scikit-learn","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.pipeline import Pipeline\n# from sklearn.linear_model import LogisticRegression\n# import torch\n# from torch import nn\n# from torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:49:01.344678Z","iopub.execute_input":"2022-01-27T12:49:01.344985Z","iopub.status.idle":"2022-01-27T12:49:01.456956Z","shell.execute_reply.started":"2022-01-27T12:49:01.344941Z","shell.execute_reply":"2022-01-27T12:49:01.456028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.8\n# MIN_COUNT = 5\n\n# sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n#                                                       max_df=MAX_DF,\n#                                                       min_df=MIN_COUNT)),\n#                              ('cls', LogisticRegression())))\n# sklearn_pipeline.fit(train_source['data'], train_source['target']);","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:49:04.927889Z","iopub.execute_input":"2022-01-27T12:49:04.930886Z","iopub.status.idle":"2022-01-27T12:49:26.375717Z","shell.execute_reply.started":"2022-01-27T12:49:04.930828Z","shell.execute_reply":"2022-01-27T12:49:26.374187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Оценка качества\n# sklearn_train_pred = sklearn_pipeline.predict_proba(train_source['data'])\n# sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n#                                                  torch.from_numpy(train_source['target']))\n# print('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\n# print('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\n# print()\n\n# sklearn_test_pred = sklearn_pipeline.predict_proba(test_source['data'])\n# sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n#                                                 torch.from_numpy(test_source['target']))\n# print('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\n# print('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.135798Z","iopub.execute_input":"2022-01-27T12:46:00.136471Z","iopub.status.idle":"2022-01-27T12:46:00.147562Z","shell.execute_reply.started":"2022-01-27T12:46:00.136421Z","shell.execute_reply":"2022-01-27T12:46:00.146772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Среднее значение функции потерь на обучении 2.495478891861736\nДоля верных ответов 0.9716280714159449\n\nСреднее значение функции потерь на валидации 2.6539022582370295\nДоля верных ответов 0.8190387679235263","metadata":{}},{"cell_type":"markdown","source":"## Реализация из семинара, которую необходимо улучшить до уровня scikit-learn или выше","metadata":{"execution":{"iopub.status.busy":"2022-01-24T12:13:49.29619Z","iopub.execute_input":"2022-01-24T12:13:49.29653Z","iopub.status.idle":"2022-01-24T12:13:49.302051Z","shell.execute_reply.started":"2022-01-24T12:13:49.296497Z","shell.execute_reply":"2022-01-24T12:13:49.3007Z"}}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import accuracy_score\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport collections\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nhistories = {}\nbest_models = {}","metadata":{"execution":{"iopub.status.busy":"2022-01-28T12:21:26.35037Z","iopub.execute_input":"2022-01-28T12:21:26.350854Z","iopub.status.idle":"2022-01-28T12:21:26.360487Z","shell.execute_reply.started":"2022-01-28T12:21:26.350822Z","shell.execute_reply":"2022-01-28T12:21:26.359707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Подготовка признаков","metadata":{}},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'])\n# test_tokenized = tokenize_corpus(test_source['data'])\n\n# print(' '.join(train_tokenized[0]))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.171262Z","iopub.execute_input":"2022-01-27T12:46:00.172084Z","iopub.status.idle":"2022-01-27T12:46:00.184504Z","shell.execute_reply.started":"2022-01-27T12:46:00.172031Z","shell.execute_reply":"2022-01-27T12:46:00.183754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.8\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.185836Z","iopub.execute_input":"2022-01-27T12:46:00.187893Z","iopub.status.idle":"2022-01-27T12:46:00.198628Z","shell.execute_reply.started":"2022-01-27T12:46:00.187827Z","shell.execute_reply":"2022-01-27T12:46:00.197808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[:20]","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.200415Z","iopub.execute_input":"2022-01-27T12:46:00.201025Z","iopub.status.idle":"2022-01-27T12:46:00.214832Z","shell.execute_reply.started":"2022-01-27T12:46:00.200987Z","shell.execute_reply":"2022-01-27T12:46:00.213875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[-20:]","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.216409Z","iopub.execute_input":"2022-01-27T12:46:00.216708Z","iopub.status.idle":"2022-01-27T12:46:00.229748Z","shell.execute_reply.started":"2022-01-27T12:46:00.216671Z","shell.execute_reply":"2022-01-27T12:46:00.228988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(word_doc_freq, bins=20)\n# plt.title('Распределение относительных частот слов')\n# plt.yscale('log');","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.23257Z","iopub.execute_input":"2022-01-27T12:46:00.233134Z","iopub.status.idle":"2022-01-27T12:46:00.240636Z","shell.execute_reply.started":"2022-01-27T12:46:00.233086Z","shell.execute_reply":"2022-01-27T12:46:00.23978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.241971Z","iopub.execute_input":"2022-01-27T12:46:00.242693Z","iopub.status.idle":"2022-01-27T12:46:00.25486Z","shell.execute_reply.started":"2022-01-27T12:46:00.242648Z","shell.execute_reply":"2022-01-27T12:46:00.253722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_vectors.data, bins=20)\n# plt.title('Распределение весов признаков')\n# plt.yscale('log');","metadata":{"execution":{"iopub.status.busy":"2022-01-27T12:46:00.256269Z","iopub.execute_input":"2022-01-27T12:46:00.256527Z","iopub.status.idle":"2022-01-27T12:46:00.272509Z","shell.execute_reply.started":"2022-01-27T12:46:00.256496Z","shell.execute_reply":"2022-01-27T12:46:00.271761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Распределение классов","metadata":{}},{"cell_type":"code","source":"UNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:08:56.870041Z","iopub.execute_input":"2022-01-28T13:08:56.870598Z","iopub.status.idle":"2022-01-28T13:08:56.882702Z","shell.execute_reply.started":"2022-01-28T13:08:56.870556Z","shell.execute_reply":"2022-01-28T13:08:56.881909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в обучающей выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(test_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в тестовой выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Обучение модели на PyTorch","metadata":{}},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# history, best_epoch_i, best_model = train_eval_loop(model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             test_dataset=test_dataset,\n#                                             best_acc_type = 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Оценка качества","metadata":{}},{"cell_type":"code","source":"# train_pred = predict_with_model(best_model, train_dataset)\n\n# train_loss = F.cross_entropy(torch.from_numpy(train_pred),\n#                              torch.from_numpy(train_source['target']).long())\n\n# print('Среднее значение функции потерь на обучении', float(train_loss))\n# print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n# print()\n\n\n\n# test_pred = predict_with_model(best_model, test_dataset)\n\n# test_loss = F.cross_entropy(torch.from_numpy(test_pred),\n#                             torch.from_numpy(test_source['target']).long())\n\n# print('Среднее значение функции потерь на валидации', float(test_loss))\n# print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Домашнее задание 2.4.Final","metadata":{}},{"cell_type":"raw","source":"В качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы попробовать улучшить качество на отложенной выборке. Что можно попробовать сделать:\n\n    + изменить способ взвешивания признаков\n        tf-lidf:    BEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.828, train = 0.998, LOSS: test = 0.837, train = 0.056\n        tf-lidf_v2: BEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.827, train = 0.998, LOSS: test = 0.755, train = 0.051\n        tf:         BEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.820, train = 0.999, LOSS: test = 0.696, train = 0.028\n        ltf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.817, train = 0.999, LOSS: test = 0.666, train = 0.014\n        tf-idf:     BEST MODEL CALC:   Epoch =  13,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.007, train = 0.023\n        ltf-idf:    BEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.027, train = 0.034\n        idf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.756, train = 0.999, LOSS: test = 0.988, train = 0.015\n        bin:        BEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.743, train = 0.999, LOSS: test = 4.418, train = 0.007\n    \n    + реализовать взвешивание признаков с помощью точечной взаимной информации (PMI)\n        Итоговые результаты оказались примерно на уровне векторизации tf-lidf (именно lidf, т.е. idf с log), хотя в процессе того, как \n        первично разобрался как происходит взвешивание у меня были существенные надежды на возможность отбора словаря, на основе PMI \n        (это к вопросу примечаниях Романа Суворова, что отбор > взвешивания). Но когда еще более подробно разобрался в получении конечных \n        весов для разных вариантов токенов, то немного снизил ожидания, т.к. на мой взгляд у этого метода есть существенный недостаток, а\n        именно слишком высокие веса у редко встречающихся токенов, по итогу, мы не можем из словаря выкинуть на основе весов, токены, \n        которые встречаются 1-2 раза для одного класса, т.к. они будут иметь ТАКОЙ ЖЕ вес, как и те, которые например встречаются \n        многократно и только для одного конкретного класса. Как следствие, модель будет явно склонна к быстрому переобучению, на основе \n        таких токенов. Понятно что здесь мы можем отсекать подобные через через MIN_COUNT, но все же именно сами веса, никак не разделяют\n        подобные токены.\n        Если это проблему как-то можно обойти, модифицировав взвешивание, просьба отписаться. Я уже не стал более глубоко копать, т.к. был \n        ограничен по времени.\n\n        Вот простой пример (на основе крайностей). Допустим мы имеем 10 000 документов, равномерно распределенных по 20 классам (т.е. по 500 \n        докумнтов на класс). И имеем один токен, который встречается только 1 раз во всем корпусе (допустим в классе №1), и имеем другой \n        токен, который, встречается в каждом документе класса №1 и только в нем (т.е. в 500 документах из 500 класса 1, и более нигде). \n        Понятно, что 2-й токен супер информативен, только на основе одного него мы спожем относить документы к классу №1. Но вот при \n        взвешивании PMI мы получим абсолютно одинаковые веса этих 2-х токенов.\n\n        Для варианта когда знаменатель во взаимной вероятности = количеству документов в классе (как в формуле из видео курса)\n        pmi(l1, w1) = log ((1/500) / ((1/10000)*(500/10000))) = 5.991\n        pmi(l1, w2) = log ((500/500) / ((500/10000)*(500/10000))) = 5.991\n\n        Для варианта когда знаменатель во взаимной вероятности = общему количеству документов в корпусе (как в классической формуле PMI)\n        pmi(l1, w1) = log ((1/500) / ((1/10000)*(500/10000))) = 2.996\n        pmi(l1, w2) = log ((500/500) / ((500/10000)*(500/10000))) = 2.996\n\n        Особой разницы в двух вариантах формул о которых \"спорили\" в комментариях к видео по PMI нет, на мой взгляд вариант из видео лучше, \n        т.к. будет учитывать неравномерное распределение классов (хотя не уверен, т.к. не проверял.\n\n        Итого по реализации. Делал ее на основе разъяснения Романа Суворова \n        здесь https://stepik.org/lesson/225311/step/9?discussion=1231897&reply=1234485&unit=198054\n        и частично из статьи (их комментариев выше)\n        https://web.eecs.umich.edu/~wangluxy/archive/neu_courses/cs6120_fa2019/slides_cs6120_fa19/semantics_part1.pdf\n\n        Реализовал на мой взгляд все возможные варианты, а именно:\n        - знаменатель взаимной вероятности: вариант c Size(l), вариант c Size(c)\n        - использовал/не использовал негативные вероятности (т.е. отдельно рассчитанные вероятности \"слово w не встретилось в документе\")\n        - пробовал варианты подсчета max и mean по классам (к слову mean показал значительное ухудшение результатов, все остальные варианты \n          выше дали +- сопоставимые результаты)\n\n        P.S. Несмотря на мои надежды, в итоге все же победил вариант взвешивания tf-lidf (хотя разница всего в несколько десятых %).\n\n      \n    + изменить способ стандартизации данных (см. начиная с 4:25 на шаге 6), например, запоминая сдвиг и масштаб с обучающей \n      выборки и применяя эти параметры для стандартизации тестовой   выборки; и/или стандартизируя каждый столбец по отдельности\n        Проводил стандартизацию через sklearn.preprocessing.StandardScaler(with_mean=False), в этом случае, она подходит \n        для разреженных матриц. Результаты модели значительно ухудшились.\n    \n    + добавить регуляризацию\n        применение регуляризации на исходной модели (без корректировки других параметров), практически всегда приводит к ухудшению \n        показателей, во многих случаях, при значениях <= 1e-3 модель вообще перестает обучаться. Пробовал диапазон от 1e-2 до 1e-7\n        \n    + извлекать признаки не через токены, а через N-граммы\n        Применение N-грамм (по словам) существенно увеличивает размер словаря (для (2,2) - 86086, (1,2) - 112018, (1,3) - 819495),\n        но не приносит значимых улучшений результатов, если использовать только n-граммы > 1 (т.е. не добавлять в словарь единичные \n        токены), то результаты значимо ухудшаются.\n        Для отбора n-грамм использовал sklearn.feature_extraction.text.TfidfVectorizer\n    \n    + добавить стемминг или простую лемматизацию\n        - замена токенов на преобразованные значения (стемминг/лемматизация) не дали значимых улучшений, \n          при этом сильное ухудшение показателей модели произошло для взвешивания tf-lidf (которая на сырых данных показала \n          наилучший результат). Лучший результат был достугнут для взвешивания tf (но он все же чуть ниже tf-lidf для сырых данных)\n\n        - а вот на варианте, когда к исходному списоку токенов добавляются токены после лемматизации/стемминга, ситуация иная,\n          существенное падение результатов наблюдается для взвешивания tf, варианты с tf-idf и tf-lidf показывают себя лучше на 0.5%,\n          лучший результат достигается с вариантом tf-lidf\n\n        - по какой причине надлюдается сильное падение в одном случае tf, а в другом tf-lidf, не понятно (нужен более глубокий анализ)\n\n        - размер словаря при замене токенов снизился (но несущественно) в сравнении c вариантами без стемминга/лемматизации - 21628\n          (ниже цифры с учетом минимального размера токена при токенизации >= 4)\n            -- для NLTK PorterStemmer: 15963 (процесс стемминга занял 1min 39s)\n            -- для NLTK WordNetLemmatizer: 19579 (процесс лемматизации занял 20.5 s)\n    \n        - размер словаря при добавлении токенов, увеличивается:\n            -- ADD_PorterStemmer:  27577\n            -- ADD_WordNetLemmatizer: 22429\n    \n    + изменить архитектуру нейросети, например, сделав два слоя вместо одного, в том числе добавить дропаут и нормализацию\n        Добавление одного дополнительного линейного слоя (в 120 нейронов) немного улучшило качество (на 0.5%), еще чуть улучшило добавление \n        одного дропаута после скрытого линейного слоя (p=0.2-0.6). Вариации количества нейронов в скрытом слое (от 60 до 500), показывают \n        +- одинаковые результаты. Слой нормализации по батчу существенно ухудшил результаты (если задуматься, что при такой нормализации \n        в пределах батча присходит, то такое ухудшение становится логичным). Добавление более 1 скрытого слоя, приводит к ухудшению \n        результатов даже в сравнении с изначальным одним слоем.\n    \n    + проанализировать, как сильно падает качество классификации с уменьшением размера словаря (для фильтрации словаря можно использовать \n      разные эвристики, например, тот же PMI)\n        Общая, однозначная тенденция, чем меньше словарь, тем хуже качество модели и наоборот. Максимальные показатели \n        всегда достигаются при взвешивании tf-lidf и lr=1e-2. Наилучший результат, при максимальном словаре, т.е. при \n        MIN_COUNT=2 и MAX_DF=0.80 (либо 0.98, что для нашего случая дает одинаковый размер словаря, об этом далее).\n        Важно, что на размер словаря очень слабо влияет отсечение верхней границы частотности (MAX_DF), особенно с учетом \n        того, что первоначальные токены формируются с параметром минимальная длина токена 4 (т.е. уже отсекаются короткие \n        и самые частотные токены), так например между MAX_DF=0.60 и MAX_DF=0.98 находится всего 2 токена.\n        А вот вариации нижней границы частостности, существенно сказывается на размере словаря (MIN_COUNT=20 словарь 7724,\n        MIN_COUNT=2 словарь 47867). \n        Выставление минимальная длина токена = 2, позволило еще чуть улучшить результат, но опять же за счет нижней\n        уменьшения нижней границы частотности, т.е. при MIN_COUNT=2. Даже снижение мин. длины токена до 2-х, не добавило \n        в верхнюю границу частотности большого количества токенов (разница между 0.95 и 0.60, всего 13 токенов, \n        между 0.95 и 0.30, всего 54 токенов)\n\n    \n    + попробовать не убирать токены менее 4 символов при токенизации (параметр пминимальная длина токенов)\n        При снижении минимальной длины токенов, наблюдаем реальный прирост метрик модели (прирост на уровне 1.5%):\n        лучший результат достигнут при min_token_size=2, tflidf, lr=1e-2 (на эпохе 39)\n        Размер словаря при этом увеличивается до 25932 (при исходных MAX_DF=0.80, MIN_COUNT=5)\n\n    + итоговые эксперименты, микс из лучших экспериментов\n\nТакже мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы улучшить качество классификации.\n\nОпишите то, что у Вас получилось, в ответе к этому шагу.\n\nБалл за этот шаг зачитывается автоматически, вне зависимости от текста, который Вы впишете в поле ответа :). Но то, насколько этот семинар и это задание будет полезным для Вас, вполне зависит - так что мы предлагаем Вам поисследовать возможности линейных моделей и подробно описать свой опыт. К тому же, после нажатия кнопки \"Отправить\" Вы получите доступ к ответам других участников и сможете обменяться своими находками.\n\nУспехов! :)","metadata":{}},{"cell_type":"raw","source":"Best of the best:\nmtoken_size=1_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.867, train = 1.000,   LOSS: test = 0.982, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.5_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.866, train = 0.999,   LOSS: test = 1.034, train = 0.002  DICT SIZE = 67164\n\n\nЛучшие результаты (в отдельных экспериментах):\ntf-lidf lr=1e-2 Default Размер словаря: 21628-------------------------------------------------\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.828, train = 0.998, LOSS: test = 0.837, train = 0.056\n\nADD_PorterStemmer__tf-lidf_lr=1e-2 Размер словаря: 27577 -------------------------------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.835, train = 0.997, LOSS: test = 0.834, train = 0.115\nADD_lemmatize__tf-lidf_lr=1e-2 Размер словаря: 22429 -----------------------------------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.834, train = 0.998, LOSS: test = 0.941, train = 0.108\n\nMIN_COUNT=2_tflidf_lr=1e-2 (при стандартном min_token_size=4) Размер словаря: 47867\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.842, train = 0.999, LOSS: test = 0.986, train = 0.093\n\nmin_token_size=2__tf-lidf_lr=1e-2 Размер словаря: 25932 -------------------------------------------------\nBEST MODEL CALC:   Epoch =  39,   ACCURACY: test = 0.844, train = 0.997, LOSS: test = 0.671, train = 0.136\n\nNN_Linear(120)_ReLU_Dropout(0.6)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.845, train = 0.998,   LOSS: test = 0.550, train = 0.007  DICT SIZE = 21628\n\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.841, train = 0.988,   LOSS: test = 0.706, train = 0.238  DICT SIZE = 20738","metadata":{}},{"cell_type":"markdown","source":"**Варианты с регуляризацией и разным lr**","metadata":{}},{"cell_type":"raw","source":"Итоги\nизменения не дали какого-либо положительного эффекта (либо тот же уровень, либо хуже).","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:48:30.349819Z","iopub.execute_input":"2022-01-24T13:48:30.350512Z","iopub.status.idle":"2022-01-24T13:48:30.353652Z","shell.execute_reply.started":"2022-01-24T13:48:30.350479Z","shell.execute_reply":"2022-01-24T13:48:30.352819Z"}}},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# history, best_epoch_i, best_model = train_eval_loop(model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             test_dataset=test_dataset,\n#                                             best_acc_type = 'acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Оценка качества","metadata":{}},{"cell_type":"code","source":"# train_pred = predict_with_model(best_model, train_dataset)\n\n# train_loss = F.cross_entropy(torch.from_numpy(train_pred),\n#                              torch.from_numpy(train_source['target']).long())\n\n# print('Среднее значение функции потерь на обучении', float(train_loss))\n# print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n# print()\n\n\n\n# test_pred = predict_with_model(best_model, test_dataset)\n\n# test_loss = F.cross_entropy(torch.from_numpy(test_pred),\n#                             torch.from_numpy(test_source['target']).long())\n\n# print('Среднее значение функции потерь на валидации', float(test_loss))\n# print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Варианты с разными подходами по взвешиванию**","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:48:17.580289Z","iopub.execute_input":"2022-01-24T13:48:17.580565Z","iopub.status.idle":"2022-01-24T13:48:17.585965Z","shell.execute_reply.started":"2022-01-24T13:48:17.580536Z","shell.execute_reply":"2022-01-24T13:48:17.585163Z"}}},{"cell_type":"raw","source":"tf по рекомендации Эдуард Смирнов\n#Эдуард Смирнов\n#sklearn для idf используют формулу idf(t) = log [ n / df(t) ] + 1\n#Здесь описано https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\n\nИТОГИ:\n\ntf-lidf:    BEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.828, train = 0.998, LOSS: test = 0.837, train = 0.056\ntf-lidf_v2: BEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.827, train = 0.998, LOSS: test = 0.755, train = 0.051\ntf:         BEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.820, train = 0.999, LOSS: test = 0.696, train = 0.028\nltf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.817, train = 0.999, LOSS: test = 0.666, train = 0.014\ntf-idf:     BEST MODEL CALC:   Epoch =  13,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.007, train = 0.023\nltf-idf:    BEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.774, train = 0.999, LOSS: test = 1.027, train = 0.034\nidf:        BEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.756, train = 0.999, LOSS: test = 0.988, train = 0.015\nbin:        BEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.743, train = 0.999, LOSS: test = 4.418, train = 0.007\n","metadata":{}},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперименты со стеммингом и лемматизацией","metadata":{}},{"cell_type":"raw","source":"ИТОГИ эксперимента:\n- замена токенов на преобразованные значения (стемминг/лемматизация) не дали значимых улучшений, \n  при этом сильное ухудшение показателей модели произошло для взвешивания tf-lidf (которая на сырых данных показала \n  наилучший результат). Лучший результат был достугнут для взвешивания tf (но он все же чуть ниже tf-lidf для сырых данных)\n  \n- а вот на варианте, когда к исходному списоку токенов добавляются токены после лемматизации/стемминга, ситуация иная,\n  существенное падение результатов наблюдается для взвешивания tf, варианты с tf-idf и tf-lidf показывают себя лучше на 0.5%,\n  лучший результат достигается с вариантом tf-lidf\n  \n- по какой причине надлюдается сильное падение в одном случае tf, а в другом tf-lidf, не понятно (нужен более глубокий анализ)\n\n- размер словаря при замене токенов снизился (но несущественно) в сравнении c вариантами без стемминга/лемматизации - 21628\n  (ниже цифры с учетом минимального размера токена при токенизации >= 4)\n    -- для NLTK PorterStemmer: 15963 (процесс стемминга занял 1min 39s)\n    -- для NLTK WordNetLemmatizer: 19579 (процесс лемматизации занял 20.5 s)\n    \n- размер словаря при добавлении токенов, увеличивается:\n    -- ADD_PorterStemmer:  27577\n    -- ADD_WordNetLemmatizer: 22429\n\n\nPorterStemmer_default-----------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.768, train = 0.999, LOSS: test = 1.109, train = 0.075\nPorterStemmer_tflidf_lr=1e-2----------------------------------------------------------------------------                         \nBEST MODEL CALC:   Epoch =  45,   ACCURACY: test = 0.764, train = 0.997, LOSS: test = 1.211, train = 0.161\n\n\n\nlemmatize_default-----------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   6,   ACCURACY: test = 0.776, train = 0.998, LOSS: test = 1.222, train = 0.129\nlemmatize_tflidf_lr=1e-2----------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  37,   ACCURACY: test = 0.769, train = 0.998, LOSS: test = 1.252, train = 0.164\nlemmatize_tf_default-----------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.821, train = 0.999, LOSS: test = 0.649, train = 0.014\nlemmatize_tf_lr=1e-2----------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.824, train = 0.999, LOSS: test = 0.717, train = 0.060\n\n\n###################\nПробую не заменять, а дополнять список исходных токенов, преобразованными:\n###################\nРазмер словаря: \nADD_PorterStemmer:  27577 (ранее словарь только стеммингов был 15963)\nADD_WordNetLemmatizer: 22429 (ранее словарь только лемматизации был 19579)\n                          \nADD_PorterStemmer__default---------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.778, train = 0.999, LOSS: test = 1.012, train = 0.026\nADD_PorterStemmer__tf_lr=1e-2------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  37,   ACCURACY: test = 0.773, train = 0.998, LOSS: test = 1.263, train = 0.175\nADD_PorterStemmer__tflidf_lr=1e-2--------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.835, train = 0.997, LOSS: test = 0.834, train = 0.115\n\n                         \nADD_lemmatize__default---------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.777, train = 0.999, LOSS: test = 1.125, train = 0.071\nADD_lemmatize__tf_lr=1e-2------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  23,   ACCURACY: test = 0.770, train = 0.995, LOSS: test = 1.530, train = 0.388\nADD_lemmatize__tflidf_lr=1e-2--------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.834, train = 0.998, LOSS: test = 0.941, train = 0.108\n    ","metadata":{}},{"cell_type":"code","source":"# %%time\n\n# ####load saved or generate tokenize lemmatization (by spacy)####\n# import os.path\n\n# filename = 'train_tokenized_lemmas.pkl'\n# if os.path.isfile('/kaggle/working/'+filename):\n#     train_tokenized_lemmas = load_object('/kaggle/working/'+filename)\n# else:\n#     train_tokenized_lemmas = tokenize_corpus_verbose(train_source['data'], tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, min_token_size=2)\n#     save_object(train_tokenized_lemmas, '/kaggle/working/'+filename)\n\n# filename = 'test_tokenized_lemmas.pkl'\n# if os.path.isfile('/kaggle/working/'+filename):\n#     test_tokenized_lemmas = load_object('/kaggle/working/'+filename)\n# else:\n#     test_tokenized_lemmas = tokenize_corpus_verbose(train_source['data'], tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, min_token_size=2)\n#     save_object(test_tokenized_lemmas, '/kaggle/working/'+filename)\n\n\n# print(' '.join(train_tokenized_lemmas[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'])\n# test_tokenized = tokenize_corpus(test_source['data'])\n\n# print(' '.join(train_tokenized[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# # #PorterStemmer:\n# # from nltk.stem.porter import PorterStemmer\n# # ps = PorterStemmer()\n# # converter = ps.stem\n\n# # #SnowballStemmer:\n# # from nltk.stem.snowball import SnowballStemmer\n# # sno = SnowballStemmer('english')\n# # converter = sno.stem\n    \n# #WordNetLemmatizer:\n# from nltk.stem import WordNetLemmatizer\n# lemma = WordNetLemmatizer()\n# converter=lemma.lemmatize\n\n\n# train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n# test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n# print(' '.join(train_tokenized[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.8\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[-20:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(word_doc_freq, bins=20)\n# plt.title('Распределение относительных частот слов')\n# plt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_vectors.data, bins=20)\n# plt.title('Распределение весов признаков')\n# plt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Распределение классов","metadata":{}},{"cell_type":"code","source":"# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в обучающей выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(test_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в тестовой выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'add_lemm_or_stem__default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'add_lemm_or_stem__tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'add_lemm_or_stem__tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - не убирать токены менее 4 символов при токенизации (параметр пминимальная длина токенов)","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\nПри снижении минимальной длины токенов, наблюдаем реальный прирост метрик модели (прирост на уровне 1.5%):\nлучший результат достигнут при min_token_size=2, tflidf, lr=1e-2 (на эпохе 39)\nРазмер словаря при этом увеличивается до 25932 (при исходных MAX_DF=0.80, MIN_COUNT=5)\n\nmin_token_size=2_default----------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.795, train = 0.999, LOSS: test = 0.826, train = 0.068\nmin_token_size=2_tf_lr=1e-2-------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  32,   ACCURACY: test = 0.788, train = 0.998, LOSS: test = 0.904, train = 0.205\nmin_token_size=2_tflidf_lr=1e-2---------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  39,   ACCURACY: test = 0.844, train = 0.997, LOSS: test = 0.671, train = 0.136","metadata":{}},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\n# print(' '.join(train_tokenized[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.8\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_doc_freq[-20:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(word_doc_freq, bins=20)\n# plt.title('Распределение относительных частот слов')\n# plt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_vectors.data, bins=20)\n# plt.title('Распределение весов признаков')\n# plt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Распределение классов","metadata":{}},{"cell_type":"code","source":"# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(train_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в обучающей выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(test_source['target'], bins=np.arange(0, 21))\n# plt.title('Распределение меток в тестовой выборке');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперименты с разной длиной словаря (пороги отсечения наиболее частых и редких токенов)","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\n\nОбщая, однозначная тенденция, чем меньше словарь, тем хуже качество модели и наоборот. Максимальные показатели \nвсегда достигаются при взвешивании tf-lidf и lr=1e-2. Наилучший результат, при максимальном словаре, т.е. при \nMIN_COUNT=2 и MAX_DF=0.80 (либо 0.98, что для нашего случая дает одинаковый размер словаря, об этом далее).\nВажно, что на размер словаря очень слабо влияет отсечение верхней границы частотности (MAX_DF), особенно с учетом \nтого, что первоначальные токены формируются с параметром минимальная длина токена 4 (т.е. уже отсекаются короткие \nи самые частотные токены), так например между MAX_DF=0.60 и MAX_DF=0.98 находится всего 2 токена.\nА вот вариации нижней границы частостности, существенно сказывается на размере словаря (MIN_COUNT=20 словарь 7724,\nMIN_COUNT=2 словарь 47867). \n\nВыставление минимальная длина токена = 2, позволило еще чуть улучшить результат, но опять же за счет нижней\nуменьшения нижней границы частотности, т.е. при MIN_COUNT=2. Даже снижение мин. длины токена до 2-х, не добавило \nв верхнюю границу частотности большого количества токенов (разница между 0.95 и 0.60, всего 13 токенов, \nмежду 0.95 и 0.30, всего 54 токенов)\n\n\n\nЛучший результат:\nMIN_COUNT=2_min_token_size=4_tflidf_lr=1e-2 Размер словаря: 47867 ----------------------------------------\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.842, train = 0.999, LOSS: test = 0.986, train = 0.093\n\nMIN_COUNT=2_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 56427 ----------------------------------------\nBEST MODEL CALC:   Epoch =  29,   ACCURACY: test = 0.847, train = 0.998, LOSS: test = 0.645, train = 0.122\n\n\nСвод результатов:\n(Первоначальный размер словаря 21628, при MAX_DF=0.80, MIN_COUNT=5, min_token_size=4)\nРазмер словаря: 21628\nMAX_DF=0.95_default---------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.777, train = 0.999, LOSS: test = 0.992, train = 0.019\nMAX_DF=0.95_tf_lr=1e-2------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  21,   ACCURACY: test = 0.769, train = 0.996, LOSS: test = 1.560, train = 0.412\nMAX_DF=0.95_tflidf_lr=1e-2--------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.833, train = 0.997, LOSS: test = 1.026, train = 0.147\n\nРазмер словаря: 21626\nMAX_DF=0.60_default---------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   6,   ACCURACY: test = 0.780, train = 0.999, LOSS: test = 1.186, train = 0.102\nMAX_DF=0.60_tf_lr=1e-2------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  23,   ACCURACY: test = 0.772, train = 0.997, LOSS: test = 1.478, train = 0.333\nMAX_DF=0.60_tflidf_lr=1e-2--------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.833, train = 0.996, LOSS: test = 1.114, train = 0.207\n\nРазмер словаря: 47867\nMIN_COUNT=2_default---------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.766, train = 0.998, LOSS: test = 1.275, train = 0.220\nMIN_COUNT=2_tf_lr=1e-2------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  40,   ACCURACY: test = 0.761, train = 0.999, LOSS: test = 1.240, train = 0.196\nMIN_COUNT=2_tflidf_lr=1e-2--------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.842, train = 0.999, LOSS: test = 0.986, train = 0.093\n\nРазмер словаря: 7724\nMIN_COUNT=20_default--------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.772, train = 0.998, LOSS: test = 0.982, train = 0.096\nMIN_COUNT=20_tf_lr=1e-2-----------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  16,   ACCURACY: test = 0.770, train = 0.993, LOSS: test = 1.178, train = 0.267\nMIN_COUNT=20_tflidf_lr=1e-2-------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.805, train = 0.993, LOSS: test = 0.880, train = 0.199\n\nРазмер словаря: 2498\nMIN_COUNT=30_default--------------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.715, train = 0.948, LOSS: test = 1.099, train = 0.475\nMIN_COUNT=80_tf_lr=1e-2-----------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  20,   ACCURACY: test = 0.716, train = 0.964, LOSS: test = 1.070, train = 0.397\nMIN_COUNT=80_tflidf_lr=1e-2-------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.729, train = 0.959, LOSS: test = 1.170, train = 0.450\n\nРазмер словаря: 47868\nMAX_DF=0.98_MIN_COUNT=2_default---------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.766, train = 0.999, LOSS: test = 1.168, train = 0.111\nMAX_DF=0.98_MIN_COUNT=2_tf_lr=1e-2------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  43,   ACCURACY: test = 0.762, train = 0.998, LOSS: test = 1.201, train = 0.146\nMAX_DF=0.98_MIN_COUNT=2_tflidf_lr=1e-2--------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.841, train = 0.998, LOSS: test = 1.112, train = 0.116\n\nРазмер словаря: 2496\nMAX_DF=0.60_MIN_COUNT=80_default--------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.713, train = 0.950, LOSS: test = 1.086, train = 0.456\nMAX_DF=0.60_MIN_COUNT=80_tf_lr=1e-2-----------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  22,   ACCURACY: test = 0.716, train = 0.971, LOSS: test = 1.046, train = 0.335\nMAX_DF=0.60_MIN_COUNT=80_tflidf_lr=1e-2-------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.731, train = 0.975, LOSS: test = 1.056, train = 0.286\n\n\nВарианты c min_token_size=2\nMAX_DF=0.95_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 25937-----------------------------------------\nBEST MODEL CALC:   Epoch =  35,   ACCURACY: test = 0.841, train = 0.996, LOSS: test = 0.660, train = 0.182\nMAX_DF=0.60_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 25924-----------------------------------------\nBEST MODEL CALC:   Epoch =  28,   ACCURACY: test = 0.844, train = 0.998, LOSS: test = 0.658, train = 0.128\nMAX_DF=0.30_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 25883-----------------------------------------\nBEST MODEL CALC:   Epoch =  31,   ACCURACY: test = 0.845, train = 0.998, LOSS: test = 0.656, train = 0.082\nMIN_COUNT=2_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 56427-----------------------------------------\nBEST MODEL CALC:   Epoch =  29,   ACCURACY: test = 0.847, train = 0.998, LOSS: test = 0.645, train = 0.122\nMIN_COUNT=20_tflidf_min_token_size=2_lr=1e-2 Размер словаря: 9189-----------------------------------------\nBEST MODEL CALC:   Epoch =  45,   ACCURACY: test = 0.821, train = 0.991, LOSS: test = 0.734, train = 0.225\nMIN_COUNT=80_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 2991-----------------------------------------\nBEST MODEL CALC:   Epoch =  45,   ACCURACY: test = 0.766, train = 0.970, LOSS: test = 0.954, train = 0.361\nMAX_DF=0.98_MIN_COUNT=2_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 56433-----------------------------\nBEST MODEL CALC:   Epoch =  37,   ACCURACY: test = 0.845, train = 0.998, LOSS: test = 0.654, train = 0.139\nMAX_DF=0.60_MIN_COUNT=80_min_token_size=2_tflidf_lr=1e-2 Размер словаря: 2983-----------------------------\nBEST MODEL CALC:   Epoch =  24,   ACCURACY: test = 0.765, train = 0.964, LOSS: test = 0.922, train = 0.436","metadata":{}},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\n# print(' '.join(train_tokenized[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.95\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.95_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.95_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.95_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.60\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.30\n# MIN_COUNT = 5\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.30_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.30_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.30_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.80\n# MIN_COUNT = 2\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=2_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=2_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=2_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.80\n# MIN_COUNT = 20\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=20_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=20_tf_min_token_size=2_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=20_tflidf_min_token_size=2_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.80\n# MIN_COUNT = 80\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=30_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=80_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MIN_COUNT=80_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.98\n# MIN_COUNT = 2\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.98_MIN_COUNT=2_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.98_MIN_COUNT=2_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.98_MIN_COUNT=2_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.60\n# MIN_COUNT = 80\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# # plt.hist(train_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в обучающей выборке');\n\n# # plt.hist(test_source['target'], bins=np.arange(0, 21))\n# # plt.title('Распределение меток в тестовой выборке');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_MIN_COUNT=80_min_token_size=2_default'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-1,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_MIN_COUNT=80_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# # print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# # print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# # print()\n# # print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# # print()\n# # print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# # print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'MAX_DF=0.60_MIN_COUNT=80_min_token_size=2_tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ниже изучение параметров частотности словаря и распеределения по по количеству токенов","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\n# print(' '.join(train_tokenized[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 1\n# MIN_COUNT = 0\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(list(vocabulary.items())[:20])\n# word_doc_freq[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(list(vocabulary.items())[-20:])\n# word_doc_freq[-20:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.hist(word_doc_freq, bins=20)\n# plt.title('Распределение относительных частот слов')\n# plt.yscale('log');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперименты с архитектурой нейросети","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\nДобавление одного дополнительного линейного слоя (в 120 нейронов) немного улучшило качество (на 0.5%), еще чуть улучшило добавление \nодного дропаута после скрытого линейного слоя (p=0.2-0.6). Вариации количества нейронов в скрытом слое (от 60 до 500), показывают \n+- одинаковые результаты. Слой нормализации по батчу существенно ухудшил результаты (если задуматься, что при такой нормализации \nв пределах батча присходит, то такое ухудшение становится логичным). Добавление более 1 скрытого слоя, приводит к ухудшению \nрезультатов даже в сравнении с изначальным одним слоем.\n\nИтого лучшие результаты:\nNN_Linear(120)_ReLU_Dropout(0.6)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.845, train = 0.998,   LOSS: test = 0.550, train = 0.007  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.2)__tflidf_lr=1e-2---------------------------------------------------------\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.843, train = 0.999,   LOSS: test = 0.566, train = 0.007  DICT SIZE = 21628\n\nВсе эксперименты:\nNN_Linear(60)_ReLU__tflidf_lr=1e-2------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.837, train = 0.999,   LOSS: test = 0.595, train = 0.006  DICT SIZE = 21628\nNN_Linear(120)_ReLU__tflidf_lr=1e-2-----------------------------------------------------\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.836, train = 0.999,   LOSS: test = 0.606, train = 0.005  DICT SIZE = 21628\nNN_Linear(240)_ReLU__tflidf_lr=1e-2-----------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.839, train = 0.999,   LOSS: test = 0.584, train = 0.005  DICT SIZE = 21628\nNN_Linear(500)_ReLU__tflidf_lr=1e-2-----------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  22,   ACCURACY: test = 0.835, train = 0.999,   LOSS: test = 0.694, train = 0.001  DICT SIZE = 21628\n\nNN_Linear(120)_ReLU_BatchNorm1d__tflidf_lr=1e-2-----------------------------------------------------------\nBEST MODEL CALC:   Epoch =  30,   ACCURACY: test = 0.787, train = 0.989,   LOSS: test = 10.439, train = 0.611  DICT SIZE = 21628\nNN_Linear(120)_ReLU_2xBatchNorm1d__tflidf_lr=1e-2---------------------------------------------------\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.775, train = 0.993,   LOSS: test = 1.492, train = 0.061  DICT SIZE = 21628\n\nNN_Linear(120)_ReLU_Dropout(0.1)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =  19,   ACCURACY: test = 0.840, train = 0.999,   LOSS: test = 0.649, train = 0.001  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.2)__tflidf_lr=1e-2---------------------------------------------------------\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.843, train = 0.999,   LOSS: test = 0.566, train = 0.007  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.3)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.837, train = 0.999,   LOSS: test = 0.613, train = 0.003  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.4)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.840, train = 0.999,   LOSS: test = 0.641, train = 0.001  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.6)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.845, train = 0.998,   LOSS: test = 0.550, train = 0.007  DICT SIZE = 21628\nNN_Linear(120)_ReLU_Dropout(0.8)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =  13,   ACCURACY: test = 0.835, train = 0.997,   LOSS: test = 0.641, train = 0.015  DICT SIZE = 21628\nNN_Linear(120)_ReLU_2xDropout(0.2)__tflidf_lr=1e-2--------------------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.838, train = 0.999,   LOSS: test = 0.570, train = 0.006  DICT SIZE = 21628\n\nNN_Linear(240)_ReLU_Dropout(0.6)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.845, train = 0.999,   LOSS: test = 0.553, train = 0.006  DICT SIZE = 21628\nNN_Linear(240)_ReLU_Dropout(0.2)__tflidf_lr=1e-2----------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.840, train = 0.999,   LOSS: test = 0.584, train = 0.004  DICT SIZE = 21628\n\nNN_Linear(480)_Linear(120)_ReLU__tflidf_lr=1e-2-----------------------------------------------------\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.796, train = 0.999,   LOSS: test = 0.933, train = 0.005  DICT SIZE = 21628\nNN_Linear(480)_Linear(120)_ReLU_2xDropout(0.2)__tflidf_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =  35,   ACCURACY: test = 0.798, train = 0.999,   LOSS: test = 1.396, train = 0.001  DICT SIZE = 21628\nNN_Linear(480)_Linear(120)_ReLU_2xDropout(0.4)__tflidf_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =  30,   ACCURACY: test = 0.816, train = 0.999,   LOSS: test = 1.085, train = 0.001  DICT SIZE = 21628\n\nNN_Dropout(0.4)__tflidf_lr=1e-2---------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.833, train = 0.997,   LOSS: test = 0.966, train = 0.124  DICT SIZE = 21628\n","metadata":{}},{"cell_type":"code","source":"train_tokenized = tokenize_corpus(train_source['data'], min_token_size=4)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=4)\n\nprint(' '.join(train_tokenized[0]))\n\nMAX_DF = 0.80\nMIN_COUNT = 5\n\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\n# word_doc_freq[:20]\n\n# word_doc_freq[-20:]\n\n# plt.hist(word_doc_freq, bins=20)\n# plt.title('Распределение относительных частот слов')\n# plt.yscale('log');\n\nVECTORIZATION_MODE = 'tflidf'\ntrain_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\ntest_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# plt.hist(train_vectors.data, bins=20)\n# plt.title('Распределение весов признаков')\n# plt.yscale('log');\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:27:21.540929Z","iopub.execute_input":"2022-01-27T10:27:21.541526Z","iopub.status.idle":"2022-01-27T10:29:15.249245Z","shell.execute_reply.started":"2022-01-27T10:27:21.54149Z","shell.execute_reply":"2022-01-27T10:29:15.248456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hidden01_neurons = 240\n# dropout_p = 0.6\n# model = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, hidden01_neurons),\n#                             torch.nn.Dropout(dropout_p),\n#                             torch.nn.ReLU(),\n#                             nn.Linear(hidden01_neurons, UNIQUE_LABELS_N))\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'NN_Linear(240)_ReLU_Dropout(0.6)__tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - изменить способ стандартизации данных","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\nПроводил стандартизацию через sklearn.preprocessing.StandardScaler(with_mean=False), в этом случае, она подходит \nдля разреженных матриц. Результаты модели значительно ухудшились.\n\nStandardScaler__tflidf_lr=1e-2----------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.687, train = 0.999,   LOSS: test = 6.822, train = 0.021  DICT SIZE = 21628\nStandardScaler__tfidf_lr=1e-2-----------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.688, train = 0.998,   LOSS: test = 6.298, train = 0.073  DICT SIZE = 21628\nStandardScaler__tf_lr=1e-2--------------------------------------------------------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.691, train = 1.000,   LOSS: test = 7.676, train = 0.019  DICT SIZE = 21628","metadata":{}},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=4)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=4)\n\n# print(' '.join(train_tokenized[0]))\n\n# MAX_DF = 0.80\n# MIN_COUNT = 5\n\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_freq[:20]\n\n# # word_doc_freq[-20:]\n\n# # plt.hist(word_doc_freq, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tflidf'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:04:14.003696Z","iopub.execute_input":"2022-01-27T13:04:14.004054Z","iopub.status.idle":"2022-01-27T13:06:28.358662Z","shell.execute_reply.started":"2022-01-27T13:04:14.004019Z","shell.execute_reply":"2022-01-27T13:06:28.356891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler(with_mean=False)\n# train_vectors = scaler.fit_transform(train_vectors)\n# test_vectors = scaler.transform(test_vectors)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:06:28.360792Z","iopub.execute_input":"2022-01-27T13:06:28.361154Z","iopub.status.idle":"2022-01-27T13:06:28.398602Z","shell.execute_reply.started":"2022-01-27T13:06:28.361111Z","shell.execute_reply":"2022-01-27T13:06:28.397834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:06:28.400112Z","iopub.execute_input":"2022-01-27T13:06:28.401161Z","iopub.status.idle":"2022-01-27T13:06:28.406852Z","shell.execute_reply.started":"2022-01-27T13:06:28.401113Z","shell.execute_reply":"2022-01-27T13:06:28.405805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'StandardScaler__tflidf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:06:28.409476Z","iopub.execute_input":"2022-01-27T13:06:28.409834Z","iopub.status.idle":"2022-01-27T13:09:16.599613Z","shell.execute_reply.started":"2022-01-27T13:06:28.409788Z","shell.execute_reply":"2022-01-27T13:09:16.598107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:09:16.601588Z","iopub.execute_input":"2022-01-27T13:09:16.601935Z","iopub.status.idle":"2022-01-27T13:09:16.610829Z","shell.execute_reply.started":"2022-01-27T13:09:16.601881Z","shell.execute_reply":"2022-01-27T13:09:16.609175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - извлекать признаки не через токены, а через N-граммы","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:57:44.360809Z","iopub.execute_input":"2022-01-27T11:57:44.36107Z","iopub.status.idle":"2022-01-27T11:57:44.365202Z","shell.execute_reply.started":"2022-01-27T11:57:44.361041Z","shell.execute_reply":"2022-01-27T11:57:44.364276Z"}}},{"cell_type":"raw","source":"ИТОГИ:\n    Применение N-грамм (по словам) существенно увеличивает размер словаря (для (2,2) - 86086, (1,2) - 112018, (1,3) - 819495),\n    но не приносит значимых улучшений результатов, если использовать только n-граммы > 1 (т.е. не добавлять в словарь единичные \n    токены), то результаты значимо ухудшаются.\n    Для отбора n-грамм использовал sklearn.feature_extraction.text.TfidfVectorizer\n    \n\nNGRAM_RANGE=(2,2)_MIN_COUNT=5__sklearn.tfidfVectorizer_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.766, train = 0.999,   LOSS: test = 1.058, train = 0.091  DICT SIZE = 86086\nNGRAM_RANGE=(1,2)_MIN_COUNT=5__sklearn.tfidfVectorizer_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.834, train = 1.000,   LOSS: test = 0.618, train = 0.003  DICT SIZE = 112018\nNGRAM_RANGE=(1,2)_MIN_COUNT=2__sklearn.tfidfVectorizer_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.839, train = 0.999,   LOSS: test = 0.726, train = 0.024  DICT SIZE = 385496\nNGRAM_RANGE=(1,3)_MIN_COUNT=2__sklearn.tfidfVectorizer_lr=1e-2--------------------------------------\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.828, train = 0.999,   LOSS: test = 0.797, train = 0.028  DICT SIZE = 819495","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# MAX_DF = 0.8\n# MIN_COUNT = 5\n# NGRAM_RANGE=(2,2)\n\n# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=NGRAM_RANGE, max_df=MAX_DF, min_df=MIN_COUNT)\n# train_vectors = vectorizer.fit_transform(train_source['data'])\n# test_vectors = vectorizer.transform(test_source['data'])\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# UNIQUE_WORDS_N = train_vectors.shape[-1]\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'NGRAM_RANGE=(2,2)_MIN_COUNT=5__sklearn.tfidfVectorizer_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:11:29.472108Z","iopub.execute_input":"2022-01-27T13:11:29.472528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# MAX_DF = 0.8\n# MIN_COUNT = 5\n# NGRAM_RANGE=(1,2)\n\n# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=NGRAM_RANGE, max_df=MAX_DF, min_df=MIN_COUNT)\n# train_vectors = vectorizer.fit_transform(train_source['data'])\n# test_vectors = vectorizer.transform(test_source['data'])\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# UNIQUE_WORDS_N = train_vectors.shape[-1]\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'NGRAM_RANGE=(1,2)_MIN_COUNT=5__sklearn.tfidfVectorizer_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:11:29.472108Z","iopub.execute_input":"2022-01-27T13:11:29.472528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# MAX_DF = 0.8\n# MIN_COUNT = 2\n# NGRAM_RANGE=(1,2)\n\n# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=NGRAM_RANGE, max_df=MAX_DF, min_df=MIN_COUNT)\n# train_vectors = vectorizer.fit_transform(train_source['data'])\n# test_vectors = vectorizer.transform(test_source['data'])\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# UNIQUE_WORDS_N = train_vectors.shape[-1]\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'NGRAM_RANGE=(1,2)_MIN_COUNT=2__sklearn.tfidfVectorizer_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:11:29.472108Z","iopub.execute_input":"2022-01-27T13:11:29.472528Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_DF = 0.8\n# MIN_COUNT = 2\n# NGRAM_RANGE=(1,3)\n\n# vectorizer = TfidfVectorizer(analyzer='word', ngram_range=NGRAM_RANGE, max_df=MAX_DF, min_df=MIN_COUNT)\n# train_vectors = vectorizer.fit_transform(train_source['data'])\n# test_vectors = vectorizer.transform(test_source['data'])\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# UNIQUE_WORDS_N = train_vectors.shape[-1]\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'NGRAM_RANGE=(1,3)_MIN_COUNT=2__sklearn.tfidfVectorizer_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T13:11:29.472108Z","iopub.execute_input":"2022-01-27T13:11:29.472528Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - взвешивание признаков с помощью точечной взаимной информации (PMI)","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\nИтоговые результаты оказались примерно на уровне векторизации tf-lidf (именно lidf, т.е. idf с log), хотя в процессе того, как \nпервично разобрался как происходит взвешивание у меня были существенные надежды на возможность отбора словаря, на основе PMI \n(это к вопросу примечаниях Романа Суворова, что отбор > взвешивания). Но когда еще более подробно разобрался в получении конечных \nвесов для разных вариантов токенов, то немного снизил ожидания, т.к. на мой взгляд у этого метода есть существенный недостаток, а\nименно слишком высокие веса у редко встречающихся токенов, по итогу, мы не можем из словаря выкинуть на основе весов, токены, \nкоторые встречаются 1-2 раза для одного класса, т.к. они будут иметь ТАКОЙ ЖЕ вес, как и те, которые например встречаются \nмногократно и только для одного конкретного класса. Как следствие, модель будет явно склонна к быстрому переобучению, на основе \nтаких токенов. Понятно что здесь мы можем отсекать подобные через через MIN_COUNT, но все же именно сами веса, никак не разделяют\nподобные токены.\nЕсли это проблему как-то можно обойти, модифицировав взвешивание, просьба отписаться. Я уже не стал более глубоко копать, т.к. был \nограничен по времени.\n\nВот простой пример (на основе крайностей). Допустим мы имеем 10 000 документов, равномерно распределенных по 20 классам (т.е. по 500 \nдокумнтов на класс). И имеем один токен, который встречается только 1 раз во всем корпусе (допустим в классе №1), и имеем другой \nтокен, который, встречается в каждом документе класса №1 и только в нем (т.е. в 500 документах из 500 класса 1, и более нигде). \nПонятно, что 2-й токен супер информативен, только на основе одного него мы спожем относить документы к классу №1. Но вот при \nвзвешивании PMI мы получим абсолютно одинаковые веса этих 2-х токенов.\n\nДля варианта когда знаменатель во взаимной вероятности = количеству документов в классе (как в формуле из видео курса)\npmi(l1, w1) = log ((1/500) / ((1/10000)*(500/10000))) = 5.991\npmi(l1, w2) = log ((500/500) / ((500/10000)*(500/10000))) = 5.991\n\nДля варианта когда знаменатель во взаимной вероятности = общему количеству документов в корпусе (как в классической формуле PMI)\npmi(l1, w1) = log ((1/500) / ((1/10000)*(500/10000))) = 2.996\npmi(l1, w2) = log ((500/500) / ((500/10000)*(500/10000))) = 2.996\n\nОсобой разницы в двух вариантах формул о которых \"спорили\" в комментариях к видео по PMI нет, на мой взгляд вариант из видео лучше, \nт.к. будет учитывать неравномерное распределение классов (хотя не уверен, т.к. не проверял.\n\nИтого по реализации. Делал ее на основе разъяснения Романа Суворова \nздесь https://stepik.org/lesson/225311/step/9?discussion=1231897&reply=1234485&unit=198054\nи частично из статьи (их комментариев выше)\nhttps://web.eecs.umich.edu/~wangluxy/archive/neu_courses/cs6120_fa2019/slides_cs6120_fa19/semantics_part1.pdf\n\nРеализовал на мой взгляд все возможные варианты, а именно:\n- знаменатель взаимной вероятности: вариант c Size(l), вариант c Size(c)\n- использовал/не использовал негативные вероятности (т.е. отдельно рассчитанные вероятности \"слово w не встретилось в документе\")\n- пробовал варианты подсчета max и mean по классам (к слову mean показал значительное ухудшение результатов, все остальные варианты \n  выше дали +- сопоставимые результаты)\n  \nP.S. Несмотря на мои надежды, в итоге все же победил вариант взвешивания tf-lidf (хотя разница всего в несколько десятых %).\n\nЛучший вариант:\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.841, train = 0.988,   LOSS: test = 0.706, train = 0.238  DICT SIZE = 20738\n\n\nВсе эксперименты:\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=_5_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  57,   ACCURACY: test = 0.836, train = 0.995,   LOSS: test = 0.716, train = 0.143  DICT SIZE = 25941\nPMI_TYPE=mean_PMI_Q_THRESHOLD=1.0_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  30,   ACCURACY: test = 0.804, train = 0.998,   LOSS: test = 0.792, train = 0.096  DICT SIZE = 25941\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.841, train = 0.988,   LOSS: test = 0.706, train = 0.238  DICT SIZE = 20738\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.829, train = 0.983,   LOSS: test = 0.754, train = 0.189  DICT SIZE = 15561\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.783, train = 0.944,   LOSS: test = 0.958, train = 0.394  DICT SIZE = 10376\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.2_MIN_COUNT=5_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.679, train = 0.827,   LOSS: test = 1.231, train = 0.653  DICT SIZE = 5185\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=2_min_token_size=2_tf_lr=1e-2----------------------------\nBEST MODEL CALC:   Epoch =  59,   ACCURACY: test = 0.840, train = 0.997,   LOSS: test = 0.725, train = 0.107  DICT SIZE = 56436\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=10_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  32,   ACCURACY: test = 0.829, train = 0.993,   LOSS: test = 0.719, train = 0.173  DICT SIZE = 15593\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=20_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  59,   ACCURACY: test = 0.817, train = 0.989,   LOSS: test = 0.786, train = 0.211  DICT SIZE = 9198\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=50_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  37,   ACCURACY: test = 0.782, train = 0.973,   LOSS: test = 0.913, train = 0.342  DICT SIZE = 4444\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=10_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  16,   ACCURACY: test = 0.837, train = 0.994,   LOSS: test = 0.632, train = 0.097  DICT SIZE = 12453\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=30_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.803, train = 0.973,   LOSS: test = 0.776, train = 0.237  DICT SIZE = 3984\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=30_min_token_size=2_tf_lr=1e-2---------------------------\nBEST MODEL CALC:   Epoch =  17,   ACCURACY: test = 0.786, train = 0.968,   LOSS: test = 0.802, train = 0.179  DICT SIZE = 2656\n\nNO_NEG:\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=_5_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  59,   ACCURACY: test = 0.837, train = 0.994,   LOSS: test = 0.703, train = 0.160  DICT SIZE = 25941\nPMI_TYPE=mean_PMI_Q_THRESHOLD=1.0_MIN_COUNT=5_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  38,   ACCURACY: test = 0.788, train = 1.000,   LOSS: test = 0.814, train = 0.034  DICT SIZE = 25941\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=5_NO_NEG_min_token_size=2_tf_lr=1e-2---------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.842, train = 0.992,   LOSS: test = 0.631, train = 0.148  DICT SIZE = 20738\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=5_NO_NEG_min_token_size=2_tf_lr=1e-2---------------------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.831, train = 0.981,   LOSS: test = 0.788, train = 0.227  DICT SIZE = 15561\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=5_NO_NEG_min_token_size=2_tf_lr=1e-2---------------------\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.784, train = 0.944,   LOSS: test = 0.959, train = 0.400  DICT SIZE = 10376\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.2_MIN_COUNT=5_NO_NEG_min_token_size=2_tf_lr=1e-2---------------------\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.679, train = 0.824,   LOSS: test = 1.253, train = 0.705  DICT SIZE = 5185\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=2_NO_NEG_min_token_size=2_tf_lr=1e-2---------------------\nBEST MODEL CALC:   Epoch =  55,   ACCURACY: test = 0.840, train = 0.997,   LOSS: test = 0.713, train = 0.120  DICT SIZE = 56436\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=10_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  43,   ACCURACY: test = 0.830, train = 0.992,   LOSS: test = 0.717, train = 0.173  DICT SIZE = 15593\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=20_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  53,   ACCURACY: test = 0.816, train = 0.987,   LOSS: test = 0.771, train = 0.235  DICT SIZE = 9198\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=50_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  22,   ACCURACY: test = 0.783, train = 0.971,   LOSS: test = 0.890, train = 0.383  DICT SIZE = 4444\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=10_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.838, train = 0.990,   LOSS: test = 0.686, train = 0.168  DICT SIZE = 12455\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=30_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.804, train = 0.973,   LOSS: test = 0.779, train = 0.245  DICT SIZE = 3984\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=30_NO_NEG_min_token_size=2_tf_lr=1e-2--------------------\nBEST MODEL CALC:   Epoch =  18,   ACCURACY: test = 0.787, train = 0.969,   LOSS: test = 0.806, train = 0.172  DICT SIZE = 2656\n\n\nPWL_DENOM=size(c)\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=_5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  48,   ACCURACY: test = 0.835, train = 0.994,   LOSS: test = 0.706, train = 0.158  DICT SIZE = 25941\nPMI_TYPE=mean_PMI_Q_THRESHOLD=1.0_MIN_COUNT=5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  34,   ACCURACY: test = 0.804, train = 0.998,   LOSS: test = 0.798, train = 0.087  DICT SIZE = 25941\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2----------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.842, train = 0.988,   LOSS: test = 0.706, train = 0.238  DICT SIZE = 20738\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2----------\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.830, train = 0.983,   LOSS: test = 0.753, train = 0.189  DICT SIZE = 15561\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2----------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.783, train = 0.953,   LOSS: test = 0.909, train = 0.295  DICT SIZE = 10376\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.2_MIN_COUNT=5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2----------\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.679, train = 0.829,   LOSS: test = 1.220, train = 0.618  DICT SIZE = 5185\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=2_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2----------\nBEST MODEL CALC:   Epoch =  49,   ACCURACY: test = 0.839, train = 0.997,   LOSS: test = 0.725, train = 0.108  DICT SIZE = 56436\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=10_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  32,   ACCURACY: test = 0.829, train = 0.992,   LOSS: test = 0.714, train = 0.185  DICT SIZE = 15593\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=20_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  43,   ACCURACY: test = 0.815, train = 0.988,   LOSS: test = 0.775, train = 0.232  DICT SIZE = 9198\nPMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=50_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  35,   ACCURACY: test = 0.783, train = 0.977,   LOSS: test = 0.932, train = 0.307  DICT SIZE = 4444\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.8_MIN_COUNT=10_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.837, train = 0.990,   LOSS: test = 0.686, train = 0.165  DICT SIZE = 12453\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.6_MIN_COUNT=30_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.804, train = 0.971,   LOSS: test = 0.792, train = 0.266  DICT SIZE = 3984\nPMI_TYPE=max_PMI_Q_THRESHOLD=0.4_MIN_COUNT=30_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2---------\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.787, train = 0.955,   LOSS: test = 0.800, train = 0.263  DICT SIZE = 2656\n","metadata":{}},{"cell_type":"code","source":"# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\n# print(' '.join(train_tokenized[0]))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:17:58.264341Z","iopub.execute_input":"2022-01-28T13:17:58.264904Z","iopub.status.idle":"2022-01-28T13:18:02.13947Z","shell.execute_reply.started":"2022-01-28T13:17:58.26486Z","shell.execute_reply":"2022-01-28T13:18:02.13853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PMI_TYPE = 'max'\n# PMI_Q_THRESHOLD = 1.0\n# MIN_COUNT = 5\n# min_token_size=2\n# NO_NEG = False\n# PWL_DENOM = 'size(c)'\n\n# vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n#                                                 min_count = MIN_COUNT, pmi_type = PMI_TYPE, \n#                                                 invers_quantile_threshold = PMI_Q_THRESHOLD, \n#                                                 no_negative = NO_NEG, p_w_l_denominator = PWL_DENOM)\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# # word_doc_pmi[:20]\n\n# # word_doc_pmi[-20:]\n\n# # plt.hist(word_doc_pmi, bins=20)\n# # plt.title('Распределение относительных частот слов')\n# # plt.yscale('log');\n\n# VECTORIZATION_MODE = 'tfpmi'\n# train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n# test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# # plt.hist(train_vectors.data, bins=20)\n# # plt.title('Распределение весов признаков')\n# # plt.yscale('log');\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n\n# experiment_name = 'PMI_TYPE=max_PMI_Q_THRESHOLD=1.0_MIN_COUNT=_5_PWL_DENOM=size(c)_min_token_size=2_tf_lr=1e-2'\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=1e-2,\n#                                             epoch_n=200,\n#                                             batch_size=32,\n#                                             l2_reg_alpha=0,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             experiment_name = experiment_name)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:13:25.749813Z","iopub.execute_input":"2022-01-28T13:13:25.750031Z","iopub.status.idle":"2022-01-28T13:14:58.301553Z","shell.execute_reply.started":"2022-01-28T13:13:25.750003Z","shell.execute_reply":"2022-01-28T13:14:58.300643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_experiments_stats(histories, show_plots = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Финальные эксперименты (микс из лучшего)","metadata":{}},{"cell_type":"raw","source":"Best of the best:\nmtoken_size=1_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.867, train = 1.000,   LOSS: test = 0.982, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.5_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.866, train = 0.999,   LOSS: test = 1.034, train = 0.002  DICT SIZE = 67164\n\n\n\nFinal experiments (run 01)\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.852, train = 0.998,   LOSS: test = 1.029, train = 0.015  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0.001_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.053, train = 0.053,   LOSS: test = 2.991, train = 2.991  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=1e-05_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.845, train = 0.994,   LOSS: test = 0.706, train = 0.090  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.1_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.470, train = 0.865,   LOSS: test = 4.134, train = 0.813  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  30,   ACCURACY: test = 0.854, train = 0.992,   LOSS: test = 0.604, train = 0.093  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.859, train = 0.998,   LOSS: test = 0.908, train = 0.017  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=128_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.858, train = 0.998,   LOSS: test = 0.785, train = 0.025  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.4_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.862, train = 0.999,   LOSS: test = 0.910, train = 0.006  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=0.001_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.086, train = 0.057,   LOSS: test = 2.989, train = 2.990  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=1e-05_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  33,   ACCURACY: test = 0.844, train = 0.988,   LOSS: test = 0.586, train = 0.157  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.98_tflidf_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.856, train = 0.996,   LOSS: test = 0.934, train = 0.027  DICT SIZE = 67171\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   7,   ACCURACY: test = 0.848, train = 0.997,   LOSS: test = 0.706, train = 0.011  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0.001_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  24,   ACCURACY: test = 0.743, train = 0.825,   LOSS: test = 1.641, train = 1.578  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=1e-05_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   3,   ACCURACY: test = 0.833, train = 0.983,   LOSS: test = 0.589, train = 0.092  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.1_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.810, train = 0.981,   LOSS: test = 1.110, train = 0.092  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  20,   ACCURACY: test = 0.848, train = 0.994,   LOSS: test = 0.557, train = 0.030  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   4,   ACCURACY: test = 0.850, train = 0.989,   LOSS: test = 0.567, train = 0.047  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=128_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  11,   ACCURACY: test = 0.850, train = 0.998,   LOSS: test = 0.634, train = 0.011  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.4_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =   2,   ACCURACY: test = 0.845, train = 0.983,   LOSS: test = 0.559, train = 0.077  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=0.001_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  55,   ACCURACY: test = 0.747, train = 0.825,   LOSS: test = 1.560, train = 1.478  DICT SIZE = 25338\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.8_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.001_BATCH=32_L2REG=1e-05_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.842, train = 0.981,   LOSS: test = 0.562, train = 0.117  DICT SIZE = 25338\n\n\nFinal experiments (run 02)\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.5_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  12,   ACCURACY: test = 0.865, train = 0.999,   LOSS: test = 1.162, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.5_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.866, train = 0.999,   LOSS: test = 1.034, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.4_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=2\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.863, train = 0.999,   LOSS: test = 1.030, train = 0.004  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.4_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   8,   ACCURACY: test = 0.858, train = 0.999,   LOSS: test = 1.103, train = 0.004  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=120_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  21,   ACCURACY: test = 0.860, train = 0.999,   LOSS: test = 1.085, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.866, train = 0.999,   LOSS: test = 0.925, train = 0.003  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.98_tfpmi_NN_Linear=120_Dropout=0.6_lr=0.01_BATCH=32_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  14,   ACCURACY: test = 0.857, train = 0.999,   LOSS: test = 1.322, train = 0.002  DICT SIZE = 31040\nmtoken_size=2_MIN_COUNT=5_ADD_STEMMS_MAX_QUAN=0.98_tfpmi_NN_Linear=240_Dropout=0.5_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=2\n\n\nFinal experiments (run 03)\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.865, train = 0.999,   LOSS: test = 0.775, train = 0.011  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=340_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  27,   ACCURACY: test = 0.864, train = 1.000,   LOSS: test = 1.071, train = 0.001  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=400_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.864, train = 0.999,   LOSS: test = 0.941, train = 0.002  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.98_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  16,   ACCURACY: test = 0.865, train = 0.999,   LOSS: test = 0.993, train = 0.002  DICT SIZE = 67171\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.3_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.862, train = 0.999,   LOSS: test = 0.910, train = 0.003  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.2_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  10,   ACCURACY: test = 0.862, train = 0.999,   LOSS: test = 0.917, train = 0.003  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=120_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   9,   ACCURACY: test = 0.863, train = 1.000,   LOSS: test = 0.838, train = 0.005  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=1e-06_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =   5,   ACCURACY: test = 0.858, train = 0.999,   LOSS: test = 0.751, train = 0.020  DICT SIZE = 67164\nmtoken_size=2_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=6\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.862, train = 0.999,   LOSS: test = 1.092, train = 0.002  DICT SIZE = 67164\nmtoken_size=1_MIN_COUNT=2_ADD_STEMMS_MAX_DF=0.8_tflidf_NN_Linear=240_Dropout=0.4_lr=0.01_BATCH=64_L2REG=0_SCHED_PAT=4\nBEST MODEL CALC:   Epoch =  15,   ACCURACY: test = 0.867, train = 1.000,   LOSS: test = 0.982, train = 0.002  DICT SIZE = 67164","metadata":{}},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 340\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 400\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.98\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.3\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.2\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 120\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 1e-6\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=2\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 6\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_token_size=1\nADD_STEMMS = True\nMIN_COUNT = 2\nVECTORIZATION_MODE = 'tflidf'\n\nif 'pmi' in VECTORIZATION_MODE:\n    MAX_QUAN = 0.8\n    MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\nelse:\n    MAX_DF = 0.8\n    MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\nHIDDEN_NEURONS = 240\nDROPOUT_p = 0.4\nBATCH_SIZE = 64\nLR = 1e-2\nL2REG = 0\nSCHED_PAT = 4\n\nexperiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n                    min_token_size, MIN_COUNT, \n                    MAX_TYPE,\n                    VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\ntrain_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\ntest_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\nprint(' '.join(train_tokenized[0]))\n\nif ADD_STEMMS:\n    #PorterStemmer:\n    from nltk.stem.porter import PorterStemmer\n    ps = PorterStemmer()\n    converter = ps.stem\n\n    train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n    test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n    print(' '.join(train_tokenized[0]))\n\n    \nif 'pmi' in VECTORIZATION_MODE:\n    vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n                                                invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\nelse:\n    vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n    train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n    test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Количество уникальных токенов', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])\n\nprint('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\nprint('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\nprint()\nprint('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\nprint()\nprint('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\nprint('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\nUNIQUE_LABELS_N = len(set(train_source['target']))\nprint('Количество уникальных меток', UNIQUE_LABELS_N)\nset(train_source['target'])\n\ntrain_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\ntest_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\nmodel = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n                            torch.nn.Dropout(DROPOUT_p),\n                            torch.nn.ReLU(),\n                            nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\nscheduler = lambda optim: \\\n    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\nhistories[experiment_name], best_models[experiment_name] = train_eval_loop(\n                                            model=model,\n                                            train_dataset=train_dataset,\n                                            val_dataset=test_dataset,\n                                            criterion=F.cross_entropy,\n                                            lr=LR,\n                                            epoch_n=200,\n                                            batch_size=BATCH_SIZE,\n                                            l2_reg_alpha=L2REG,\n                                            lr_scheduler_ctor=scheduler,\n                                            best_acc_type = 'acc',\n                                            test_dataset=test_dataset,\n                                            early_stopping_patience=6,\n                                            experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_token_size=2\n# ADD_STEMMS = True\n# MIN_COUNT = 2\n# VECTORIZATION_MODE = 'tfpmi'\n\n# if 'pmi' in VECTORIZATION_MODE:\n#     MAX_QUAN = 0.98\n#     MAX_TYPE = 'MAX_QUAN={}'.format(MAX_QUAN) #only for generate experiment_name\n# else:\n#     MAX_DF = 0.8\n#     MAX_TYPE = 'MAX_DF={}'.format(MAX_DF) #only for generate experiment_name\n\n# HIDDEN_NEURONS = 240\n# DROPOUT_p = 0.4\n# BATCH_SIZE = 32\n# LR = 1e-2\n# L2REG = 0\n# SCHED_PAT = 4\n\n# experiment_name = 'mtoken_size={}_MIN_COUNT={}_ADD_STEMMS_{}_{}_NN_Linear={}_Dropout={}_lr={}_BATCH={}_L2REG={}_SCHED_PAT={}'.format(\n#                     min_token_size, MIN_COUNT, \n#                     MAX_TYPE,\n#                     VECTORIZATION_MODE, HIDDEN_NEURONS, DROPOUT_p, LR, BATCH_SIZE, L2REG, SCHED_PAT)\n\n\n# train_tokenized = tokenize_corpus(train_source['data'], min_token_size=2)\n# test_tokenized = tokenize_corpus(test_source['data'], min_token_size=2)\n\n# print(' '.join(train_tokenized[0]))\n\n# if ADD_STEMMS:\n#     #PorterStemmer:\n#     from nltk.stem.porter import PorterStemmer\n#     ps = PorterStemmer()\n#     converter = ps.stem\n\n#     train_tokenized = tokenize_corpus_convert(train_tokenized, converter=converter, addition = True)\n#     test_tokenized = tokenize_corpus_convert(test_tokenized, converter=converter, addition = True)\n\n#     print(' '.join(train_tokenized[0]))\n\n    \n# if 'pmi' in VECTORIZATION_MODE:\n#     vocabulary, word_doc_pmi = build_vocabulary_pmi(train_tokenized, train_source['target'], \n#                                                 invers_quantile_threshold = MAX_QUAN, min_count = MIN_COUNT)\n    \n#     train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n#     test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_pmi, mode=VECTORIZATION_MODE)\n# else:\n#     vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n    \n#     train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n#     test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n\n\n\n# UNIQUE_WORDS_N = len(vocabulary)\n# print('Количество уникальных токенов', UNIQUE_WORDS_N)\n# print(list(vocabulary.items())[:10])\n\n# print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n# print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n# print()\n# print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n# print()\n# print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n# print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n\n# UNIQUE_LABELS_N = len(set(train_source['target']))\n# print('Количество уникальных меток', UNIQUE_LABELS_N)\n# set(train_source['target'])\n\n# train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n# test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])\n\n# model = torch.nn.Sequential(nn.Linear(UNIQUE_WORDS_N, HIDDEN_NEURONS),\n#                             torch.nn.Dropout(DROPOUT_p),\n#                             torch.nn.ReLU(),\n#                             nn.Linear(HIDDEN_NEURONS, UNIQUE_LABELS_N))\n\n# scheduler = lambda optim: \\\n#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=SCHED_PAT, factor=0.5, verbose=True)\n\n# histories[experiment_name], best_models[experiment_name] = train_eval_loop(\n#                                             model=model,\n#                                             train_dataset=train_dataset,\n#                                             val_dataset=test_dataset,\n#                                             criterion=F.cross_entropy,\n#                                             lr=LR,\n#                                             epoch_n=200,\n#                                             batch_size=BATCH_SIZE,\n#                                             l2_reg_alpha=L2REG,\n#                                             lr_scheduler_ctor=scheduler,\n#                                             best_acc_type = 'acc',\n#                                             test_dataset=test_dataset,\n#                                             early_stopping_patience=6,\n#                                             experiment_name = experiment_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_experiments_stats(histories, show_plots = True, only_BEST_MODEL_CALC = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}