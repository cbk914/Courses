{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:57:20.494118Z","iopub.execute_input":"2022-02-27T06:57:20.494472Z","iopub.status.idle":"2022-02-27T06:57:20.514862Z","shell.execute_reply.started":"2022-02-27T06:57:20.494393Z","shell.execute_reply":"2022-02-27T06:57:20.51423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\ndef character_tokenize(txt):\n    return list(txt)\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef tokenize_corpus_verbose(texts, tokenizer=tokenize_text_simple_regex, verbose_chunk=1000, **tokenizer_kwargs):\n    tokenize_texts = []\n    for i, text in enumerate(texts):\n        tokenize_texts.append(tokenizer(text, **tokenizer_kwargs))\n        if i % verbose_chunk == 0:\n            print('Complete: {}/{}'.format(i,len(texts)))\n    return tokenize_texts\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None, start_end_tag = False):\n    #modified add start/end tags of words\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n    \n    # добавим теги начала и конца слова, для задачи POS-tagging\n    if start_end_tag and pad_word is not None:\n        sorted_word_counts = [('<START>', 1)] + [('<END>', 2)] + sorted_word_counts\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf', 'tfpmi'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n        \n    elif mode == 'tfpmi': # TFPMI=TF⋅PMI\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(word2freq)  # домножить каждую строку на word2freq (это массив PMI Scores)\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    test_dataset = None,\n                    experiment_name = 'NoName',\n                    no_calculate_accuracy = False):\n    \"\"\"\n    v2.1\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                if not no_calculate_accuracy:\n                    train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                else: train_accuracy_epoch.append(0.)\n                    \n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    if not no_calculate_accuracy:\n                        val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                        #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    else:\n                        val_accuracy_epoch.append(0.)\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.4f}, train = {:.4f}    LOSS: val = {:.4f}, train = {:.4f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.4f}, train = {:.4f}, LOSS: val = {:.4f}, train = {:.4f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['BEST'] = {}\n    history['BEST']['epoch'] = best_epoch_i\n    history['BEST']['dict_size'] = batch_x.shape[-1]\n    \n    \n    #calculate and save final metrics best_model on train/val/test datasets\n    if test_dataset is not None:\n        history['BEST']['acc'] = {}\n        history['BEST']['loss'] = {}\n        \n        #save validation metrics (no calculate again)\n        history['BEST']['acc']['val'] = history['acc']['val'][best_epoch_i]\n        history['BEST']['loss']['val'] = history['loss']['val'][best_epoch_i]\n        \n        #calculate and save train metrics\n        train_pred = predict_with_model(best_model, train_dataset, return_labels=True)\n        history['BEST']['loss']['train'] = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n        history['BEST']['acc']['train'] = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n        \n        #calculate and save test metrics\n        test_pred = predict_with_model(best_model, test_dataset, return_labels=True)\n        history['BEST']['loss']['test'] = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n        history['BEST']['acc']['test'] = accuracy_score(test_pred[1], test_pred[0].argmax(-1))    \n    \n    \n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)\n\n\n#########stepik-dl-nlp/dlnlputils/nnets.py#########\n\nfrom torch.utils.data import Dataset\n\n\ndef ensure_length(txt, out_len, pad_value):\n    if len(txt) < out_len:\n        txt = list(txt) + [pad_value] * (out_len - len(txt))\n    else:\n        txt = txt[:out_len]\n    return txt\n\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, texts, targets, out_len=100, pad_value=0):\n        self.texts = texts\n        self.targets = targets\n        self.out_len = out_len\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n\n#########stepik-dl-nlp/dlnlputils/embeddings.py#########\n\nclass Embeddings:\n    def __init__(self, embeddings, word2id):\n        self.embeddings = embeddings\n        self.embeddings /= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n        self.word2id = word2id\n        self.id2word = {i: w for w, i in word2id.items()}\n\n    def most_similar(self, positive=None, negative=None, topk=10, with_mean = False):\n        #modified by wis, converted to gensim syntax\n        \n        if positive is not None:\n            if type(positive) != list:\n                positive = [positive]\n            pos_vec = [self.get_vector(word) for word in positive]\n            pos_len = len(positive)\n        else:\n            pos_vec = 0\n            pos_len = 1\n            \n        if negative is not None:\n            if type(negative) != list:\n                negative = [negative]\n            neg_vec = [self.get_vector(word) for word in negative]\n            neg_len = len(negative)\n        else:\n            neg_vec = 0\n            neg_len = 1\n        \n        if with_mean:\n            result_vec = np.array(pos_vec).sum(0) / pos_len - np.array(neg_vec).sum(0) / neg_len\n        else:\n            result_vec = np.array(pos_vec).sum(0) - np.array(neg_vec).sum(0)\n        \n        return self.most_similar_by_vector(result_vec, topk=topk)\n    \n    def most_similar_legacy(self, word, topk=10):\n        return self.most_similar_by_vector(self.get_vector(word), topk=topk)\n\n    def analogy(self, a1, b1, a2, topk=10):\n        a1_v = self.get_vector(a1)\n        b1_v = self.get_vector(b1)\n        a2_v = self.get_vector(a2)\n        query = b1_v - a1_v + a2_v\n        return self.most_similar_by_vector(query, topk=topk)\n\n    def most_similar_by_vector(self, query_vector, topk=10):\n        similarities = (self.embeddings * query_vector).sum(-1)\n        best_indices = np.argpartition(-similarities, topk, axis=0)[:topk]\n        result = [(self.id2word[i], similarities[i]) for i in best_indices]\n        result.sort(key=lambda pair: -pair[1])\n        return result\n\n    def get_vector(self, word):\n        if word not in self.word2id:\n            raise ValueError('Неизвестное слово \"{}\"'.format(word))\n        return self.embeddings[self.word2id[word]]\n\n    def get_vectors(self, *words):\n        word_ids = [self.word2id[i] for i in words]\n        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n        return vectors\n\n#########stepik-dl-nlp/dlnlputils/visualization.py#########\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\n\ndef plot_vectors(vectors, labels, how='tsne', ax=None, xy_lim=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n\n    x = projections[:, 0]\n    y = projections[:, 1]\n    if xy_lim is not None:\n        ax.set_xlim(xy_lim)\n        ax.set_ylim(xy_lim)\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))\n        \n\n#########stepik-dl-nlp/dlnlputils/data/pos.py#########\nimport torch\nfrom torch.utils.data import TensorDataset\n\ndef pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    #v1.2 add custom start/end tag functionaly\n    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    inputs[:,:,0] = char2id.get('<START>', 0)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            if token.form is not None:\n                for char_i, char in enumerate(token.form):\n                    inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)\n            else:\n                for char_i, char in enumerate('-'):\n                    inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)\n            inputs[sent_i, token_i, char_i + 2] = char2id.get('<END>', 0)\n\n    return inputs, targets\n\n\nclass POSTagger:\n    def __init__(self, model, char2id, id2label, max_sent_len, max_token_len):\n        self.model = model\n        self.char2id = char2id\n        self.id2label = id2label\n        self.max_sent_len = max_sent_len\n        self.max_token_len = max_token_len\n\n    def __call__(self, sentences):\n        tokenized_corpus = tokenize_corpus(sentences, min_token_size=1)\n\n        inputs = torch.zeros((len(sentences), self.max_sent_len, self.max_token_len + 2), dtype=torch.long)\n        inputs[:,:,0] = self.char2id.get('<START>', 0)\n\n        for sent_i, sentence in enumerate(tokenized_corpus):\n            for token_i, token in enumerate(sentence):\n                for char_i, char in enumerate(token):\n                    inputs[sent_i, token_i, char_i + 1] = self.char2id.get(char, 0)\n                inputs[sent_i, token_i, char_i + 2] = self.char2id.get('<END>', 0)\n\n        dataset = TensorDataset(inputs, torch.zeros(len(sentences)))\n        predicted_probs = predict_with_model(self.model, dataset)  # SentenceN x TagsN x MaxSentLen\n        predicted_classes = predicted_probs.argmax(1)\n\n        result = []\n        for sent_i, sent in enumerate(tokenized_corpus):\n            result.append([self.id2label[cls] for cls in predicted_classes[sent_i, :len(sent)]])\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:57:20.691581Z","iopub.execute_input":"2022-02-27T06:57:20.691965Z","iopub.status.idle":"2022-02-27T06:57:23.081594Z","shell.execute_reply.started":"2022-02-27T06:57:20.691932Z","shell.execute_reply":"2022-02-27T06:57:23.080872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n# import spacy\n# !python -m spacy download ru_core_news_md\n# spacy_nlp = spacy.load('ru_core_news_md', disable=['parser', 'ner'])\n\ndef tokenize_text_spacy_lemmatize(txt, spacy_nlp, min_token_size=4, with_pos = True, remove_stopwords = False):\n    doc = spacy_nlp(txt)\n    \n    if remove_stopwords:\n        lemmatized_doc = [token for token in doc if (len(token) >= min_token_size) and (not token.is_stop)]\n    else:\n        lemmatized_doc = [token for token in doc if len(token) >= min_token_size]\n    \n    if with_pos:\n        return ['_'.join([token.lemma_, token.pos_]) for token in lemmatized_doc]\n    else:\n        return [token.lemma_ for token in lemmatized_doc]\n\ndef tokenize_corpus_convert(tokenized_corpus, converter, addition = False):\n    '''\n    Convert each token in tokenized_corpus by converter\n    \n    Sample (PorterStemmer):\n    import nltk\n    ps = nltk.stemmer.PorterStemmer()\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=ps.stem)\n    \n    Sample (SnowballStemmer):\n    import nltk\n    sno = nltk.stem.SnowballStemmer('english')\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=sno.stem)\n    \n    Sample (WordNetLemmatizer):\n    import nltk\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    tokenized_lemmas_corpus = tokenize_corpus_convert(tokenized_corpus, converter=lemma.lemmatize)\n    '''\n    output = []\n    if not addition: #возвращаем только преобразованные токены\n        for doc in tokenized_corpus:\n            output.append([converter(token) for token in doc])\n    else: #возвращаем списк из исходных токенов, дополненных списком преобразованных\n        for doc in tokenized_corpus:\n            output.append(doc + [converter(token) for token in doc])        \n    return output\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), show_plots = True, only_BEST_MODEL_CALC = False):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        \n        if not only_BEST_MODEL_CALC:\n            epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n            print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.4f}, train = {:.4f},   LOSS: val  = {:.4f}, train = {:.4f}'\\\n                  .format(epoch_max_acc, \n                          histories[experiment_id]['acc']['val'][epoch_max_acc], \n                          histories[experiment_id]['acc']['train'][epoch_max_acc],\n                          histories[experiment_id]['loss']['val'][epoch_max_acc],\n                          histories[experiment_id]['loss']['train'][epoch_max_acc]))\n            epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n            print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.4f}, train = {:.4f},   LOSS: val  = {:.4f}, train = {:.4f}'\\\n                  .format(epoch_min_loss, \n                          histories[experiment_id]['acc']['val'][epoch_min_loss], \n                          histories[experiment_id]['acc']['train'][epoch_min_loss],\n                          histories[experiment_id]['loss']['val'][epoch_min_loss],\n                          histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if 'acc' in histories[experiment_id]['BEST']:\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.4f}, train = {:.4f},   LOSS: test = {:.4f}, train = {:.4f}  DICT SIZE = {}\"\\\n                  .format(histories[experiment_id]['BEST']['epoch'], \n                          histories[experiment_id]['BEST']['acc']['test'],\n                          histories[experiment_id]['BEST']['acc']['train'],\n                          histories[experiment_id]['BEST']['loss']['test'],\n                          histories[experiment_id]['BEST']['loss']['train'],\n                          histories[experiment_id]['BEST']['dict_size']))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n\ndef run_most_sumilars(func_most_similars, words_list, verbose = True, **kwargs):\n    most_similars = {word: func_most_similars(word, **kwargs) for word in words_list}\n    if verbose:\n        for word, similars in most_similars.items():\n            print('{}:'.format(word))\n            print('\\n'.join(map(str,similars)))\n            print(' ')\n    return most_similars\n        \n\n#https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/4529901\nimport pickle\ndef save_object(obj, filename):\n    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\ndef load_object(filename):\n    with open(filename, 'rb') as inp:\n        return pickle.load(inp)\n\n# sample usage\n#company1 = [1,2,3,4,5]\n#save_object(company1, '/kaggle/working/company1.pkl')\n#del company\n#company1 = load_object(filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:57:23.084154Z","iopub.execute_input":"2022-02-27T06:57:23.084572Z","iopub.status.idle":"2022-02-27T06:57:23.110246Z","shell.execute_reply.started":"2022-02-27T06:57:23.084535Z","shell.execute_reply":"2022-02-27T06:57:23.109577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories = {} #history of expeiments\nhdatasets = {} #history of datasets","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:57:23.11319Z","iopub.execute_input":"2022-02-27T06:57:23.113386Z","iopub.status.idle":"2022-02-27T06:57:23.121906Z","shell.execute_reply.started":"2022-02-27T06:57:23.113363Z","shell.execute_reply":"2022-02-27T06:57:23.121178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Свёрточные нейросети и POS-теггинг\n\nPOS-теггинг - определение частей речи (снятие частеречной неоднозначности)","metadata":{}},{"cell_type":"code","source":"!pip install pyconll\n# !pip install spacy_udpipe","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:42:57.976431Z","start_time":"2019-10-29T19:42:57.959538Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:23.124625Z","iopub.execute_input":"2022-02-27T06:57:23.124987Z","iopub.status.idle":"2022-02-27T06:57:32.348186Z","shell.execute_reply.started":"2022-02-27T06:57:23.124959Z","shell.execute_reply":"2022-02-27T06:57:32.347324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys; sys.path.append('./stepik-dl-nlp')\n\nfrom sklearn.metrics import classification_report, f1_score\n\nimport numpy as np\n\nimport pyconll\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\n\n# import dlnlputils\n# from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n#     character_tokenize, pos_corpus_to_tensor, POSTagger\n# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n\ninit_random_seed()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:34.549739Z","start_time":"2019-10-29T19:49:32.179692Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:32.349989Z","iopub.execute_input":"2022-02-27T06:57:32.350239Z","iopub.status.idle":"2022-02-27T06:57:32.413592Z","shell.execute_reply.started":"2022-02-27T06:57:32.350204Z","shell.execute_reply":"2022-02-27T06:57:32.412953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка текстов и разбиение на обучающую и тестовую подвыборки","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n!mkdir ./stepik-dl-nlp/\n!mkdir ./stepik-dl-nlp/datasets/\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:08.433599Z","start_time":"2019-10-29T19:46:05.110693Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:32.414948Z","iopub.execute_input":"2022-02-27T06:57:32.415185Z","iopub.status.idle":"2022-02-27T06:57:36.45743Z","shell.execute_reply.started":"2022-02-27T06:57:32.415153Z","shell.execute_reply":"2022-02-27T06:57:36.456481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nfull_train = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu')\nfull_test = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu')\nprint(len(full_train), len(full_test))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.525561Z","start_time":"2019-10-29T19:49:37.315213Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:36.460872Z","iopub.execute_input":"2022-02-27T06:57:36.461374Z","iopub.status.idle":"2022-02-27T06:57:51.23082Z","shell.execute_reply.started":"2022-02-27T06:57:36.461329Z","shell.execute_reply":"2022-02-27T06:57:51.228563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent in full_train[:2]:\n    for token in sent:\n        print(token.form, token.upos)\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.548127Z","start_time":"2019-10-29T19:49:56.527559Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:51.232193Z","iopub.execute_input":"2022-02-27T06:57:51.232575Z","iopub.status.idle":"2022-02-27T06:57:51.315625Z","shell.execute_reply.started":"2022-02-27T06:57:51.232531Z","shell.execute_reply":"2022-02-27T06:57:51.314863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SENT_LEN = max(len(sent) for sent in full_train)\nMAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\nprint('Наибольшая длина предложения', MAX_SENT_LEN)\nprint('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.916262Z","start_time":"2019-10-29T19:49:56.549806Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:51.320151Z","iopub.execute_input":"2022-02-27T06:57:51.320493Z","iopub.status.idle":"2022-02-27T06:57:51.751396Z","shell.execute_reply.started":"2022-02-27T06:57:51.320448Z","shell.execute_reply":"2022-02-27T06:57:51.748197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\nprint('\\n'.join(all_train_texts[:10]))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:57.251433Z","start_time":"2019-10-29T19:49:56.919818Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:51.757204Z","iopub.execute_input":"2022-02-27T06:57:51.75756Z","iopub.status.idle":"2022-02-27T06:57:52.166656Z","shell.execute_reply.started":"2022-02-27T06:57:51.757527Z","shell.execute_reply":"2022-02-27T06:57:52.165887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\nchar_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\nprint(\"Количество уникальных символов\", len(char_vocab))\nprint(list(char_vocab.items())[:10])","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:52.167879Z","iopub.execute_input":"2022-02-27T06:57:52.168397Z","iopub.status.idle":"2022-02-27T06:57:52.707983Z","shell.execute_reply.started":"2022-02-27T06:57:52.168356Z","shell.execute_reply":"2022-02-27T06:57:52.707163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\nlabel2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\nlabel2id","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.524125Z","start_time":"2019-10-29T19:49:58.125577Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:52.709326Z","iopub.execute_input":"2022-02-27T06:57:52.709824Z","iopub.status.idle":"2022-02-27T06:57:52.899077Z","shell.execute_reply.started":"2022-02-27T06:57:52.709781Z","shell.execute_reply":"2022-02-27T06:57:52.898355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2022-02-27T06:57:52.900431Z","iopub.execute_input":"2022-02-27T06:57:52.900668Z","iopub.status.idle":"2022-02-27T06:58:16.299592Z","shell.execute_reply.started":"2022-02-27T06:57:52.900634Z","shell.execute_reply":"2022-02-27T06:58:16.298828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SentCount x MAX_SENT_LEN x MAX_ORIG_TOKEN_LEN\ntrain_inputs.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.300979Z","iopub.execute_input":"2022-02-27T06:58:16.301229Z","iopub.status.idle":"2022-02-27T06:58:16.341474Z","shell.execute_reply.started":"2022-02-27T06:58:16.301198Z","shell.execute_reply":"2022-02-27T06:58:16.340778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs[1][:5]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.754883Z","start_time":"2019-10-29T19:49:40.582Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-02-27T06:58:16.342773Z","iopub.execute_input":"2022-02-27T06:58:16.343063Z","iopub.status.idle":"2022-02-27T06:58:16.389296Z","shell.execute_reply.started":"2022-02-27T06:58:16.343026Z","shell.execute_reply":"2022-02-27T06:58:16.388587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SentCount x MAX_SENT_LEN\ntrain_labels.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.390528Z","iopub.execute_input":"2022-02-27T06:58:16.390931Z","iopub.status.idle":"2022-02-27T06:58:16.427609Z","shell.execute_reply.started":"2022-02-27T06:58:16.390894Z","shell.execute_reply":"2022-02-27T06:58:16.42686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels[1]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.756496Z","start_time":"2019-10-29T19:49:40.711Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.42884Z","iopub.execute_input":"2022-02-27T06:58:16.429552Z","iopub.status.idle":"2022-02-27T06:58:16.469394Z","shell.execute_reply.started":"2022-02-27T06:58:16.429515Z","shell.execute_reply":"2022-02-27T06:58:16.468642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for calc final statistics\nhdatasets['base'] = {}\nhdatasets['base']['char_vocab'] = char_vocab\nhdatasets['base']['train_labels'] = train_labels\nhdatasets['base']['train_dataset'] = train_dataset\nhdatasets['base']['test_labels'] = test_labels\nhdatasets['base']['test_dataset'] = test_dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.470952Z","iopub.execute_input":"2022-02-27T06:58:16.47126Z","iopub.status.idle":"2022-02-27T06:58:16.507431Z","shell.execute_reply.started":"2022-02-27T06:58:16.471188Z","shell.execute_reply":"2022-02-27T06:58:16.506778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательная свёрточная архитектура","metadata":{}},{"cell_type":"code","source":"# FeaturesNum - изначально размер эмбеддингов (символов, либо слов), далее размерность сохраняется, \n# по сути это становится количеством входных каналов\n\nclass StackedConv1d(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for _ in range(layers_n):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.316516Z","start_time":"2019-10-29T19:46:17.539Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.508526Z","iopub.execute_input":"2022-02-27T06:58:16.509334Z","iopub.status.idle":"2022-02-27T06:58:16.546834Z","shell.execute_reply.started":"2022-02-27T06:58:16.509296Z","shell.execute_reply":"2022-02-27T06:58:16.546092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне отдельных токенов","metadata":{}},{"cell_type":"code","source":"class SingleTokenPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.backbone = StackedConv1d(embedding_size, **kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Linear(embedding_size, labels_num)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        \n        features = self.backbone(char_embeddings)\n        \n        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n        \n        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.317452Z","start_time":"2019-10-29T19:46:23.135Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.547972Z","iopub.execute_input":"2022-02-27T06:58:16.548641Z","iopub.status.idle":"2022-02-27T06:58:16.588828Z","shell.execute_reply.started":"2022-02-27T06:58:16.548596Z","shell.execute_reply":"2022-02-27T06:58:16.588066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline_no_context'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': nn.Conv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=hyps['emb_size'], layers_n=hyps['sng_layers_n'], \n#                                           kernel_size=hyps['sng_kernel_size'], dropout=hyps['sng_dropout'])\n# print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(single_token_model,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             F.cross_entropy,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=500,\n#                                             max_batches_per_epoch_val=300,\n#                                             experiment_name = exp_name,\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                        factor=hyps['lr_sched_fac'],\n#                                                                                                                        verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.31947Z","start_time":"2019-10-29T19:46:25.552Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.589946Z","iopub.execute_input":"2022-02-27T06:58:16.590447Z","iopub.status.idle":"2022-02-27T06:58:16.627225Z","shell.execute_reply.started":"2022-02-27T06:58:16.590404Z","shell.execute_reply":"2022-02-27T06:58:16.62648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# !mkdir ./stepik-dl-nlp/models/\n# torch.save(histories[exp_name]['best_model'].state_dict(), './stepik-dl-nlp/models/single_token_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.320568Z","start_time":"2019-10-29T19:46:47.579Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.628271Z","iopub.execute_input":"2022-02-27T06:58:16.628806Z","iopub.status.idle":"2022-02-27T06:58:16.663622Z","shell.execute_reply.started":"2022-02-27T06:58:16.628765Z","shell.execute_reply":"2022-02-27T06:58:16.66285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# histories[exp_name]['best_model'].load_state_dict(torch.load('./stepik-dl-nlp/models/single_token_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.664689Z","iopub.execute_input":"2022-02-27T06:58:16.665078Z","iopub.status.idle":"2022-02-27T06:58:16.70086Z","shell.execute_reply.started":"2022-02-27T06:58:16.665045Z","shell.execute_reply":"2022-02-27T06:58:16.700082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне предложений (с учётом контекста)","metadata":{}},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.701941Z","iopub.execute_input":"2022-02-27T06:58:16.702332Z","iopub.status.idle":"2022-02-27T06:58:16.742401Z","shell.execute_reply.started":"2022-02-27T06:58:16.702296Z","shell.execute_reply":"2022-02-27T06:58:16.741729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline_context'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': nn.Conv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': nn.Conv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=hyps['emb_size'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'], kernel_size=hyps['sng_kernel_size'], \n#                                                                           dropout=hyps['sng_dropout'], conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'], kernel_size=hyps['ctx_kernel_size'], \n#                                                                            dropout=hyps['ctx_dropout'], conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.743949Z","iopub.execute_input":"2022-02-27T06:58:16.744206Z","iopub.status.idle":"2022-02-27T06:58:16.78165Z","shell.execute_reply.started":"2022-02-27T06:58:16.744172Z","shell.execute_reply":"2022-02-27T06:58:16.78102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# torch.save(histories[exp_name]['best_model'].state_dict(), './stepik-dl-nlp/models/sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.52911Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.783056Z","iopub.execute_input":"2022-02-27T06:58:16.783616Z","iopub.status.idle":"2022-02-27T06:58:16.817734Z","shell.execute_reply.started":"2022-02-27T06:58:16.78356Z","shell.execute_reply":"2022-02-27T06:58:16.817019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# histories[exp_name]['best_model'].load_state_dict(torch.load('./stepik-dl-nlp/models/sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.564926Z","start_time":"2019-08-29T13:56:16.544481Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.819188Z","iopub.execute_input":"2022-02-27T06:58:16.819676Z","iopub.status.idle":"2022-02-27T06:58:16.85293Z","shell.execute_reply.started":"2022-02-27T06:58:16.819612Z","shell.execute_reply":"2022-02-27T06:58:16.852242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Свёрточный модуль своими руками","metadata":{}},{"cell_type":"code","source":"class MyConv1d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, dilation=1):\n        #dilation not released, add to parameters only for no error\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n                                   requires_grad=True)\n        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n\n        batch_size, src_channels, sequence_len = x.shape        \n        if self.padding > 0:\n            pad = x.new_zeros(batch_size, src_channels, self.padding)\n            x = torch.cat((pad, x, pad), dim=-1)\n            sequence_len = x.shape[-1]\n\n        chunks = []\n        chunk_size = sequence_len - self.kernel_size + 1\n        for offset in range(self.kernel_size):\n            chunks.append(x[:, :, offset:offset + chunk_size])\n\n        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n        return out_features","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.19314Z","start_time":"2019-08-29T13:56:42.170233Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:16.872991Z","iopub.execute_input":"2022-02-27T06:58:16.87329Z","iopub.status.idle":"2022-02-27T06:58:16.914575Z","shell.execute_reply.started":"2022-02-27T06:58:16.873261Z","shell.execute_reply":"2022-02-27T06:58:16.913926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline_MyConv1d'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': MyConv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': MyConv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=hyps['emb_size'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'], kernel_size=hyps['sng_kernel_size'], \n#                                                                           dropout=hyps['sng_dropout'], conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'], kernel_size=hyps['ctx_kernel_size'], \n#                                                                            dropout=hyps['ctx_dropout'], conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.917417Z","iopub.execute_input":"2022-02-27T06:58:16.917657Z","iopub.status.idle":"2022-02-27T06:58:16.954504Z","shell.execute_reply.started":"2022-02-27T06:58:16.917632Z","shell.execute_reply":"2022-02-27T06:58:16.953852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Финальное задание 3.7.Final","metadata":{}},{"cell_type":"raw","source":"Оригинальный текст:\nВ качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы лучше понять особенности свёрточных нейросетей и попробовать улучшить качество определения частей речи. Что можно попробовать сделать:\n    - поиграться с параметрами и архитектурой - количеством каналов (размерностью эмбеддинга), глубиной нейросети, силой Dropout, добавить BatchNorm или другую нормализацию\n    - подключить прореженные (dilated) свёртки, чтобы увеличить рецептивное поле без увеличения числа параметров\n    - добавить взвешивание классов\n    - использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)\n\nТакже мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы улучшить качество определения частей речи.\n\nОпишите то, что у Вас получилось, в ответе к этому шагу.\n\nБалл за этот шаг зачитывается автоматически, вне зависимости от текста, который Вы впишете в поле ответа :). Но то, насколько этот семинар и это задание будет полезным для Вас, вполне зависит - так что мы предлагаем Вам поисследовать возможности линейных моделей и подробно описать свой опыт. К тому же, после нажатия кнопки \"Отправить\" Вы получите доступ к ответам других участников и сможете обменяться своими находками.","metadata":{}},{"cell_type":"raw","source":"Итоговые пунткты для меня:\n\n    + детально разобраться в коде baseline, понять суть и смысл кода (последовательно, от более простой модели, к более сложной)\n    \n    + вынести все настройки в гиперпараметры, для нормального проведения экспериментов (выводов по сравнению и т.д.)\n    \n    + вручную проставить правильно метки в проверочных предложениях и подсвечивать ошибки, проводить дополнительное сравнение экспериментов на них\n    \n    + переписать код миниреснета на структуру без циклов\n    \n    + поиграться с параметрами и архитектурой - количеством каналов (размерностью эмбеддинга), глубиной нейросети, силой Dropout, добавить BatchNorm или другую нормализацию\n    \n    + подключить прореженные (dilated) свёртки, чтобы увеличить рецептивное поле без увеличения числа параметров\n    \n    + использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)\n    \n    - добавить взвешивание классов\n\nНа этом остановиться, этого будет вполне достаточно!","metadata":{}},{"cell_type":"raw","source":"Итоги экспериментов следующие:\n\n- сильных продвижений в сравнении с baseline нет (удалось улучшить только на 1,5%, это для эксперимента: в 70 эпох, количество слоев single_backbone = 12, эмбеддинг = 100, кастомные (обучаемые) теги начала и конца слов)\n\n- выставление эмбеддинга более 150 или увеличение кол-ва слоев > 12 (или темболее микс из этих параметров) не удается реализовать, в связи с нехваткой памяти (должно помочь снижение батча, но не проверял)\n\n- использование deliated сверток, не дало существенного улучшения/ухудшение результатов (все +- в пределах baseline), при этом обязательно нужно выставлять torch.backends.cudnn.benchmark=True и torch.backends.cudnn.deterministic=False (спасибо комментариям ниже), иначе скорость падает просто на порядки, а вот с ними, получается даже быстрее (причем обычные свертки тоже в этом случае работают быстрее в 2 раза)\n\n- взвешивание balanсed через sklearn.utils.class_weight.compute_class_weight и nn.CrossEntropyLoss(weight=class_weights, reduction='mean') ухудшило результат (т.к. дисбаланс очень серьезный и измеряется порядками), а вот дополнительное логарифмирование весов (ln(w+1)) или занижение вручную только веса 0 тега, вернула метрики к baseline (но не улучшило)\n\n- переписал StackedConv1d на вариант без циклов в forward, т.к. предполагал, что циклы могут негативно сказиваться на производительности/параллелизации, но в итоге к моему удивлению, это дало выигрыш максимум 1-3% по времени, так что циклы допустимы, хотя все же думаю их лучше при возможности избегать.","metadata":{}},{"cell_type":"raw","source":"baseline----------------------------\nf1_macro: Train = 0.937, Val = 0.915\n\nlr=1e-2-----------------------------\nf1_macro: Train = 0.941, Val = 0.917\n\nemb_size=150------------------------\nf1_macro: Train = 0.962, Val = 0.921\n\ncustom_start_end_tags_01------------\nf1_macro: Train = 0.952, Val = 0.926\n\n\n\nkernel_size=5-----------------------\nf1_macro: Train = 0.959, Val = 0.921\n\n\nsng_layers_n=12---------------------\nf1_macro: Train = 0.959, Val = 0.928\n\nlayers_n=12-------------------------\nf1_macro: Train = 0.968, Val = 0.922","metadata":{}},{"cell_type":"raw","source":"baseline----------------------------\nf1_macro: Train = 0.937, Val = 0.915\nbaseline----------------------------\nf1_macro: Train = 0.937, Val = 0.915\n\nlr=1e-2-----------------------------\nf1_macro: Train = 0.941, Val = 0.917\nemb_size=100------------------------\nf1_macro: Train = 0.957, Val = 0.917\nemb_size=150------------------------\nf1_macro: Train = 0.962, Val = 0.921\ndropout=0.1-------------------------\nf1_macro: Train = 0.968, Val = 0.915\nnn.BatchNorm1d----------------------\nf1_macro: Train = 0.942, Val = 0.912\nnn.InstanceNorm1d-------------------\nf1_macro: Train = 0.915, Val = 0.894\nsng_kernel_size=5-------------------\nf1_macro: Train = 0.962, Val = 0.918\nctx_kernel_size=5-------------------\nf1_macro: Train = 0.949, Val = 0.918\nkernel_size=5-----------------------\nf1_macro: Train = 0.959, Val = 0.921\nbatch=128---------------------------\nf1_macro: Train = 0.944, Val = 0.917\nbatch=256---------------------------\nf1_macro: Train = 0.929, Val = 0.902\nsng_layers_n=6----------------------\nf1_macro: Train = 0.955, Val = 0.923\nsng_layers_n=12---------------------\nf1_macro: Train = 0.959, Val = 0.928\nctx_layers_n=6----------------------\nf1_macro: Train = 0.945, Val = 0.917\nctx_layers_n=12---------------------\nf1_macro: Train = 0.948, Val = 0.924\nlayers_n=6--------------------------\nf1_macro: Train = 0.953, Val = 0.921\nlayers_n=12-------------------------\nf1_macro: Train = 0.968, Val = 0.922\n\nctx_deliated------------------------\nf1_macro: Train = 0.941, Val = 0.916\nctx_deliated_ctx_layers_n=6---------\nf1_macro: Train = 0.946, Val = 0.917\nsng_deliated------------------------\nf1_macro: Train = 0.947, Val = 0.918\nall_deliated------------------------\nf1_macro: Train = 0.940, Val = 0.916\nall_deliated_layers_n=6-------------\nf1_macro: Train = 0.937, Val = 0.912\n\ncustom_start_end_tags_01------------\nf1_macro: Train = 0.952, Val = 0.926\ncustom_start_end_tags_02------------\nf1_macro: Train = 0.949, Val = 0.918\n\nclass_weights_01--------------------\nf1_macro: Train = 0.909, Val = 0.878\nclass_weights_02--------------------\nf1_macro: Train = 0.899, Val = 0.880\nclass_weights_log_01----------------\nf1_macro: Train = 0.959, Val = 0.917\nclass_weights_manual_01-------------\nf1_macro: Train = 0.940, Val = 0.916\n\ncustom_init_embeddings_weights------\nf1_macro: Train = 0.932, Val = 0.917\n\n\nFinal experiments (mix of best):\nlr=1e-2_emb_size=100_start_end_tags_sng_layers_n=12\nf1_macro: Train = 0.975, Val = 0.930\nlr=1e-2_emb_size=100_start_end_tags_all_layers_n=12\nf1_macro: Train = 0.971, Val = 0.929","metadata":{}},{"cell_type":"raw","source":"baseline----------------------------\nACC: 0.7826, correct = 36 of 46\nbaseline----------------------------\nACC: 0.7826, correct = 36 of 46\nlr=1e-2-----------------------------\nACC: 0.8043, correct = 37 of 46\nemb_size=100------------------------\nACC: 0.8913, correct = 41 of 46\nemb_size=150------------------------\nACC: 0.9130, correct = 42 of 46\ndropout=0.1-------------------------\nACC: 0.7609, correct = 35 of 46\nnn.BatchNorm1d----------------------\nACC: 0.8043, correct = 37 of 46\nnn.InstanceNorm1d-------------------\nACC: 0.8261, correct = 38 of 46\nsng_kernel_size=5-------------------\nACC: 0.8043, correct = 37 of 46\nctx_kernel_size=5-------------------\nACC: 0.8478, correct = 39 of 46\nkernel_size=5-----------------------\nACC: 0.8478, correct = 39 of 46\nbatch=128---------------------------\nACC: 0.7826, correct = 36 of 46\nbatch=256---------------------------\nACC: 0.8261, correct = 38 of 46\nsng_layers_n=6----------------------\nACC: 0.8478, correct = 39 of 46\nsng_layers_n=12---------------------\nACC: 0.8913, correct = 41 of 46\nctx_layers_n=6----------------------\nACC: 0.8696, correct = 40 of 46\nctx_layers_n=12---------------------\nACC: 0.8478, correct = 39 of 46\nlayers_n=6--------------------------\nACC: 0.8696, correct = 40 of 46\nlayers_n=12-------------------------\nACC: 0.8261, correct = 38 of 46\n\nctx_deliated------------------------\nACC: 0.8043, correct = 37 of 46\nctx_deliated_ctx_layers_n=6---------\nACC: 0.8261, correct = 38 of 46\nsng_deliated------------------------\nACC: 0.8478, correct = 39 of 46\nall_deliated------------------------\nACC: 0.8913, correct = 41 of 46\nall_deliated_layers_n=6-------------\nACC: 0.8913, correct = 41 of 46\n\ncustom_start_end_tags_01------------\nACC: 0.8043, correct = 37 of 46\ncustom_start_end_tags_02------------\nACC: 0.8478, correct = 39 of 46\n\nclass_weights_01--------------------\nACC: 0.8696, correct = 40 of 46\nclass_weights_02--------------------\nACC: 0.8043, correct = 37 of 46\nclass_weights_log_01----------------\nACC: 0.8478, correct = 39 of 46\nclass_weights_manual_01-------------\nACC: 0.8261, correct = 38 of 46\n\ncustom_init_embeddings_weights------\nACC: 0.8913, correct = 41 of 46\n\n\nFinal experiments (mix of best):\nlr=1e-2_emb_size=100_start_end_tags_sng_layers_n=12\nACC: 0.8913, correct = 41 of 46\nlr=1e-2_emb_size=100_start_end_tags_all_layers_n=12\nACC: 0.8043, correct = 37 of 46","metadata":{}},{"cell_type":"markdown","source":"### Эксперимент - переписать код миниреснета на структуру без циклов","metadata":{}},{"cell_type":"code","source":"class wisResidualBlock(torch.nn.Module):\n    #v1.0\n    def __init__(self, features_num, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0, batchnorm = None):\n        #batchnorm = None, nn.InstanceNorm1d, nn.BatchNorm1d\n        \n        super().__init__()\n        if batchnorm is None:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU())\n        else:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                batchnorm(features_num),\n                nn.Dropout(dropout),\n                nn.LeakyReLU())\n            \n    def forward(self, x):        \n        return x + self.layer(x)\n\nclass wisResidualBlock_act_out(torch.nn.Module):\n    #v1.0\n    def __init__(self, features_num, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0, batchnorm = None):\n        #batchnorm = None, nn.InstanceNorm1d, nn.BatchNorm1d\n        \n        super().__init__()\n        if batchnorm is None:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout))\n            self.act = nn.LeakyReLU()\n        else:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                batchnorm(features_num),\n                nn.Dropout(dropout),\n                nn.LeakyReLU())\n            \n    def forward(self, x):\n        return self.act(x + self.layer(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.955979Z","iopub.execute_input":"2022-02-27T06:58:16.956266Z","iopub.status.idle":"2022-02-27T06:58:16.997422Z","shell.execute_reply.started":"2022-02-27T06:58:16.95623Z","shell.execute_reply":"2022-02-27T06:58:16.996752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class wisStackedConv1d(nn.Module):\n    #v1.0\n    def __init__(self, features_num, layers_n=1, res_block = wisResidualBlock, \n                 kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0, batchnorm = None):\n        super().__init__()\n        self.layers = nn.Sequential(*[res_block(features_num, \n                                                kernel_size=kernel_size, \n                                                conv_layer=conv_layer,\n                                                dropout=dropout,\n                                                batchnorm=batchnorm) for _ in range(layers_n)])\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:16.999688Z","iopub.execute_input":"2022-02-27T06:58:17.000941Z","iopub.status.idle":"2022-02-27T06:58:17.037232Z","shell.execute_reply.started":"2022-02-27T06:58:17.000904Z","shell.execute_reply":"2022-02-27T06:58:17.03635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    #v1.0\n    def __init__(self, vocab_size, labels_num, embedding_size=32, res_block = wisResidualBlock, \n                 single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = wisStackedConv1d(embedding_size, res_block=res_block, **single_backbone_kwargs)\n        self.context_backbone = wisStackedConv1d(embedding_size, res_block=res_block, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.038762Z","iopub.execute_input":"2022-02-27T06:58:17.039414Z","iopub.status.idle":"2022-02-27T06:58:17.078972Z","shell.execute_reply.started":"2022-02-27T06:58:17.039355Z","shell.execute_reply":"2022-02-27T06:58:17.078227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'wisResidualBlock_MyConv1d_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.BatchNorm1d },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.BatchNorm1d },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 1,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.082026Z","iopub.execute_input":"2022-02-27T06:58:17.082454Z","iopub.status.idle":"2022-02-27T06:58:17.119107Z","shell.execute_reply.started":"2022-02-27T06:58:17.082425Z","shell.execute_reply":"2022-02-27T06:58:17.118448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'wisResidualBlock_MyConv1d_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.InstanceNorm1d },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.InstanceNorm1d },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 1,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'loss',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.120617Z","iopub.execute_input":"2022-02-27T06:58:17.120911Z","iopub.status.idle":"2022-02-27T06:58:17.157967Z","shell.execute_reply.started":"2022-02-27T06:58:17.120878Z","shell.execute_reply":"2022-02-27T06:58:17.157247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'wisResidualBlock_MyConv1d_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': MyConv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': MyConv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 40,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'],\n#                                                                           kernel_size=hyps['sng_kernel_size'],\n#                                                                           dropout=hyps['sng_dropout'],\n#                                                                           conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'],\n#                                                                            kernel_size=hyps['ctx_kernel_size'],\n#                                                                            dropout=hyps['ctx_dropout'],\n#                                                                            conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'loss',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.160912Z","iopub.execute_input":"2022-02-27T06:58:17.162794Z","iopub.status.idle":"2022-02-27T06:58:17.198279Z","shell.execute_reply.started":"2022-02-27T06:58:17.162763Z","shell.execute_reply":"2022-02-27T06:58:17.197591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'wisResidualBlock_MyConv1d_02'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': MyConv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': MyConv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'],\n#                                                                           kernel_size=hyps['sng_kernel_size'],\n#                                                                           dropout=hyps['sng_dropout'],\n#                                                                           conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'],\n#                                                                            kernel_size=hyps['ctx_kernel_size'],\n#                                                                            dropout=hyps['ctx_dropout'],\n#                                                                            conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'loss',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.200876Z","iopub.execute_input":"2022-02-27T06:58:17.201278Z","iopub.status.idle":"2022-02-27T06:58:17.236506Z","shell.execute_reply.started":"2022-02-27T06:58:17.201242Z","shell.execute_reply":"2022-02-27T06:58:17.235618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline_wisResidualBlock_act_out_MyConv1d_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock_act_out,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': MyConv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': MyConv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 40,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'],\n#                                                                           kernel_size=hyps['sng_kernel_size'],\n#                                                                           dropout=hyps['sng_dropout'],\n#                                                                           conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'],\n#                                                                            kernel_size=hyps['ctx_kernel_size'],\n#                                                                            dropout=hyps['ctx_dropout'],\n#                                                                            conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'loss',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.237939Z","iopub.execute_input":"2022-02-27T06:58:17.238266Z","iopub.status.idle":"2022-02-27T06:58:17.274567Z","shell.execute_reply.started":"2022-02-27T06:58:17.23823Z","shell.execute_reply":"2022-02-27T06:58:17.273865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline_wisResidualBlock_act_out_MyConv1d_02'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock_act_out,\n#     'sng_layers_n': 3, \n#     'sng_kernel_size': 3, \n#     'sng_dropout': 0.3,\n#     'sng_conv_layer': MyConv1d,\n#     'ctx_layers_n': 3, \n#     'ctx_kernel_size': 3, \n#     'ctx_dropout': 0.3,\n#     'ctx_conv_layer': MyConv1d,\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=dict(layers_n=hyps['sng_layers_n'],\n#                                                                           kernel_size=hyps['sng_kernel_size'],\n#                                                                           dropout=hyps['sng_dropout'],\n#                                                                           conv_layer=hyps['sng_conv_layer']),\n#                                               context_backbone_kwargs=dict(layers_n=hyps['ctx_layers_n'],\n#                                                                            kernel_size=hyps['ctx_kernel_size'],\n#                                                                            dropout=hyps['ctx_dropout'],\n#                                                                            conv_layer=hyps['ctx_conv_layer']))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'loss',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.277542Z","iopub.execute_input":"2022-02-27T06:58:17.27783Z","iopub.status.idle":"2022-02-27T06:58:17.313346Z","shell.execute_reply.started":"2022-02-27T06:58:17.27779Z","shell.execute_reply":"2022-02-27T06:58:17.312672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - поиграться с параметрами и архитектурой - количеством каналов (размерностью эмбеддинга), глубиной нейросети, силой Dropout, добавить BatchNorm или другую нормализацию","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'baseline'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.316451Z","iopub.execute_input":"2022-02-27T06:58:17.316659Z","iopub.status.idle":"2022-02-27T06:58:17.351693Z","shell.execute_reply.started":"2022-02-27T06:58:17.316621Z","shell.execute_reply":"2022-02-27T06:58:17.35098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lr=1e-2'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 1e-2,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.35441Z","iopub.execute_input":"2022-02-27T06:58:17.354625Z","iopub.status.idle":"2022-02-27T06:58:17.390093Z","shell.execute_reply.started":"2022-02-27T06:58:17.354601Z","shell.execute_reply":"2022-02-27T06:58:17.389393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'emb_size=100'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 100,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.392896Z","iopub.execute_input":"2022-02-27T06:58:17.39311Z","iopub.status.idle":"2022-02-27T06:58:17.432079Z","shell.execute_reply.started":"2022-02-27T06:58:17.393086Z","shell.execute_reply":"2022-02-27T06:58:17.431347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'emb_size=150'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 150,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.434044Z","iopub.execute_input":"2022-02-27T06:58:17.43474Z","iopub.status.idle":"2022-02-27T06:58:17.473372Z","shell.execute_reply.started":"2022-02-27T06:58:17.434697Z","shell.execute_reply":"2022-02-27T06:58:17.472624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'dropout=0.1'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.1,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.1,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.476714Z","iopub.execute_input":"2022-02-27T06:58:17.477103Z","iopub.status.idle":"2022-02-27T06:58:17.51663Z","shell.execute_reply.started":"2022-02-27T06:58:17.477072Z","shell.execute_reply":"2022-02-27T06:58:17.515796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'nn.BatchNorm1d'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.BatchNorm1d },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.BatchNorm1d },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.519482Z","iopub.execute_input":"2022-02-27T06:58:17.520033Z","iopub.status.idle":"2022-02-27T06:58:17.558354Z","shell.execute_reply.started":"2022-02-27T06:58:17.519992Z","shell.execute_reply":"2022-02-27T06:58:17.557346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'nn.InstanceNorm1d'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.InstanceNorm1d },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': nn.InstanceNorm1d },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.559926Z","iopub.execute_input":"2022-02-27T06:58:17.560248Z","iopub.status.idle":"2022-02-27T06:58:17.60104Z","shell.execute_reply.started":"2022-02-27T06:58:17.560209Z","shell.execute_reply":"2022-02-27T06:58:17.600095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'sng_kernel_size=5'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 5, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.602806Z","iopub.execute_input":"2022-02-27T06:58:17.603138Z","iopub.status.idle":"2022-02-27T06:58:17.642851Z","shell.execute_reply.started":"2022-02-27T06:58:17.603098Z","shell.execute_reply":"2022-02-27T06:58:17.642088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'ctx_kernel_size=5'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 5, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.646022Z","iopub.execute_input":"2022-02-27T06:58:17.64629Z","iopub.status.idle":"2022-02-27T06:58:17.686125Z","shell.execute_reply.started":"2022-02-27T06:58:17.646264Z","shell.execute_reply":"2022-02-27T06:58:17.685399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'kernel_size=5'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 5, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 5, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.687794Z","iopub.execute_input":"2022-02-27T06:58:17.68824Z","iopub.status.idle":"2022-02-27T06:58:17.724511Z","shell.execute_reply.started":"2022-02-27T06:58:17.688182Z","shell.execute_reply":"2022-02-27T06:58:17.723841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'batch=128'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 128,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.727311Z","iopub.execute_input":"2022-02-27T06:58:17.727996Z","iopub.status.idle":"2022-02-27T06:58:17.763338Z","shell.execute_reply.started":"2022-02-27T06:58:17.727958Z","shell.execute_reply":"2022-02-27T06:58:17.762598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'batch=256'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 256,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.764506Z","iopub.execute_input":"2022-02-27T06:58:17.764902Z","iopub.status.idle":"2022-02-27T06:58:17.802209Z","shell.execute_reply.started":"2022-02-27T06:58:17.764869Z","shell.execute_reply":"2022-02-27T06:58:17.801557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'sng_layers_n=6'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 6, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.803505Z","iopub.execute_input":"2022-02-27T06:58:17.803918Z","iopub.status.idle":"2022-02-27T06:58:17.839538Z","shell.execute_reply.started":"2022-02-27T06:58:17.803883Z","shell.execute_reply":"2022-02-27T06:58:17.838857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'sng_layers_n=12'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 16, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.840736Z","iopub.execute_input":"2022-02-27T06:58:17.841007Z","iopub.status.idle":"2022-02-27T06:58:17.877104Z","shell.execute_reply.started":"2022-02-27T06:58:17.84097Z","shell.execute_reply":"2022-02-27T06:58:17.876203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'ctx_layers_n=6'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 6, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.878431Z","iopub.execute_input":"2022-02-27T06:58:17.878879Z","iopub.status.idle":"2022-02-27T06:58:17.915542Z","shell.execute_reply.started":"2022-02-27T06:58:17.878842Z","shell.execute_reply":"2022-02-27T06:58:17.914856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'ctx_layers_n=12'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 12, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.918496Z","iopub.execute_input":"2022-02-27T06:58:17.91872Z","iopub.status.idle":"2022-02-27T06:58:17.957266Z","shell.execute_reply.started":"2022-02-27T06:58:17.918693Z","shell.execute_reply":"2022-02-27T06:58:17.956344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'layers_n=6'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 6,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 6,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.960334Z","iopub.execute_input":"2022-02-27T06:58:17.960743Z","iopub.status.idle":"2022-02-27T06:58:17.999706Z","shell.execute_reply.started":"2022-02-27T06:58:17.960707Z","shell.execute_reply":"2022-02-27T06:58:17.998709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'layers_n=12'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 12,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 12,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.001052Z","iopub.execute_input":"2022-02-27T06:58:18.00166Z","iopub.status.idle":"2022-02-27T06:58:18.040843Z","shell.execute_reply.started":"2022-02-27T06:58:18.001621Z","shell.execute_reply":"2022-02-27T06:58:18.040052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - подключить прореженные (dilated) свёртки, чтобы увеличить рецептивное поле без увеличения числа параметров","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class wisResidualBlock(torch.nn.Module):\n    #v1.1 with dilation\n    def __init__(self, features_num, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0, batchnorm = None, dilation=1):\n        #batchnorm = None, nn.InstanceNorm1d, nn.BatchNorm1d\n        \n        super().__init__()\n        if batchnorm is None:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=(kernel_size//2)*dilation, dilation=dilation),\n                nn.Dropout(dropout),\n                nn.LeakyReLU())\n        else:\n            self.layer = nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=(kernel_size//2)*dilation, dilation=dilation),\n                batchnorm(features_num),\n                nn.Dropout(dropout),\n                nn.LeakyReLU())\n            \n    def forward(self, x):        \n        return x + self.layer(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.042432Z","iopub.execute_input":"2022-02-27T06:58:18.042784Z","iopub.status.idle":"2022-02-27T06:58:18.084606Z","shell.execute_reply.started":"2022-02-27T06:58:18.042747Z","shell.execute_reply":"2022-02-27T06:58:18.083837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class wisStackedConv1d(nn.Module):\n    #v1.1 with dilation\n    def __init__(self, features_num, layers_n=1, res_block = wisResidualBlock, \n                 kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0, batchnorm = None, dilation=False):\n        super().__init__()\n        if not dilation:\n            self.layers = nn.Sequential(*[res_block(features_num, \n                                                    kernel_size=kernel_size, \n                                                    conv_layer=conv_layer,\n                                                    dropout=dropout,\n                                                    batchnorm=batchnorm) for _ in range(layers_n)])\n        else:\n            self.layers = nn.Sequential(*[res_block(features_num, \n                                                    kernel_size=kernel_size, \n                                                    conv_layer=conv_layer,\n                                                    dropout=dropout,\n                                                    batchnorm=batchnorm,\n                                                    dilation=2**i) for i in range(layers_n)])\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.087548Z","iopub.execute_input":"2022-02-27T06:58:18.088091Z","iopub.status.idle":"2022-02-27T06:58:18.128348Z","shell.execute_reply.started":"2022-02-27T06:58:18.088061Z","shell.execute_reply":"2022-02-27T06:58:18.127474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'ctx_deliated'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True  },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# #Dilated conv is too slow - https://github.com/pytorch/pytorch/issues/15054\n# torch.backends.cudnn.benchmark=True\n# torch.backends.cudnn.deterministic=False\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.129737Z","iopub.execute_input":"2022-02-27T06:58:18.130192Z","iopub.status.idle":"2022-02-27T06:58:18.169135Z","shell.execute_reply.started":"2022-02-27T06:58:18.130153Z","shell.execute_reply":"2022-02-27T06:58:18.168295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'ctx_deliated_ctx_layers_n=6'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 6,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True  },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# #Dilated conv is too slow - https://github.com/pytorch/pytorch/issues/15054\n# torch.backends.cudnn.benchmark=True\n# torch.backends.cudnn.deterministic=False\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.172143Z","iopub.execute_input":"2022-02-27T06:58:18.17251Z","iopub.status.idle":"2022-02-27T06:58:18.210489Z","shell.execute_reply.started":"2022-02-27T06:58:18.17248Z","shell.execute_reply":"2022-02-27T06:58:18.209677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'sng_deliated'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# #Dilated conv is too slow - https://github.com/pytorch/pytorch/issues/15054\n# torch.backends.cudnn.benchmark=True\n# torch.backends.cudnn.deterministic=False\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.212573Z","iopub.execute_input":"2022-02-27T06:58:18.213283Z","iopub.status.idle":"2022-02-27T06:58:18.251247Z","shell.execute_reply.started":"2022-02-27T06:58:18.213242Z","shell.execute_reply":"2022-02-27T06:58:18.250574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'all_deliated'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True  },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# #Dilated conv is too slow - https://github.com/pytorch/pytorch/issues/15054\n# torch.backends.cudnn.benchmark=True\n# torch.backends.cudnn.deterministic=False\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.253595Z","iopub.execute_input":"2022-02-27T06:58:18.254256Z","iopub.status.idle":"2022-02-27T06:58:18.291538Z","shell.execute_reply.started":"2022-02-27T06:58:18.254217Z","shell.execute_reply":"2022-02-27T06:58:18.290859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'all_deliated_layers_n=6'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 6, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 6,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': nn.Conv1d,\n#         'batchnorm': None,\n#         'dilation': True  },\n\n#     #train_eval_loop\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# #Dilated conv is too slow - https://github.com/pytorch/pytorch/issues/15054\n# torch.backends.cudnn.benchmark=True\n# torch.backends.cudnn.deterministic=False\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               F.cross_entropy,\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.294513Z","iopub.execute_input":"2022-02-27T06:58:18.294897Z","iopub.status.idle":"2022-02-27T06:58:18.332742Z","shell.execute_reply.started":"2022-02-27T06:58:18.294866Z","shell.execute_reply":"2022-02-27T06:58:18.332071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)","metadata":{}},{"cell_type":"code","source":"","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'custom_start_end_tags_01'\n# hyps = {\n#     'dataset': 'setags',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': True,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3ф,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': F.cross_entropy,\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.335766Z","iopub.execute_input":"2022-02-27T06:58:18.335993Z","iopub.status.idle":"2022-02-27T06:58:18.375239Z","shell.execute_reply.started":"2022-02-27T06:58:18.335968Z","shell.execute_reply":"2022-02-27T06:58:18.374494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'custom_start_end_tags_02'\n# hyps = {\n#     'dataset': 'setags',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': True,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': F.cross_entropy,\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.376739Z","iopub.execute_input":"2022-02-27T06:58:18.377051Z","iopub.status.idle":"2022-02-27T06:58:18.416204Z","shell.execute_reply.started":"2022-02-27T06:58:18.377014Z","shell.execute_reply":"2022-02-27T06:58:18.415358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - добавить взвешивание классов","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.utils import class_weight\n# class_weights = class_weight.compute_class_weight('balanced',np.unique(train_labels.data),\n#                                                 train_labels.data.detach().cpu().numpy().reshape(-1))\n# class_weights = torch.tensor(class_weights, dtype=torch.float).cuda()\n# class_weights\n\n# class_weights_log = torch.log(class_weights+1)\n# class_weights_log","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.417827Z","iopub.execute_input":"2022-02-27T06:58:18.418187Z","iopub.status.idle":"2022-02-27T06:58:18.452203Z","shell.execute_reply.started":"2022-02-27T06:58:18.418151Z","shell.execute_reply":"2022-02-27T06:58:18.451502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'class_weights_log_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': False,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': nn.CrossEntropyLoss(weight=class_weights_log,reduction='mean'),\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.453557Z","iopub.execute_input":"2022-02-27T06:58:18.453923Z","iopub.status.idle":"2022-02-27T06:58:18.491556Z","shell.execute_reply.started":"2022-02-27T06:58:18.453887Z","shell.execute_reply":"2022-02-27T06:58:18.490743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights_manual = torch.tensor(np.ones(len(np.unique(train_labels.data))), dtype=torch.float).cuda()\n# class_weights_manual[0] = 0.0001\n# class_weights_manual","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.493658Z","iopub.execute_input":"2022-02-27T06:58:18.493876Z","iopub.status.idle":"2022-02-27T06:58:18.52782Z","shell.execute_reply.started":"2022-02-27T06:58:18.493841Z","shell.execute_reply":"2022-02-27T06:58:18.527128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'class_weights_manual_01'\n# hyps = {\n#     'dataset': 'base',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': False,\n    \n#     #TokenPOSTagger\n#     'emb_size': 64,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 3, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': nn.CrossEntropyLoss(weight=class_weights_manual,reduction='mean'),\n#     'lr': 5e-3,\n#     'epoch_n': 30,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:18.529382Z","iopub.execute_input":"2022-02-27T06:58:18.529802Z","iopub.status.idle":"2022-02-27T06:58:18.567273Z","shell.execute_reply.started":"2022-02-27T06:58:18.529739Z","shell.execute_reply":"2022-02-27T06:58:18.566619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Итоговые эксперименты","metadata":{}},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lr=1e-2_emb_size=100_start_end_tags_sng_layers_n=12'\n# hyps = {\n#     'dataset': 'setags',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': True,\n    \n#     #TokenPOSTagger\n#     'emb_size': 100,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 12, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': F.cross_entropy,\n#     'lr': 1e-2,\n#     'epoch_n': 70,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.570151Z","iopub.execute_input":"2022-02-27T06:58:18.570336Z","iopub.status.idle":"2022-02-27T06:58:18.606931Z","shell.execute_reply.started":"2022-02-27T06:58:18.570314Z","shell.execute_reply":"2022-02-27T06:58:18.606311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lr=1e-2_emb_size=100_start_end_tags_all_layers_n=12'\n# hyps = {\n#     'dataset': 'setags',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': True,\n    \n#     #TokenPOSTagger\n#     'emb_size': 100,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 12, \n#         'kernel_size': 3, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 12,\n#         'kernel_size': 3,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': F.cross_entropy,\n#     'lr': 1e-2,\n#     'epoch_n': 60,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.608138Z","iopub.execute_input":"2022-02-27T06:58:18.608405Z","iopub.status.idle":"2022-02-27T06:58:18.645458Z","shell.execute_reply.started":"2022-02-27T06:58:18.60837Z","shell.execute_reply":"2022-02-27T06:58:18.644837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lr=1e-2_emb_size=100_start_end_tags_sng_layers_n=12_kernel_size=5'\n# hyps = {\n#     'dataset': 'setags',\n#     'char_min_count': 5,\n#     'char_max_doc_freq': 1.0, \n#     'MAX_SENT_LEN': MAX_SENT_LEN,\n#     'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n#     'start_end_tag': True,\n    \n#     #TokenPOSTagger\n#     'emb_size': 100,\n#     'res_block': wisResidualBlock,\n#     'sng_backbone_kwargs': {\n#         'layers_n': 12, \n#         'kernel_size': 5, \n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False },\n#     'ctx_backbone_kwargs': {\n#         'layers_n': 3,\n#         'kernel_size': 5,\n#         'dropout': 0.3,\n#         'conv_layer': MyConv1d,\n#         'batchnorm': None,\n#         'dilation': False  },\n\n#     #train_eval_loop\n#     'loss_func': F.cross_entropy,\n#     'lr': 1e-2,\n#     'epoch_n': 40,\n#     'batch': 64,\n#     'device': 'cuda',\n#     'stop_pat': 5,\n#     'lr_sched_pat': 2,\n#     'lr_sched_fac': 0.5\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n# char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n#                                              max_doc_freq=hyps['char_max_doc_freq'], \n#                                              min_count=hyps['char_min_count'], \n#                                              pad_word='<PAD>', \n#                                              start_end_tag=hyps['start_end_tag'])\n\n# print(\"Количество уникальных символов\", len(char_vocab))\n# print(list(char_vocab.items())[:10])\n\n\n# UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n# label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n\n# train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# train_dataset = TensorDataset(train_inputs, train_labels)\n\n# test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n# test_dataset = TensorDataset(test_inputs, test_labels)\n\n# if hyps['start_end_tag']:\n#     #for calc final statistics\n#     hdatasets['setags'] = {}\n#     hdatasets['setags']['char_vocab'] = char_vocab\n#     hdatasets['setags']['train_labels'] = train_labels\n#     hdatasets['setags']['train_dataset'] = train_dataset\n#     hdatasets['setags']['test_labels'] = test_labels\n#     hdatasets['setags']['test_dataset'] = test_dataset\n\n# sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n#                                               embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n#                                               single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n#                                               context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n# (histories[exp_name]['train_history'],\n#  histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               hyps['loss_func'],\n#                                               lr=hyps['lr'],\n#                                               epoch_n=hyps['epoch_n'],\n#                                               batch_size=hyps['batch'],\n#                                               device=hyps['device'],\n#                                               early_stopping_patience=hyps['stop_pat'],\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=300,\n#                                               best_acc_type = 'acc',\n#                                               experiment_name = exp_name,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n#                                                                                                                          factor=hyps['lr_sched_fac'],\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:18.648338Z","iopub.execute_input":"2022-02-27T06:58:18.648701Z","iopub.status.idle":"2022-02-27T06:58:53.006258Z","shell.execute_reply.started":"2022-02-27T06:58:18.648675Z","shell.execute_reply":"2022-02-27T06:58:53.005483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Еще один эксперимент, иная инициализация весов эмбеддингов","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    #v1.1 Custom init embeddings\n    def __init__(self, vocab_size, labels_num, embedding_size=32, res_block = wisResidualBlock, \n                 single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.char_embeddings.weight.data.uniform_(-1.0 / embedding_size, 1.0 / embedding_size)\n        self.char_embeddings.weight.data[0] = 0\n        \n        self.single_token_backbone = wisStackedConv1d(embedding_size, res_block=res_block, **single_backbone_kwargs)\n        self.context_backbone = wisStackedConv1d(embedding_size, res_block=res_block, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.038762Z","iopub.execute_input":"2022-02-27T06:58:17.039414Z","iopub.status.idle":"2022-02-27T06:58:17.078972Z","shell.execute_reply.started":"2022-02-27T06:58:17.039355Z","shell.execute_reply":"2022-02-27T06:58:17.078227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hyperparameters\nexp_name = 'custom_init_embeddings_weights'\nhyps = {\n    'dataset': 'base',\n    'char_min_count': 5,\n    'char_max_doc_freq': 1.0, \n    'MAX_SENT_LEN': MAX_SENT_LEN,\n    'MAX_ORIG_TOKEN_LEN': MAX_ORIG_TOKEN_LEN,\n    \n    #TokenPOSTagger\n    'emb_size': 64,\n    'res_block': wisResidualBlock,\n    'sng_backbone_kwargs': {\n        'layers_n': 3, \n        'kernel_size': 3, \n        'dropout': 0.3,\n        'conv_layer': MyConv1d,\n        'batchnorm': None },\n    'ctx_backbone_kwargs': {\n        'layers_n': 3, \n        'kernel_size': 3, \n        'dropout': 0.3,\n        'conv_layer': MyConv1d,\n        'batchnorm': None },\n\n    #train_eval_loop\n    'lr': 5e-3,\n    'epoch_n': 30,\n    'batch': 64,\n    'device': 'cuda',\n    'stop_pat': 5,\n    'lr_sched_pat': 2,\n    'lr_sched_fac': 0.5\n}\n\nhistories[exp_name] = {\n    'hyps': hyps\n}\n\n\nsentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), \n                                              embedding_size=hyps['emb_size'], res_block=hyps['res_block'],\n                                              single_backbone_kwargs=hyps['sng_backbone_kwargs'],\n                                              context_backbone_kwargs=hyps['ctx_backbone_kwargs'])\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))\n\n(histories[exp_name]['train_history'],\n histories[exp_name]['best_model']) = train_eval_loop(sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              F.cross_entropy,\n                                              lr=hyps['lr'],\n                                              epoch_n=hyps['epoch_n'],\n                                              batch_size=hyps['batch'],\n                                              device=hyps['device'],\n                                              early_stopping_patience=hyps['stop_pat'],\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=300,\n                                              best_acc_type = 'acc',\n                                              experiment_name = exp_name,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=hyps['lr_sched_pat'],\n                                                                                                                         factor=hyps['lr_sched_fac'],\n                                                                                                                         verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:17.316451Z","iopub.execute_input":"2022-02-27T06:58:17.316659Z","iopub.status.idle":"2022-02-27T06:58:17.351693Z","shell.execute_reply.started":"2022-02-27T06:58:17.316621Z","shell.execute_reply":"2022-02-27T06:58:17.35098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Итоги экспериментов","metadata":{}},{"cell_type":"code","source":"#show_general_experiments_stats\ntrain_history = {}\nfor exp_name, exp in histories.items():\n    if 'train_history' in exp:\n        train_history[exp_name] = exp['train_history']\nshow_experiments_stats(train_history, show_plots = True, only_BEST_MODEL_CALC = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T06:58:53.007479Z","iopub.execute_input":"2022-02-27T06:58:53.007769Z","iopub.status.idle":"2022-02-27T06:58:54.034994Z","shell.execute_reply.started":"2022-02-27T06:58:53.00773Z","shell.execute_reply":"2022-02-27T06:58:54.03431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show_reports_stats\nfor exp_name in histories:\n    print('{:-<100}'.format(exp_name))\n    #choise saved dataset type\n    dtype = histories[exp_name]['hyps']['dataset']\n    train_labels = hdatasets[dtype]['train_labels']\n    train_dataset = hdatasets[dtype]['train_dataset']\n    test_labels = hdatasets[dtype]['test_labels']\n    test_dataset = hdatasets[dtype]['test_dataset']\n    \n    train_pred = predict_with_model(histories[exp_name]['best_model'], train_dataset)\n    train_loss = F.cross_entropy(torch.tensor(train_pred),\n                                              torch.tensor(train_labels))\n    print('Среднее значение функции потерь на обучении', float(train_loss))\n    print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n    histories[exp_name]['f1_macro'] = {}\n    histories[exp_name]['f1_macro']['train'] = f1_score(train_labels.view(-1), train_pred.argmax(1).reshape(-1), average='macro')\n    print()\n\n    test_pred = predict_with_model(histories[exp_name]['best_model'], test_dataset)\n    test_loss = F.cross_entropy(torch.tensor(test_pred),\n                                torch.tensor(test_labels))\n    print('Среднее значение функции потерь на валидации', float(test_loss))\n    print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n    histories[exp_name]['f1_macro']['val'] = f1_score(test_labels.view(-1), test_pred.argmax(1).reshape(-1), average='macro')\n    print()\n\nprint()\nfor exp_name in histories:\n    print('{:-<100}'.format(exp_name))\n    print('f1_macro: Train = {:.3f}, Val = {:.3f}'.format(histories[exp_name]['f1_macro']['train'],\n                                                          histories[exp_name]['f1_macro']['val']))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2022-02-27T06:58:54.038862Z","iopub.execute_input":"2022-02-27T06:58:54.040721Z","iopub.status.idle":"2022-02-27T07:01:54.993099Z","shell.execute_reply.started":"2022-02-27T06:58:54.040682Z","shell.execute_reply":"2022-02-27T07:01:54.991382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Применение полученных теггеров и сравнение на тестовых предложениях","metadata":{}},{"cell_type":"code","source":"import re\ntest_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок.'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)\n\ntest_sentences_with_true_pos = [\n    'мама-NOUN мыла-VERB раму-NOUN',\n    'косил-VERB косой-NOUN косой-ADJ косой-NOUN',\n    'глокая-ADJ куздра-NOUN штеко-ADV будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN',\n    'сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN',\n    'пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-VERB',\n    'ведро-NOUN дало-VERB течь-NOUN вода-NOUN стала-VERB течь-VERB',\n    'три-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN',\n    'три-NUM да-CCONJ три-NUM будет-AUX шесть-NUM',\n    'сорок-NUM сорок-NOUN'\n]\npos_pattern = r\"[A-Za-z]+\"\ntest_sentences_true_pos = [re.findall(pos_pattern, sent_with_pos) for sent_with_pos in test_sentences_with_true_pos]\n\nfor exp_name in histories:\n    print('{:-<100}'.format(exp_name))\n    print('')\n    \n    #choise saved dataset/vocab type\n    dtype = histories[exp_name]['hyps']['dataset']\n    char_vocab = hdatasets[dtype]['char_vocab']\n    \n    token_pos_tagger = POSTagger(histories[exp_name]['best_model'], char_vocab, UNIQUE_TAGS, \n                                 histories[exp_name]['hyps']['MAX_SENT_LEN'], histories[exp_name]['hyps']['MAX_ORIG_TOKEN_LEN'])\n    correct_predicts = []\n    for sent_tokens, sent_tags, sent_true_tags in zip(test_sentences_tokenized, token_pos_tagger(test_sentences), test_sentences_true_pos):\n        out_tags = []\n        for tag, true_tag in zip(sent_tags, sent_true_tags):\n            correct_predicts.append(tag == true_tag)\n            if correct_predicts[-1]:\n                out_tags.append(tag)\n            else:\n                out_tags.append(\"{}!BAD!->{}\".format(tag, true_tag))\n                         \n        print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, out_tags)))\n        print()\n    histories[exp_name]['TEST_SENTENS'] = {\n        'ACC': np.mean(np.array(correct_predicts)),\n        'correct': sum(np.array(correct_predicts)),\n        'all': len(correct_predicts)\n    }\n    print('ACC: {:.4f}, correct = {} of {}'.format(histories[exp_name]['TEST_SENTENS']['ACC'], \n                                                  histories[exp_name]['TEST_SENTENS']['correct'], \n                                                  histories[exp_name]['TEST_SENTENS']['all']))\n    print('')\n    print('')\n\nfor exp_name in histories:\n    print('{:-<100}'.format(exp_name))\n    print('ACC: {:.4f}, correct = {} of {}'.format(histories[exp_name]['TEST_SENTENS']['ACC'], \n                                                  histories[exp_name]['TEST_SENTENS']['correct'], \n                                                  histories[exp_name]['TEST_SENTENS']['all']))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.12693Z"},"execution":{"iopub.status.busy":"2022-02-27T07:01:54.994704Z","iopub.execute_input":"2022-02-27T07:01:54.994994Z","iopub.status.idle":"2022-02-27T07:01:55.10026Z","shell.execute_reply.started":"2022-02-27T07:01:54.994959Z","shell.execute_reply":"2022-02-27T07:01:55.098592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}