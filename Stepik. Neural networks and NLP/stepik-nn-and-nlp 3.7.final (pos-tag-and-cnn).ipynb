{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\ndef character_tokenize(txt):\n    return list(txt)\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef tokenize_corpus_verbose(texts, tokenizer=tokenize_text_simple_regex, verbose_chunk=1000, **tokenizer_kwargs):\n    tokenize_texts = []\n    for i, text in enumerate(texts):\n        tokenize_texts.append(tokenizer(text, **tokenizer_kwargs))\n        if i % verbose_chunk == 0:\n            print('Complete: {}/{}'.format(i,len(texts)))\n    return tokenize_texts\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf', 'tfpmi'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n        \n    elif mode == 'tfpmi': # TFPMI=TF⋅PMI\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(word2freq)  # домножить каждую строку на word2freq (это массив PMI Scores)\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    test_dataset = None,\n                    experiment_name = 'NoName',\n                    no_calculate_accuracy = False):\n    \"\"\"\n    v2.1\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                if not no_calculate_accuracy:\n                    train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                else: train_accuracy_epoch.append(0.)\n                    \n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    if not no_calculate_accuracy:\n                        val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                        #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    else:\n                        val_accuracy_epoch.append(0.)\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.3f}, train = {:.3f}    LOSS: val = {:.3f}, train = {:.3f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.3f}, train = {:.3f}, LOSS: val = {:.3f}, train = {:.3f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['BEST'] = {}\n    history['BEST']['epoch'] = best_epoch_i\n    history['BEST']['dict_size'] = batch_x.shape[-1]\n    \n    \n    #calculate and save final metrics best_model on train/val/test datasets\n    if test_dataset is not None:\n        history['BEST']['acc'] = {}\n        history['BEST']['loss'] = {}\n        \n        #save validation metrics (no calculate again)\n        history['BEST']['acc']['val'] = history['acc']['val'][best_epoch_i]\n        history['BEST']['loss']['val'] = history['loss']['val'][best_epoch_i]\n        \n        #calculate and save train metrics\n        train_pred = predict_with_model(best_model, train_dataset, return_labels=True)\n        history['BEST']['loss']['train'] = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n        history['BEST']['acc']['train'] = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n        \n        #calculate and save test metrics\n        test_pred = predict_with_model(best_model, test_dataset, return_labels=True)\n        history['BEST']['loss']['test'] = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n        history['BEST']['acc']['test'] = accuracy_score(test_pred[1], test_pred[0].argmax(-1))    \n    \n    \n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)\n\n\n#########stepik-dl-nlp/dlnlputils/nnets.py#########\n\nfrom torch.utils.data import Dataset\n\n\ndef ensure_length(txt, out_len, pad_value):\n    if len(txt) < out_len:\n        txt = list(txt) + [pad_value] * (out_len - len(txt))\n    else:\n        txt = txt[:out_len]\n    return txt\n\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, texts, targets, out_len=100, pad_value=0):\n        self.texts = texts\n        self.targets = targets\n        self.out_len = out_len\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n\n#########stepik-dl-nlp/dlnlputils/embeddings.py#########\n\nclass Embeddings:\n    def __init__(self, embeddings, word2id):\n        self.embeddings = embeddings\n        self.embeddings /= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n        self.word2id = word2id\n        self.id2word = {i: w for w, i in word2id.items()}\n\n    def most_similar(self, positive=None, negative=None, topk=10, with_mean = False):\n        #modified by wis, converted to gensim syntax\n        \n        if positive is not None:\n            if type(positive) != list:\n                positive = [positive]\n            pos_vec = [self.get_vector(word) for word in positive]\n            pos_len = len(positive)\n        else:\n            pos_vec = 0\n            pos_len = 1\n            \n        if negative is not None:\n            if type(negative) != list:\n                negative = [negative]\n            neg_vec = [self.get_vector(word) for word in negative]\n            neg_len = len(negative)\n        else:\n            neg_vec = 0\n            neg_len = 1\n        \n        if with_mean:\n            result_vec = np.array(pos_vec).sum(0) / pos_len - np.array(neg_vec).sum(0) / neg_len\n        else:\n            result_vec = np.array(pos_vec).sum(0) - np.array(neg_vec).sum(0)\n        \n        return self.most_similar_by_vector(result_vec, topk=topk)\n    \n    def most_similar_legacy(self, word, topk=10):\n        return self.most_similar_by_vector(self.get_vector(word), topk=topk)\n\n    def analogy(self, a1, b1, a2, topk=10):\n        a1_v = self.get_vector(a1)\n        b1_v = self.get_vector(b1)\n        a2_v = self.get_vector(a2)\n        query = b1_v - a1_v + a2_v\n        return self.most_similar_by_vector(query, topk=topk)\n\n    def most_similar_by_vector(self, query_vector, topk=10):\n        similarities = (self.embeddings * query_vector).sum(-1)\n        best_indices = np.argpartition(-similarities, topk, axis=0)[:topk]\n        result = [(self.id2word[i], similarities[i]) for i in best_indices]\n        result.sort(key=lambda pair: -pair[1])\n        return result\n\n    def get_vector(self, word):\n        if word not in self.word2id:\n            raise ValueError('Неизвестное слово \"{}\"'.format(word))\n        return self.embeddings[self.word2id[word]]\n\n    def get_vectors(self, *words):\n        word_ids = [self.word2id[i] for i in words]\n        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n        return vectors\n\n#########stepik-dl-nlp/dlnlputils/visualization.py#########\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\n\ndef plot_vectors(vectors, labels, how='tsne', ax=None, xy_lim=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n\n    x = projections[:, 0]\n    y = projections[:, 1]\n    if xy_lim is not None:\n        ax.set_xlim(xy_lim)\n        ax.set_ylim(xy_lim)\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))\n        \n\n#########stepik-dl-nlp/dlnlputils/data/pos.py#########\nimport torch\nfrom torch.utils.data import TensorDataset\n\ndef pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            if token.form is not None:\n                for char_i, char in enumerate(token.form):\n                    inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)\n            else:\n                for char_i, char in enumerate('-'):\n                    inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n\n    return inputs, targets\n\n\nclass POSTagger:\n    def __init__(self, model, char2id, id2label, max_sent_len, max_token_len):\n        self.model = model\n        self.char2id = char2id\n        self.id2label = id2label\n        self.max_sent_len = max_sent_len\n        self.max_token_len = max_token_len\n\n    def __call__(self, sentences):\n        tokenized_corpus = tokenize_corpus(sentences, min_token_size=1)\n\n        inputs = torch.zeros((len(sentences), self.max_sent_len, self.max_token_len + 2), dtype=torch.long)\n\n        for sent_i, sentence in enumerate(tokenized_corpus):\n            for token_i, token in enumerate(sentence):\n                for char_i, char in enumerate(token):\n                    inputs[sent_i, token_i, char_i + 1] = self.char2id.get(char, 0)\n\n        dataset = TensorDataset(inputs, torch.zeros(len(sentences)))\n        predicted_probs = predict_with_model(self.model, dataset)  # SentenceN x TagsN x MaxSentLen\n        predicted_classes = predicted_probs.argmax(1)\n\n        result = []\n        for sent_i, sent in enumerate(tokenized_corpus):\n            result.append([self.id2label[cls] for cls in predicted_classes[sent_i, :len(sent)]])\n        return result","metadata":{"execution":{"iopub.status.busy":"2022-02-22T12:37:14.927051Z","iopub.execute_input":"2022-02-22T12:37:14.927310Z","iopub.status.idle":"2022-02-22T12:37:15.068473Z","shell.execute_reply.started":"2022-02-22T12:37:14.927279Z","shell.execute_reply":"2022-02-22T12:37:15.067760Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n# import spacy\n# !python -m spacy download ru_core_news_md\n# spacy_nlp = spacy.load('ru_core_news_md', disable=['parser', 'ner'])\n\ndef tokenize_text_spacy_lemmatize(txt, spacy_nlp, min_token_size=4, with_pos = True, remove_stopwords = False):\n    doc = spacy_nlp(txt)\n    \n    if remove_stopwords:\n        lemmatized_doc = [token for token in doc if (len(token) >= min_token_size) and (not token.is_stop)]\n    else:\n        lemmatized_doc = [token for token in doc if len(token) >= min_token_size]\n    \n    if with_pos:\n        return ['_'.join([token.lemma_, token.pos_]) for token in lemmatized_doc]\n    else:\n        return [token.lemma_ for token in lemmatized_doc]\n\ndef tokenize_corpus_convert(tokenized_corpus, converter, addition = False):\n    '''\n    Convert each token in tokenized_corpus by converter\n    \n    Sample (PorterStemmer):\n    import nltk\n    ps = nltk.stemmer.PorterStemmer()\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=ps.stem)\n    \n    Sample (SnowballStemmer):\n    import nltk\n    sno = nltk.stem.SnowballStemmer('english')\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=sno.stem)\n    \n    Sample (WordNetLemmatizer):\n    import nltk\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    tokenized_lemmas_corpus = tokenize_corpus_convert(tokenized_corpus, converter=lemma.lemmatize)\n    '''\n    output = []\n    if not addition: #возвращаем только преобразованные токены\n        for doc in tokenized_corpus:\n            output.append([converter(token) for token in doc])\n    else: #возвращаем списк из исходных токенов, дополненных списком преобразованных\n        for doc in tokenized_corpus:\n            output.append(doc + [converter(token) for token in doc])        \n    return output\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), show_plots = True, only_BEST_MODEL_CALC = False):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        \n        if not only_BEST_MODEL_CALC:\n            epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n            print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_max_acc, \n                          histories[experiment_id]['acc']['val'][epoch_max_acc], \n                          histories[experiment_id]['acc']['train'][epoch_max_acc],\n                          histories[experiment_id]['loss']['val'][epoch_max_acc],\n                          histories[experiment_id]['loss']['train'][epoch_max_acc]))\n            epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n            print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_min_loss, \n                          histories[experiment_id]['acc']['val'][epoch_min_loss], \n                          histories[experiment_id]['acc']['train'][epoch_min_loss],\n                          histories[experiment_id]['loss']['val'][epoch_min_loss],\n                          histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if 'acc' in histories[experiment_id]['BEST']:\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.3f}, train = {:.3f},   LOSS: test = {:.3f}, train = {:.3f}  DICT SIZE = {}\"\\\n                  .format(histories[experiment_id]['BEST']['epoch'], \n                          histories[experiment_id]['BEST']['acc']['test'],\n                          histories[experiment_id]['BEST']['acc']['train'],\n                          histories[experiment_id]['BEST']['loss']['test'],\n                          histories[experiment_id]['BEST']['loss']['train'],\n                          histories[experiment_id]['BEST']['dict_size']))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n\ndef run_most_sumilars(func_most_similars, words_list, verbose = True, **kwargs):\n    most_similars = {word: func_most_similars(word, **kwargs) for word in words_list}\n    if verbose:\n        for word, similars in most_similars.items():\n            print('{}:'.format(word))\n            print('\\n'.join(map(str,similars)))\n            print(' ')\n    return most_similars\n        \n\n#https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/4529901\nimport pickle\ndef save_object(obj, filename):\n    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\ndef load_object(filename):\n    with open(filename, 'rb') as inp:\n        return pickle.load(inp)\n\n# sample usage\n#company1 = [1,2,3,4,5]\n#save_object(company1, '/kaggle/working/company1.pkl')\n#del company\n#company1 = load_object(filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T12:02:09.729742Z","iopub.execute_input":"2022-02-22T12:02:09.729977Z","iopub.status.idle":"2022-02-22T12:02:09.755056Z","shell.execute_reply.started":"2022-02-22T12:02:09.729945Z","shell.execute_reply":"2022-02-22T12:02:09.754403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"histories = {}","metadata":{"execution":{"iopub.status.busy":"2022-02-22T12:02:09.756271Z","iopub.execute_input":"2022-02-22T12:02:09.756656Z","iopub.status.idle":"2022-02-22T12:02:09.767298Z","shell.execute_reply.started":"2022-02-22T12:02:09.756620Z","shell.execute_reply":"2022-02-22T12:02:09.766585Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Свёрточные нейросети и POS-теггинг\n\nPOS-теггинг - определение частей речи (снятие частеречной неоднозначности)","metadata":{}},{"cell_type":"code","source":"!pip install pyconll\n# !pip install spacy_udpipe","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:42:57.976431Z","start_time":"2019-10-29T19:42:57.959538Z"},"execution":{"iopub.status.busy":"2022-02-22T12:04:26.161067Z","iopub.execute_input":"2022-02-22T12:04:26.161850Z","iopub.status.idle":"2022-02-22T12:04:35.442303Z","shell.execute_reply.started":"2022-02-22T12:04:26.161801Z","shell.execute_reply":"2022-02-22T12:04:35.441505Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys; sys.path.append('./stepik-dl-nlp')\n\nfrom sklearn.metrics import classification_report\n\nimport numpy as np\n\nimport pyconll\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\n\n# import dlnlputils\n# from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n#     character_tokenize, pos_corpus_to_tensor, POSTagger\n# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n\ninit_random_seed()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:34.549739Z","start_time":"2019-10-29T19:49:32.179692Z"},"execution":{"iopub.status.busy":"2022-02-22T12:04:41.314719Z","iopub.execute_input":"2022-02-22T12:04:41.315322Z","iopub.status.idle":"2022-02-22T12:04:41.391236Z","shell.execute_reply.started":"2022-02-22T12:04:41.315281Z","shell.execute_reply":"2022-02-22T12:04:41.390423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка текстов и разбиение на обучающую и тестовую подвыборки","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n!mkdir ./stepik-dl-nlp/\n!mkdir ./stepik-dl-nlp/datasets/\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:08.433599Z","start_time":"2019-10-29T19:46:05.110693Z"},"execution":{"iopub.status.busy":"2022-02-22T12:07:26.650117Z","iopub.execute_input":"2022-02-22T12:07:26.650428Z","iopub.status.idle":"2022-02-22T12:07:32.716137Z","shell.execute_reply.started":"2022-02-22T12:07:26.650390Z","shell.execute_reply":"2022-02-22T12:07:32.715381Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nfull_train = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu')\nfull_test = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu')\nprint(len(full_train), len(full_test))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.525561Z","start_time":"2019-10-29T19:49:37.315213Z"},"execution":{"iopub.status.busy":"2022-02-22T12:10:38.136773Z","iopub.execute_input":"2022-02-22T12:10:38.137441Z","iopub.status.idle":"2022-02-22T12:10:52.644336Z","shell.execute_reply.started":"2022-02-22T12:10:38.137399Z","shell.execute_reply":"2022-02-22T12:10:52.643566Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"for sent in full_train[:2]:\n    for token in sent:\n        print(token.form, token.upos)\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.548127Z","start_time":"2019-10-29T19:49:56.527559Z"},"execution":{"iopub.status.busy":"2022-02-22T12:11:00.970271Z","iopub.execute_input":"2022-02-22T12:11:00.970611Z","iopub.status.idle":"2022-02-22T12:11:01.019929Z","shell.execute_reply.started":"2022-02-22T12:11:00.970576Z","shell.execute_reply":"2022-02-22T12:11:01.019233Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"MAX_SENT_LEN = max(len(sent) for sent in full_train)\nMAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\nprint('Наибольшая длина предложения', MAX_SENT_LEN)\nprint('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.916262Z","start_time":"2019-10-29T19:49:56.549806Z"},"execution":{"iopub.status.busy":"2022-02-22T12:11:13.078245Z","iopub.execute_input":"2022-02-22T12:11:13.078979Z","iopub.status.idle":"2022-02-22T12:11:13.340682Z","shell.execute_reply.started":"2022-02-22T12:11:13.078937Z","shell.execute_reply":"2022-02-22T12:11:13.339843Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\nprint('\\n'.join(all_train_texts[:10]))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:57.251433Z","start_time":"2019-10-29T19:49:56.919818Z"},"execution":{"iopub.status.busy":"2022-02-22T12:11:14.462846Z","iopub.execute_input":"2022-02-22T12:11:14.463320Z","iopub.status.idle":"2022-02-22T12:11:14.712015Z","shell.execute_reply.started":"2022-02-22T12:11:14.463282Z","shell.execute_reply":"2022-02-22T12:11:14.711199Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\nchar_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\nprint(\"Количество уникальных символов\", len(char_vocab))\nprint(list(char_vocab.items())[:10])","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"execution":{"iopub.status.busy":"2022-02-22T12:12:36.315894Z","iopub.execute_input":"2022-02-22T12:12:36.316169Z","iopub.status.idle":"2022-02-22T12:12:36.787353Z","shell.execute_reply.started":"2022-02-22T12:12:36.316136Z","shell.execute_reply":"2022-02-22T12:12:36.785883Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\nlabel2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\nlabel2id","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.524125Z","start_time":"2019-10-29T19:49:58.125577Z"},"execution":{"iopub.status.busy":"2022-02-22T12:12:40.626552Z","iopub.execute_input":"2022-02-22T12:12:40.627107Z","iopub.status.idle":"2022-02-22T12:12:40.818889Z","shell.execute_reply.started":"2022-02-22T12:12:40.627072Z","shell.execute_reply":"2022-02-22T12:12:40.818213Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2022-02-22T12:24:49.508713Z","iopub.execute_input":"2022-02-22T12:24:49.508962Z","iopub.status.idle":"2022-02-22T12:25:10.400769Z","shell.execute_reply.started":"2022-02-22T12:24:49.508932Z","shell.execute_reply":"2022-02-22T12:25:10.400034Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_inputs[1][:5]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.754883Z","start_time":"2019-10-29T19:49:40.582Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-02-22T12:25:13.607933Z","iopub.execute_input":"2022-02-22T12:25:13.608194Z","iopub.status.idle":"2022-02-22T12:25:13.660894Z","shell.execute_reply.started":"2022-02-22T12:25:13.608165Z","shell.execute_reply":"2022-02-22T12:25:13.660250Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_labels[1]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.756496Z","start_time":"2019-10-29T19:49:40.711Z"},"execution":{"iopub.status.busy":"2022-02-22T12:25:16.547751Z","iopub.execute_input":"2022-02-22T12:25:16.548274Z","iopub.status.idle":"2022-02-22T12:25:16.588965Z","shell.execute_reply.started":"2022-02-22T12:25:16.548237Z","shell.execute_reply":"2022-02-22T12:25:16.588220Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательная свёрточная архитектура","metadata":{}},{"cell_type":"code","source":"class StackedConv1d(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for _ in range(layers_n):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.316516Z","start_time":"2019-10-29T19:46:17.539Z"},"execution":{"iopub.status.busy":"2022-02-22T12:25:22.330859Z","iopub.execute_input":"2022-02-22T12:25:22.331525Z","iopub.status.idle":"2022-02-22T12:25:22.370938Z","shell.execute_reply.started":"2022-02-22T12:25:22.331487Z","shell.execute_reply":"2022-02-22T12:25:22.369979Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне отдельных токенов","metadata":{}},{"cell_type":"code","source":"class SingleTokenPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.backbone = StackedConv1d(embedding_size, **kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Linear(embedding_size, labels_num)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        \n        features = self.backbone(char_embeddings)\n        \n        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n        \n        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.317452Z","start_time":"2019-10-29T19:46:23.135Z"},"execution":{"iopub.status.busy":"2022-02-22T12:25:27.315306Z","iopub.execute_input":"2022-02-22T12:25:27.315850Z","iopub.status.idle":"2022-02-22T12:25:27.358508Z","shell.execute_reply.started":"2022-02-22T12:25:27.315813Z","shell.execute_reply":"2022-02-22T12:25:27.357642Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\nprint('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.318497Z","start_time":"2019-10-29T19:46:23.764Z"},"execution":{"iopub.status.busy":"2022-02-22T12:25:42.527216Z","iopub.execute_input":"2022-02-22T12:25:42.527820Z","iopub.status.idle":"2022-02-22T12:25:42.587088Z","shell.execute_reply.started":"2022-02-22T12:25:42.527779Z","shell.execute_reply":"2022-02-22T12:25:42.586412Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_single_token_model) = train_eval_loop(single_token_model,\n                                            train_dataset,\n                                            test_dataset,\n                                            F.cross_entropy,\n                                            lr=5e-3,\n                                            epoch_n=10,\n                                            batch_size=64,\n                                            device='cuda',\n                                            early_stopping_patience=5,\n                                            max_batches_per_epoch_train=500,\n                                            max_batches_per_epoch_val=100,\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                       factor=0.5,\n                                                                                                                       verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.31947Z","start_time":"2019-10-29T19:46:25.552Z"},"execution":{"iopub.status.busy":"2022-02-22T12:26:14.836813Z","iopub.execute_input":"2022-02-22T12:26:14.837312Z","iopub.status.idle":"2022-02-22T12:27:34.956248Z","shell.execute_reply.started":"2022-02-22T12:26:14.837274Z","shell.execute_reply":"2022-02-22T12:27:34.955548Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n!mkdir ./stepik-dl-nlp/models/\ntorch.save(best_single_token_model.state_dict(), './stepik-dl-nlp/models/single_token_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.320568Z","start_time":"2019-10-29T19:46:47.579Z"},"execution":{"iopub.status.busy":"2022-02-22T12:29:03.168620Z","iopub.execute_input":"2022-02-22T12:29:03.169430Z","iopub.status.idle":"2022-02-22T12:29:03.956824Z","shell.execute_reply.started":"2022-02-22T12:29:03.169386Z","shell.execute_reply":"2022-02-22T12:29:03.955969Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsingle_token_model.load_state_dict(torch.load('./stepik-dl-nlp/models/single_token_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(single_token_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(single_token_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2022-02-22T12:29:10.277033Z","iopub.execute_input":"2022-02-22T12:29:10.277308Z","iopub.status.idle":"2022-02-22T12:29:32.224098Z","shell.execute_reply.started":"2022-02-22T12:29:10.277277Z","shell.execute_reply":"2022-02-22T12:29:32.223383Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне предложений (с учётом контекста)","metadata":{}},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2022-02-22T12:29:56.968892Z","iopub.execute_input":"2022-02-22T12:29:56.969153Z","iopub.status.idle":"2022-02-22T12:29:57.011901Z","shell.execute_reply.started":"2022-02-22T12:29:56.969122Z","shell.execute_reply":"2022-02-22T12:29:57.011163Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.326925Z","start_time":"2019-10-29T19:46:50.31Z"},"execution":{"iopub.status.busy":"2022-02-22T12:29:57.967155Z","iopub.execute_input":"2022-02-22T12:29:57.967716Z","iopub.status.idle":"2022-02-22T12:29:58.013073Z","shell.execute_reply.started":"2022-02-22T12:29:57.967676Z","shell.execute_reply":"2022-02-22T12:29:58.012343Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model) = train_eval_loop(sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              F.cross_entropy,\n                                              lr=5e-3,\n                                              epoch_n=10,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2022-02-22T12:30:03.990448Z","iopub.execute_input":"2022-02-22T12:30:03.990977Z","iopub.status.idle":"2022-02-22T12:31:28.429774Z","shell.execute_reply.started":"2022-02-22T12:30:03.990940Z","shell.execute_reply":"2022-02-22T12:31:28.429049Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntorch.save(best_sentence_level_model.state_dict(), './stepik-dl-nlp/models/sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.52911Z"},"execution":{"iopub.status.busy":"2022-02-22T12:32:08.115713Z","iopub.execute_input":"2022-02-22T12:32:08.115997Z","iopub.status.idle":"2022-02-22T12:32:08.161551Z","shell.execute_reply.started":"2022-02-22T12:32:08.115964Z","shell.execute_reply":"2022-02-22T12:32:08.160698Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsentence_level_model.load_state_dict(torch.load('./stepik-dl-nlp/models/sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.564926Z","start_time":"2019-08-29T13:56:16.544481Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.092139Z","start_time":"2019-08-29T13:56:16.567242Z"},"execution":{"iopub.status.busy":"2022-02-22T12:32:12.424915Z","iopub.execute_input":"2022-02-22T12:32:12.425459Z","iopub.status.idle":"2022-02-22T12:32:34.174191Z","shell.execute_reply.started":"2022-02-22T12:32:12.425422Z","shell.execute_reply":"2022-02-22T12:32:34.173430Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Применение полученных теггеров и сравнение","metadata":{}},{"cell_type":"code","source":"single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nsentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"},"execution":{"iopub.status.busy":"2022-02-22T12:37:28.664871Z","iopub.execute_input":"2022-02-22T12:37:28.665636Z","iopub.status.idle":"2022-02-22T12:37:28.702682Z","shell.execute_reply.started":"2022-02-22T12:37:28.665598Z","shell.execute_reply":"2022-02-22T12:37:28.701853Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.12554Z","start_time":"2019-08-29T13:56:42.106771Z"},"execution":{"iopub.status.busy":"2022-02-22T12:37:47.630580Z","iopub.execute_input":"2022-02-22T12:37:47.631095Z","iopub.status.idle":"2022-02-22T12:37:47.668569Z","shell.execute_reply.started":"2022-02-22T12:37:47.631059Z","shell.execute_reply":"2022-02-22T12:37:47.667877Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.12693Z"},"execution":{"iopub.status.busy":"2022-02-22T12:37:52.350222Z","iopub.execute_input":"2022-02-22T12:37:52.350488Z","iopub.status.idle":"2022-02-22T12:37:52.405189Z","shell.execute_reply.started":"2022-02-22T12:37:52.350456Z","shell.execute_reply":"2022-02-22T12:37:52.404446Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.16881Z","start_time":"2019-08-29T13:56:42.149698Z"},"execution":{"iopub.status.busy":"2022-02-22T12:37:57.777112Z","iopub.execute_input":"2022-02-22T12:37:57.777392Z","iopub.status.idle":"2022-02-22T12:37:57.829424Z","shell.execute_reply.started":"2022-02-22T12:37:57.777341Z","shell.execute_reply":"2022-02-22T12:37:57.828728Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Свёрточный модуль своими руками","metadata":{}},{"cell_type":"code","source":"class MyConv1d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n                                   requires_grad=True)\n        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n\n        batch_size, src_channels, sequence_len = x.shape        \n        if self.padding > 0:\n            pad = x.new_zeros(batch_size, src_channels, self.padding)\n            x = torch.cat((pad, x, pad), dim=-1)\n            sequence_len = x.shape[-1]\n\n        chunks = []\n        chunk_size = sequence_len - self.kernel_size + 1\n        for offset in range(self.kernel_size):\n            chunks.append(x[:, :, offset:offset + chunk_size])\n\n        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n        return out_features","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.19314Z","start_time":"2019-08-29T13:56:42.170233Z"},"execution":{"iopub.status.busy":"2022-02-22T12:38:01.743167Z","iopub.execute_input":"2022-02-22T12:38:01.743434Z","iopub.status.idle":"2022-02-22T12:38:01.787156Z","shell.execute_reply.started":"2022-02-22T12:38:01.743403Z","shell.execute_reply":"2022-02-22T12:38:01.786274Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.210013Z","start_time":"2019-08-29T13:56:42.19462Z"},"execution":{"iopub.status.busy":"2022-02-22T12:38:05.641849Z","iopub.execute_input":"2022-02-22T12:38:05.642106Z","iopub.status.idle":"2022-02-22T12:38:05.692654Z","shell.execute_reply.started":"2022-02-22T12:38:05.642075Z","shell.execute_reply":"2022-02-22T12:38:05.691933Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n                                                      train_dataset,\n                                                      test_dataset,\n                                                      F.cross_entropy,\n                                                      lr=5e-3,\n                                                      epoch_n=10,\n                                                      batch_size=64,\n                                                      device='cuda',\n                                                      early_stopping_patience=5,\n                                                      max_batches_per_epoch_train=500,\n                                                      max_batches_per_epoch_val=100,\n                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                                 factor=0.5,\n                                                                                                                                 verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:00.233326Z","start_time":"2019-08-29T13:56:42.211456Z"},"execution":{"iopub.status.busy":"2022-02-22T12:39:45.741678Z","iopub.execute_input":"2022-02-22T12:39:45.741983Z","iopub.status.idle":"2022-02-22T12:40:24.483220Z","shell.execute_reply.started":"2022-02-22T12:39:45.741951Z","shell.execute_reply":"2022-02-22T12:40:24.482530Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:39.145214Z","start_time":"2019-08-29T14:06:00.234936Z"},"execution":{"iopub.status.busy":"2022-02-22T12:40:28.207520Z","iopub.execute_input":"2022-02-22T12:40:28.207779Z","iopub.status.idle":"2022-02-22T12:41:00.026937Z","shell.execute_reply.started":"2022-02-22T12:40:28.207751Z","shell.execute_reply":"2022-02-22T12:41:00.026173Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}