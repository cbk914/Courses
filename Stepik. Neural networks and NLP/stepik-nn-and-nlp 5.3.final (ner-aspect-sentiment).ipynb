{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-03-25T13:02:45.753476Z","iopub.execute_input":"2022-03-25T13:02:45.754118Z","iopub.status.idle":"2022-03-25T13:02:45.781780Z","shell.execute_reply.started":"2022-03-25T13:02:45.754016Z","shell.execute_reply":"2022-03-25T13:02:45.781001Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"raw","source":"Итоговое задание с курса:\nИ, вот, мы запускаем нашу модель на валидационном датасете, где она... досрочно завершается с ошибкой!\n\nМы поместили в embedding слой из модели fasttext векторы всех слов, которые входят в train и test выборки, но не подумали о том, как мы будем обрабатывать незнакомые слова.\n\nПо ссылке ниже вы можете ознакомиться с решением этой проблемы, использующим возможности fastText:\n\nhttps://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task7.1_aspect_sentiment_eval.ipynb\n\nА какие еще решения позволят решить проблему с незнакомыми словами (ниже варианты ответа)?\n\n- Для незнакомых слов добавить тэг <unknown> и задать для него случайный вектор в embedding слое; Сделать embedding слой обучаемым.\n- Помимо замороженного embedding слоя сделать тренируемый embedding слой, состоящий из вектора <unknown>>\n- Положиться на возможности fastText: убрать из модели embedding слой и сделать отдельный конвертер: слово -> вектор fastText. ","metadata":{}},{"cell_type":"code","source":"'''\nИтоговые задания для меня:\n    - перенести весь необходимый для запуска семинара код dlnlputils (с учетом моих ранее проиизведенных модификаций)\n    - проанализировать и понять код семинара (если нужно, добавить развернутые комментарии), в особенности понять \n      изменения, которые позволяют учитывать слова не только слева, но и справа\n    - разобраться с приведенным в .../Samsung-IT-Academy/stepik-dl-nlp/blob/master/task7.1_aspect_sentiment_eval.ipynb \n      решением для обработки незнакомых слов (предварительно добиться ошибки на новых словах текущей модели), реализовать\n      предложенные изменения\n    - реализовать другие (из предложенных в задании выше) решения по обработке незнакомых слов\n    - улучшить результаты достигаемые предложенным baseline\n\nP.S. C учетом быстро пройденного предыдущего семинара, могу спокойно отвести на этот семинар неделю, спешить не стоит.\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Аспектный анализ тональности текстов","metadata":{}},{"cell_type":"markdown","source":"* [1] https://github.com/nlpub/pymystem3\n* [2] SentiRuEval2015\n* [3] https://rusvectores.org/ru/models/","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle,\n# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n\n# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n# import sys; sys.path.append('./stepik-dl-nlp')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntorch.manual_seed(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Оригинальная разметка","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nxml_sentiments = 'datasets/sentirueval2015/SentiRuEval_car_markup_train.xml'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import parse_xml_sentiment, parse_xml_aspect, show_markup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"тексты с разметкой аспектов и тональностей: ","metadata":{}},{"cell_type":"code","source":"texts_w_sentiment_spans = parse_xml_sentiment(xml_sentiments)\ntexts_w_aspect_spans    = parse_xml_aspect(xml_sentiments)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amount = len(texts_w_sentiment_spans)\n\nprint('Загружено {} текстов с разметкой тональности\\n'.format(amount))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### выберем 2 текста, на которых будем рассматривать все примеры:","metadata":{}},{"cell_type":"code","source":"random_picks = [random.randint(0,amount-1) for _ in range(0,2)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  тональность (sentiment)","metadata":{}},{"cell_type":"code","source":"for rand_i in random_picks:\n    text, spans = texts_w_sentiment_spans[rand_i]\n    \n    print('Текст №:',rand_i)\n    show_markup(text,spans)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####  Аспекты (aspects)","metadata":{}},{"cell_type":"code","source":"for rand_i in random_picks:\n    text,spans = texts_w_aspect_spans[rand_i]\n    \n    print('Текст №:',rand_i)\n    show_markup(text,spans)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BIO-тэги для обучения модели","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import fill_gaps, extract_BIO_tagged_tokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for rand_i in random_picks:\n    text, aspect_spans = texts_w_aspect_spans[rand_i]\n    cover_spans       = fill_gaps(text, aspect_spans)\n    \n    print('Полное покрытие разметкой текста №:',rand_i) \n    show_markup(text, cover_spans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Разбиение текста на предложения, а предложений - на слова.","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import regex_sentence_detector, sentence_spans,sentence_splitter","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\nword_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for rand_i in random_picks:\n    text, aspect_spans = texts_w_aspect_spans[rand_i]\n\n    print('Разбиение на предложения и BIO токенизация текста №:',rand_i) \n    for sentence, spans in sentence_splitter(text, aspect_spans):\n\n        cover_spans      = fill_gaps(sentence,spans)\n        tokens_w_biotags = extract_BIO_tagged_tokens(sentence, \n                                                     cover_spans, \n                                                     word_tokenizer.tokenize)\n\n        show_markup(sentence, cover_spans)\n        print(tokens_w_biotags[:10],'\\n')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Подготовка данных для обучения: ","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import prepare_data, form_vocabulary_and_tagset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nxml_train = 'datasets/sentirueval2015/SentiRuEval_car_markup_train.xml'\nxml_test  = 'datasets/sentirueval2015/SentiRuEval_car_markup_test.xml'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Токенизация: ","metadata":{}},{"cell_type":"code","source":"texts_w_aspect_spans = parse_xml_aspect(xml_train)\ntraining_data        = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n\ntexts_w_aspect_spans = parse_xml_aspect(xml_test)\ntest_data            = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### разбиение на предложения дало нам столько коротких текстов:","metadata":{}},{"cell_type":"code","source":"len(training_data), len(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = training_data + test_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary,labels = form_vocabulary_and_tagset(all_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### а размер словаря:","metadata":{}},{"cell_type":"code","source":"len(vocabulary), len(labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### индексация:","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import Converter, generate_markup","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converter = Converter(vocabulary,labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_recipe, test_tags = training_data[1211]\n\ntext, spans = generate_markup(test_recipe, test_tags) \n\nshow_markup(text, spans)\n\nencoded_recipe = converter.words_to_index(test_recipe)\nencoded_tags   = converter.tags_to_index(test_tags)\n\nprint(encoded_recipe)\nprint(encoded_tags)\nprint()\n\ndecoded_recipe = converter.indices_to_words(encoded_recipe)\ndecoded_tags   = converter.indices_to_tags(encoded_tags)\n\ntext, spans = generate_markup(decoded_recipe, decoded_tags) \n\nshow_markup(text, spans)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Нейросети","metadata":{}},{"cell_type":"code","source":"EMBEDDING_DIM = 300\nHIDDEN_DIM    = 32\nVOCAB_SIZE    = len(converter.word_to_idx)\nTAGSET_SIZE   = len(converter.tag_to_idx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Предобученные векторы слов\n\nАлгоритм fastText обученный на корпусе Тайга, смотрите подробности на сайте: https://rusvectores.org/ru/models/\n","metadata":{}},{"cell_type":"code","source":"# !pip3 install wget","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport gensim\n#import wget\n\nmodel_url = 'http://vectors.nlpl.eu/repository/11/187.zip'\n#wget.download(model_url)\nmodel_file = 'datasets/' + model_url.split('/')[-1]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = gensim.models.KeyedVectors.load('datasets/187/model.model')\n\nwords = ['тачка', 'двигатель', 'ауди']\n\nfor word in words:\n    #if word in w2v_model:\n           \n    for i in w2v_model.most_similar(positive=[word], topn=10):\n        nearest_word      = i[0]\n        cosine_similarity = i[1]\n        print(nearest_word, cosine_similarity)\n    print('\\n')\n    \n    #else: print(word + ' is not present in the model')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numpy_embeddings = np.zeros(shape=[VOCAB_SIZE, EMBEDDING_DIM],dtype=np.float32)\n\nfor word in vocabulary:\n    #if word in w2v_model:\n    vector = w2v_model.get_vector(word)\n    index  = converter.words_to_index([word])\n    numpy_embeddings[index] = vector\n        \n    #else: print(word + ' - такого слова нет в модели fasttext')\n        \npretrained_embeddings = torch.FloatTensor(numpy_embeddings)\npretrained_embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM \n\n1. Использует предобученные вектора слов и не изменяет их\n2. Двунаправленная","metadata":{}},{"cell_type":"code","source":"class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings):\n        super(LSTMTagger, self).__init__()\n        \n        self.hidden_dim      = hidden_dim\n        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n        self.lstm            = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n        self.hidden2tag      = nn.Linear(2*hidden_dim, tagset_size)\n\n    def forward(self, words):\n        embeds      = self.word_embeddings(words)\n        lstm_out, _ = self.lstm(embeds.view(len(words), 1, -1))\n        tag_space   = self.hidden2tag(lstm_out.view(len(words), -1))\n        tag_scores  = F.log_softmax(tag_space, dim=1)\n        \n        return tag_scores\n    \n    def predict_tags(self, words):\n        with torch.no_grad():\n            tags_pred = model(words).numpy()\n            tags_pred = np.argmax(tags_pred, axis=1)\n            \n        return tags_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Взвешеная функция потерь","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ntag_counter = Counter()\nfor _,tokens in training_data:\n    for token in tokens:\n        tag_counter[token]+=1\n        \ntag_counter.most_common()     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = torch.ones(15)\nclass_divs    = torch.ones(15)\n\nfor tag, inv_weight in tag_counter.most_common():\n    tag_idx             = converter.tags_to_index([tag])\n    class_divs[tag_idx] = inv_weight\n    \nnorm       = torch.norm(class_divs, p=2, dim=0).detach()\nclass_divs = class_divs.div(norm.expand_as(class_divs))\n\n\nclass_weights /= class_divs\n\n\nprint(class_weights.detach())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model         = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE, pretrained_embeddings)\nloss_function = nn.NLLLoss(class_weights) \noptimizer     = optim.SGD(model.parameters(), lr=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### training","metadata":{}},{"cell_type":"code","source":"from livelossplot import PlotLosses\n\nliveplot = PlotLosses()\n\nfor epoch in range(5): \n    for i, (recipe, tags) in enumerate(training_data):\n        \n        model.zero_grad()\n        \n        encoded_recipe = converter.words_to_index(recipe) # слово -> его номер в словаре \n        encoded_tags   = converter.tags_to_index(tags)    # тэг   -> его номер в списке тэгов\n        tag_scores     = model(encoded_recipe)\n        \n        loss = loss_function(tag_scores, encoded_tags)\n        loss.backward()\n        optimizer.step()\n        \n        \n        if i % 100 == 0:\n            liveplot.update({'negative log likelihood loss': loss})\n            liveplot.draw()\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_tags(model, converter, recipe):\n    \n    encoded_recipe = converter.words_to_index(recipe)        # слово -> его номер в словаре\n\n    encoded_tags   = model.predict_tags(encoded_recipe)      # предсказанные тэги (номера)\n\n    decoded_tags   = converter.indices_to_tags(encoded_tags) # номер тэга -> тэг\n    \n    return decoded_tags","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,10):\n\n    recipe, tags = test_data[np.random.randint(0,1000)]\n    \n    tags_pred    = predict_tags(model, converter, recipe)\n\n    print('истинные тэги:')\n    text, spans = generate_markup(recipe, tags) \n    show_markup(text, spans)\n\n    print('предсказанные тэги:')\n    text, spans = generate_markup(recipe, tags_pred) \n\n    show_markup(text, spans)\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Статистика","metadata":{}},{"cell_type":"markdown","source":"##### 1.  Количество верно предсказанных тэгов:","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import tag_statistics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_correct, total_tags = tag_statistics(model, converter, test_data)\n\n\nprint('Статистика верно предсказанных тэгов:\\n')\n\nfor tag in total_tags.keys():\n    print('для {}:'.format(tag))\n    print('  корректно:\\t', total_correct[tag])\n    print('      всего:\\t',   total_tags[tag])\n    print('% корректно:\\t', 100 * (total_correct[tag] / float(total_tags[tag])))\n    print()\n\nprint('----------')\nprint('в итоге:')\nprint('  корректно:\\t', sum(total_correct.values()))\nprint('      всего:\\t', sum(total_tags.values()))\nprint('% корректно:\\t', 100 * (sum(total_correct.values()) / sum(total_tags.values())))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 2.  Количество верно предсказанных тэгов в виде матрицы ошибок:","metadata":{}},{"cell_type":"code","source":"# from dlnlputils.sentiment_utils import plot_confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\ny_true = []\n\nfor sentence, tags in test_data:\n    y_pred += predict_tags(model, converter, sentence)\n    y_true += tags","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_true, y_pred, classes=list(total_tags.keys()), title='Матрица ошибок')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_true, y_pred, classes=list(total_tags.keys()), normalize=True, \n                      title='Нормализованная матрица ошибок')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}