{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-04T11:34:38.689500Z","iopub.execute_input":"2022-02-04T11:34:38.689768Z","iopub.status.idle":"2022-02-04T11:34:38.712998Z","shell.execute_reply.started":"2022-02-04T11:34:38.689741Z","shell.execute_reply":"2022-02-04T11:34:38.712055Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Итоговое задание семинара (3.3.Final)","metadata":{}},{"cell_type":"raw","source":"Формулировка из курса:\nВ качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы лучше понять свойства эмбеддингов и попробовать улучшить их качество. Что можно попробовать сделать:\n\n    - поиграться с параметрами - количеством отрицательных слов, размером батча, скоростью обучения, размером окна\n    - убрать разбиение текстов на предложения и увеличить окно\n    - изменить токенизацию, например, разобравшись с библиотекой SpaCy и подключив лемматизацию и POS-теггинг, чтобы строить эмбеддинги не для словоформ, а для лемм (например, chicked_NOUN)\n    - реализовать FastText и сравнить, как отличаются списки похожих документов, получаемых с помощью Word2Vec и FastText\n    - усложнить алгоритм оценки вероятности совместной встречаемости слов, например, заменив скалярное произведение на нейросеть с парой слоёв\n\nТакже мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы заставить Word2Vec выучить что-то интересное и полезное.\n\nОпишите то, что у Вас получилось, в ответе к этому шагу.\n\nБалл за этот шаг зачитывается автоматически, вне зависимости от текста, который Вы впишете в поле ответа :). Но то, насколько этот семинар и это задание будет полезным для Вас, вполне зависит - так что мы предлагаем Вам поисследовать возможности линейных моделей и подробно описать свой опыт. К тому же, после нажатия кнопки \"Отправить\" Вы получите доступ к ответам других участников и сможете обменяться своими находками.","metadata":{}},{"cell_type":"raw","source":"Мое индивидуальное задание:\n+ 1) сформировать датасет из выбранной мной русскоязычной книги (в формате txt)\n    + загрузить/сформировать из txt файла книги первоначальный корпус, где книга = корпус, строка (это в txt формате является обзатцем) = документ\n    + провести дальнейшую разбивку на предложения (на основе знаков препинания, ., ..., !, ? и т.п.), по итогу, книга = корпус, предложение = документ\n2) Обучить ембеддинги word2vec:\n    + через самописные функции из библиотеки dlnlputils\n    - через gensim\n3) Покрутить (с упором на принципы, ценности, общий окрас/особенности книги), выполнять циклично для разных вариантов моделей (далее), настроек, подходов к построению и т.п.:\n    - статистика по словам\n    + похожие слова\n    - сложение/вычитание\n    + карта схожести различных слов (особенно которые относятся к принципам)\n    + постараться определиться с основными метриками получаемых эмбеддингов (loss, применимо ли здесь в каком то виде accuracy и т.п.)\n4) Модификации основной модели (из библиотеки dlnlputils):\n    - поиграться с параметрами - количеством отрицательных слов, размером батча, скоростью обучения, размером окна\n    - убрать разбиение текстов на предложения и увеличить окно\n    - изменить токенизацию, например, разобравшись с библиотекой SpaCy и подключив лемматизацию и POS-теггинг, чтобы строить эмбеддинги не для словоформ, а для лемм (например, chicked_NOUN)\n    - реализовать FastText и сравнить, как отличаются списки похожих документов, получаемых с помощью Word2Vec и FastText\n    - усложнить алгоритм оценки вероятности совместной встречаемости слов, например, заменив скалярное произведение на нейросеть с парой слоёв  \n    ","metadata":{}},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef tokenize_corpus_verbose(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    tokenize_texts = []\n    for i, text in enumerate(texts):\n        tokenize_texts.append(tokenizer(text, **tokenizer_kwargs))\n        if i % 1000 == 0:\n            print('Complete:', i)\n    return tokenize_texts\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf', 'tfpmi'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n        \n    elif mode == 'tfpmi': # TFPMI=TF⋅PMI\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(word2freq)  # домножить каждую строку на word2freq (это массив PMI Scores)\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    test_dataset = None,\n                    experiment_name = 'NoName',\n                    no_calculate_accuracy = False):\n    \"\"\"\n    v2.1\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                if not no_calculate_accuracy:\n                    train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                else: train_accuracy_epoch.append(0.)\n                    \n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    if not no_calculate_accuracy:\n                        val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                        #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    else:\n                        val_accuracy_epoch.append(0.)\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.3f}, train = {:.3f}    LOSS: val = {:.3f}, train = {:.3f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.3f}, train = {:.3f}, LOSS: val = {:.3f}, train = {:.3f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['BEST'] = {}\n    history['BEST']['epoch'] = best_epoch_i\n    history['BEST']['dict_size'] = batch_x.shape[-1]\n    \n    \n    #calculate and save final metrics best_model on train/val/test datasets\n    if test_dataset is not None:\n        history['BEST']['acc'] = {}\n        history['BEST']['loss'] = {}\n        \n        #save validation metrics (no calculate again)\n        history['BEST']['acc']['val'] = history['acc']['val'][best_epoch_i]\n        history['BEST']['loss']['val'] = history['loss']['val'][best_epoch_i]\n        \n        #calculate and save train metrics\n        train_pred = predict_with_model(best_model, train_dataset, return_labels=True)\n        history['BEST']['loss']['train'] = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n        history['BEST']['acc']['train'] = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n        \n        #calculate and save test metrics\n        test_pred = predict_with_model(best_model, test_dataset, return_labels=True)\n        history['BEST']['loss']['test'] = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n        history['BEST']['acc']['test'] = accuracy_score(test_pred[1], test_pred[0].argmax(-1))    \n    \n    \n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)\n\n\n#########stepik-dl-nlp/dlnlputils/nnets.py#########\n\nfrom torch.utils.data import Dataset\n\n\ndef ensure_length(txt, out_len, pad_value):\n    if len(txt) < out_len:\n        txt = list(txt) + [pad_value] * (out_len - len(txt))\n    else:\n        txt = txt[:out_len]\n    return txt\n\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, texts, targets, out_len=100, pad_value=0):\n        self.texts = texts\n        self.targets = targets\n        self.out_len = out_len\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n\n#########stepik-dl-nlp/dlnlputils/embeddings.py#########\n\nclass Embeddings:\n    def __init__(self, embeddings, word2id):\n        self.embeddings = embeddings\n        self.embeddings /= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n        self.word2id = word2id\n        self.id2word = {i: w for w, i in word2id.items()}\n\n    def most_similar(self, word, topk=10):\n        return self.most_similar_by_vector(self.get_vector(word), topk=topk)\n\n    def analogy(self, a1, b1, a2, topk=10):\n        a1_v = self.get_vector(a1)\n        b1_v = self.get_vector(b1)\n        a2_v = self.get_vector(a2)\n        query = b1_v - a1_v + a2_v\n        return self.most_similar_by_vector(query, topk=topk)\n\n    def most_similar_by_vector(self, query_vector, topk=10):\n        similarities = (self.embeddings * query_vector).sum(-1)\n        best_indices = np.argpartition(-similarities, topk, axis=0)[:topk]\n        result = [(self.id2word[i], similarities[i]) for i in best_indices]\n        result.sort(key=lambda pair: -pair[1])\n        return result\n\n    def get_vector(self, word):\n        if word not in self.word2id:\n            raise ValueError('Неизвестное слово \"{}\"'.format(word))\n        return self.embeddings[self.word2id[word]]\n\n    def get_vectors(self, *words):\n        word_ids = [self.word2id[i] for i in words]\n        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n        return vectors\n\n#########stepik-dl-nlp/dlnlputils/visualization.py#########\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\n\ndef plot_vectors(vectors, labels, how='tsne', ax=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n\n    x = projections[:, 0]\n    y = projections[:, 1]\n\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:38.868301Z","iopub.execute_input":"2022-02-04T11:34:38.868737Z","iopub.status.idle":"2022-02-04T11:34:39.093391Z","shell.execute_reply.started":"2022-02-04T11:34:38.868698Z","shell.execute_reply":"2022-02-04T11:34:39.092540Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport spacy\nspacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\ndef tokenize_text_spacy_lemmatize(txt, spacy_nlp, min_token_size=1):\n    doc = spacy_nlp(txt)\n    return [token.lemma_ for token in doc if len(token) >= min_token_size]\n\ndef tokenize_corpus_convert(tokenized_corpus, converter, addition = False):\n    '''\n    Convert each token in tokenized_corpus by converter\n    \n    Sample (PorterStemmer):\n    import nltk\n    ps = nltk.stemmer.PorterStemmer()\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=ps.stem)\n    \n    Sample (SnowballStemmer):\n    import nltk\n    sno = nltk.stem.SnowballStemmer('english')\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=sno.stem)\n    \n    Sample (WordNetLemmatizer):\n    import nltk\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    tokenized_lemmas_corpus = tokenize_corpus_convert(tokenized_corpus, converter=lemma.lemmatize)\n    '''\n    output = []\n    if not addition: #возвращаем только преобразованные токены\n        for doc in tokenized_corpus:\n            output.append([converter(token) for token in doc])\n    else: #возвращаем списк из исходных токенов, дополненных списком преобразованных\n        for doc in tokenized_corpus:\n            output.append(doc + [converter(token) for token in doc])        \n    return output\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), show_plots = True, only_BEST_MODEL_CALC = False):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        \n        if not only_BEST_MODEL_CALC:\n            epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n            print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_max_acc, \n                          histories[experiment_id]['acc']['val'][epoch_max_acc], \n                          histories[experiment_id]['acc']['train'][epoch_max_acc],\n                          histories[experiment_id]['loss']['val'][epoch_max_acc],\n                          histories[experiment_id]['loss']['train'][epoch_max_acc]))\n            epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n            print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_min_loss, \n                          histories[experiment_id]['acc']['val'][epoch_min_loss], \n                          histories[experiment_id]['acc']['train'][epoch_min_loss],\n                          histories[experiment_id]['loss']['val'][epoch_min_loss],\n                          histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if 'acc' in histories[experiment_id]['BEST']:\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.3f}, train = {:.3f},   LOSS: test = {:.3f}, train = {:.3f}  DICT SIZE = {}\"\\\n                  .format(histories[experiment_id]['BEST']['epoch'], \n                          histories[experiment_id]['BEST']['acc']['test'],\n                          histories[experiment_id]['BEST']['acc']['train'],\n                          histories[experiment_id]['BEST']['loss']['test'],\n                          histories[experiment_id]['BEST']['loss']['train'],\n                          histories[experiment_id]['BEST']['dict_size']))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n        \n\n#https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/4529901\nimport pickle\ndef save_object(obj, filename):\n    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\ndef load_object(filename):\n    with open(filename, 'rb') as inp:\n        return pickle.load(inp)\n\n# sample usage\n#company1 = [1,2,3,4,5]\n#save_object(company1, '/kaggle/working/company1.pkl')\n#del company\n#company1 = load_object(filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:39.095260Z","iopub.execute_input":"2022-02-04T11:34:39.095520Z","iopub.status.idle":"2022-02-04T11:34:47.636493Z","shell.execute_reply.started":"2022-02-04T11:34:39.095490Z","shell.execute_reply":"2022-02-04T11:34:47.635534Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Baseline","metadata":{}},{"cell_type":"markdown","source":"### Загрузка данных и подготовка корпуса","metadata":{}},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:47.638259Z","iopub.execute_input":"2022-02-04T11:34:47.638576Z","iopub.status.idle":"2022-02-04T11:34:47.646104Z","shell.execute_reply.started":"2022-02-04T11:34:47.638535Z","shell.execute_reply":"2022-02-04T11:34:47.645094Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"BOOK_FILENAME = '/kaggle/input/my-private-datasets/CVbooks/7h_rus.txt'\nBOOK_LANGUAGE='russian'\nwith open(BOOK_FILENAME) as file:\n    corpus_paragraphs = [line.rstrip() for line in file]\ncorpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\nprint(len(corpus_paragraphs))\ncorpus_paragraphs[200:205]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:47.647522Z","iopub.execute_input":"2022-02-04T11:34:47.647734Z","iopub.status.idle":"2022-02-04T11:34:47.696447Z","shell.execute_reply.started":"2022-02-04T11:34:47.647709Z","shell.execute_reply":"2022-02-04T11:34:47.695602Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#convert corpus of paragraphs to corpus of sentences\ncorpus_sentences = sent_tokenize(' '.join(corpus_paragraphs), language=BOOK_LANGUAGE)\nprint(len(corpus_sentences))\ncorpus_sentences[200:205]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:47.699253Z","iopub.execute_input":"2022-02-04T11:34:47.699759Z","iopub.status.idle":"2022-02-04T11:34:48.121137Z","shell.execute_reply.started":"2022-02-04T11:34:47.699716Z","shell.execute_reply":"2022-02-04T11:34:48.120351Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"random.shuffle(corpus_sentences)\ncorpus_sentences[:5]\n\nTRAIN_VAL_SPLIT = int(len(corpus_sentences) * 0.8)\ntrain_source = corpus_sentences[:TRAIN_VAL_SPLIT]\ntest_source = corpus_sentences[TRAIN_VAL_SPLIT:]\nprint(\"Обучающая выборка\", len(train_source))\nprint(\"Тестовая выборка\", len(test_source))\nprint()\nprint('\\n'.join(train_source[:5]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.122136Z","iopub.execute_input":"2022-02-04T11:34:48.122359Z","iopub.status.idle":"2022-02-04T11:34:48.137367Z","shell.execute_reply.started":"2022-02-04T11:34:48.122334Z","shell.execute_reply":"2022-02-04T11:34:48.136413Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# токенизируем\ntrain_tokenized = tokenize_corpus(train_source)\ntest_tokenized = tokenize_corpus(test_source)\nprint('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.138756Z","iopub.execute_input":"2022-02-04T11:34:48.139061Z","iopub.status.idle":"2022-02-04T11:34:48.229700Z","shell.execute_reply.started":"2022-02-04T11:34:48.139021Z","shell.execute_reply":"2022-02-04T11:34:48.228752Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# строим словарь\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=1, pad_word='<PAD>')\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.230929Z","iopub.execute_input":"2022-02-04T11:34:48.231116Z","iopub.status.idle":"2022-02-04T11:34:48.278877Z","shell.execute_reply.started":"2022-02-04T11:34:48.231092Z","shell.execute_reply":"2022-02-04T11:34:48.278000Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\nprint('\\n'.join(' '.join(str(t) for t in sent)\n                for sent in train_token_ids[:10]))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.280091Z","iopub.execute_input":"2022-02-04T11:34:48.280339Z","iopub.status.idle":"2022-02-04T11:34:48.305992Z","shell.execute_reply.started":"2022-02-04T11:34:48.280296Z","shell.execute_reply":"2022-02-04T11:34:48.305167Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in train_token_ids], bins=20);\nplt.title('Гистограмма длин предложений');","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.307186Z","iopub.execute_input":"2022-02-04T11:34:48.307423Z","iopub.status.idle":"2022-02-04T11:34:48.628505Z","shell.execute_reply.started":"2022-02-04T11:34:48.307394Z","shell.execute_reply":"2022-02-04T11:34:48.627827Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in train_token_ids], bins=20);\nplt.yscale('log')\nplt.title('Гистограмма длин предложений');","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:48.629432Z","iopub.execute_input":"2022-02-04T11:34:48.630070Z","iopub.status.idle":"2022-02-04T11:34:49.370344Z","shell.execute_reply.started":"2022-02-04T11:34:48.630037Z","shell.execute_reply":"2022-02-04T11:34:49.369542Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"MAX_SENTENCE_LEN = 35\nsum(np.array([len(s) for s in train_token_ids]) <= MAX_SENTENCE_LEN), sum(np.array([len(s) for s in train_token_ids]) > MAX_SENTENCE_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:49.371557Z","iopub.execute_input":"2022-02-04T11:34:49.371798Z","iopub.status.idle":"2022-02-04T11:34:49.415148Z","shell.execute_reply.started":"2022-02-04T11:34:49.371768Z","shell.execute_reply":"2022-02-04T11:34:49.414271Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train_dataset = PaddedSequenceDataset(train_token_ids,\n                                      np.zeros(len(train_token_ids)),\n                                      out_len=MAX_SENTENCE_LEN)\ntest_dataset = PaddedSequenceDataset(test_token_ids,\n                                     np.zeros(len(test_token_ids)),\n                                     out_len=MAX_SENTENCE_LEN)\nprint(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:49.416521Z","iopub.execute_input":"2022-02-04T11:34:49.416763Z","iopub.status.idle":"2022-02-04T11:34:49.454662Z","shell.execute_reply.started":"2022-02-04T11:34:49.416702Z","shell.execute_reply":"2022-02-04T11:34:49.454052Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Алгоритм обучения - Skip Gram Negative Sampling\n\n**Skip Gram** - предсказываем соседние слова по центральному слову\n\n**Negative Sampling** - аппроксимация softmax\n\n$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n\n$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n\n$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n    \n$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$","metadata":{}},{"cell_type":"code","source":"def make_diag_mask(size, radius):\n    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n    idxs = torch.arange(size)\n    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n    return mask\n\nmake_diag_mask(10, 5)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:49.457186Z","iopub.execute_input":"2022-02-04T11:34:49.457561Z","iopub.status.idle":"2022-02-04T11:34:49.531621Z","shell.execute_reply.started":"2022-02-04T11:34:49.457526Z","shell.execute_reply":"2022-02-04T11:34:49.530851Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n\n* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n\n$$ \\\\ $$\n\n* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n\n$$ \\\\ $$\n\n$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$","metadata":{}},{"cell_type":"code","source":"class SkipGramNegativeSamplingTrainer(nn.Module):\n    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.negative_samples_n = negative_samples_n\n\n        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.center_emb.weight.data[0] = 0\n\n        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.context_emb.weight.data[0] = 0\n\n        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n    \n    def forward(self, sentences):\n        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n        batch_size = sentences.shape[0]\n        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n\n        # оценить сходство с настоящими соседними словами\n        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n        positive_probs = torch.sigmoid(positive_sims)\n\n        # увеличить оценку вероятности встретить эти пары слов вместе\n        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n                                               positive_mask.expand_as(positive_probs))\n\n        # выбрать случайные \"отрицательные\" слова\n        negative_words = torch.randint(1, self.vocab_size,\n                                       size=(batch_size, self.negative_samples_n),\n                                       device=sentences.device)  # Batch x NegSamplesN\n        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n        \n        # уменьшить оценку вероятность встретить эти пары слов вместе\n        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n                                                           negative_sims.new_zeros(negative_sims.shape))\n\n        return positive_loss + negative_loss\n\n\ndef no_loss(pred, target):\n    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:49.532659Z","iopub.execute_input":"2022-02-04T11:34:49.533203Z","iopub.status.idle":"2022-02-04T11:34:49.547795Z","shell.execute_reply.started":"2022-02-04T11:34:49.533165Z","shell.execute_reply":"2022-02-04T11:34:49.546918Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n                                          radius=5, negative_samples_n=25)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:34:49.548796Z","iopub.execute_input":"2022-02-04T11:34:49.549378Z","iopub.status.idle":"2022-02-04T11:34:49.625372Z","shell.execute_reply.started":"2022-02-04T11:34:49.549337Z","shell.execute_reply":"2022-02-04T11:34:49.624620Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"best_val_loss, best_model = train_eval_loop(trainer,\n                                            train_dataset,\n                                            test_dataset,\n                                            no_loss,\n                                            lr=1e-2,\n                                            epoch_n=10,\n                                            batch_size=8,\n                                            device='cpu',\n                                            early_stopping_patience=4,\n                                            max_batches_per_epoch_train=2000,\n                                            max_batches_per_epoch_val=len(test_dataset),\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True),\n                                            no_calculate_accuracy = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:47:07.790374Z","iopub.execute_input":"2022-02-04T11:47:07.790773Z","iopub.status.idle":"2022-02-04T11:57:05.387776Z","shell.execute_reply.started":"2022-02-04T11:47:07.790745Z","shell.execute_reply":"2022-02-04T11:57:05.386928Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntorch.save(trainer.state_dict(), './sgns.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:50.933143Z","iopub.execute_input":"2022-02-04T11:36:50.934076Z","iopub.status.idle":"2022-02-04T11:36:50.958508Z","shell.execute_reply.started":"2022-02-04T11:36:50.934030Z","shell.execute_reply":"2022-02-04T11:36:50.957795Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntrainer.load_state_dict(torch.load('./sgns.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:50.962082Z","iopub.execute_input":"2022-02-04T11:36:50.962520Z","iopub.status.idle":"2022-02-04T11:36:50.973439Z","shell.execute_reply.started":"2022-02-04T11:36:50.962462Z","shell.execute_reply":"2022-02-04T11:36:50.972412Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Исследуем характеристики полученных векторов","metadata":{}},{"cell_type":"code","source":"words_to_similar = ['вклад', 'парадигма', 'лидерство', 'доверие', 'служение']\nwords_to_plot = ['идея', 'один', 'между', 'который', 'отношения', 'могут', 'можно', 'однако', 'человек', 'день',\n              'время', 'делать', 'должен', 'больше', 'меньше', 'личность', 'характер', 'каждый', \n              'принцип', 'совесть', 'парадигма', 'доверие', 'верность', 'мужество', 'ответственность', 'скромность', 'терпение', 'простота', \n              'трудолюбие', 'справедливость', 'служение', 'вклад', 'качество', 'любовь', 'зависимость', \n              'взаимозависимость', 'забота', 'поддержка', 'простота', 'цельность']\n#'честность', 'добросовестность', 'отзывчивость'\n#Здесь речь шла о таких свойствах, как цельность личности, скромность, верность, умеренность, мужество, справедливость, терпеливость, трудолюбие, простота,","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:50.975038Z","iopub.execute_input":"2022-02-04T11:36:50.975457Z","iopub.status.idle":"2022-02-04T11:36:50.981245Z","shell.execute_reply.started":"2022-02-04T11:36:50.975417Z","shell.execute_reply":"2022-02-04T11:36:50.980537Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:50.982424Z","iopub.execute_input":"2022-02-04T11:36:50.982770Z","iopub.status.idle":"2022-02-04T11:36:51.003660Z","shell.execute_reply.started":"2022-02-04T11:36:50.982742Z","shell.execute_reply":"2022-02-04T11:36:51.002866Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#vocabulary","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.004833Z","iopub.execute_input":"2022-02-04T11:36:51.005430Z","iopub.status.idle":"2022-02-04T11:36:51.008464Z","shell.execute_reply.started":"2022-02-04T11:36:51.005399Z","shell.execute_reply":"2022-02-04T11:36:51.007839Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:02:11.125984Z","iopub.execute_input":"2022-02-04T12:02:11.126620Z","iopub.status.idle":"2022-02-04T12:02:11.130991Z","shell.execute_reply.started":"2022-02-04T12:02:11.126587Z","shell.execute_reply":"2022-02-04T12:02:11.130487Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"print('\\n'.join(['1','3','3']))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:30.118078Z","iopub.execute_input":"2022-02-04T12:09:30.118370Z","iopub.status.idle":"2022-02-04T12:09:30.124050Z","shell.execute_reply.started":"2022-02-04T12:09:30.118331Z","shell.execute_reply":"2022-02-04T12:09:30.123109Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"str(embeddings.most_similar('вклад', topk=10))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:10:05.806232Z","iopub.execute_input":"2022-02-04T12:10:05.806718Z","iopub.status.idle":"2022-02-04T12:10:05.816272Z","shell.execute_reply.started":"2022-02-04T12:10:05.806668Z","shell.execute_reply":"2022-02-04T12:10:05.815487Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"print('\\n'.join(map(str,embeddings.most_similar('вклад', topk=10))))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:10:20.509720Z","iopub.execute_input":"2022-02-04T12:10:20.510010Z","iopub.status.idle":"2022-02-04T12:10:20.518376Z","shell.execute_reply.started":"2022-02-04T12:10:20.509980Z","shell.execute_reply":"2022-02-04T12:10:20.517374Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"embeddings.most_similar('парадигма', topk=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.026202Z","iopub.execute_input":"2022-02-04T11:36:51.026501Z","iopub.status.idle":"2022-02-04T11:36:51.037682Z","shell.execute_reply.started":"2022-02-04T11:36:51.026466Z","shell.execute_reply":"2022-02-04T11:36:51.036726Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"embeddings.most_similar('синергия', topk=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.041195Z","iopub.execute_input":"2022-02-04T11:36:51.041659Z","iopub.status.idle":"2022-02-04T11:36:51.050549Z","shell.execute_reply.started":"2022-02-04T11:36:51.041622Z","shell.execute_reply":"2022-02-04T11:36:51.049426Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#решение семантической пропорции — то есть задача аналогии\n#word2 - word1 + word3\nembeddings.analogy('лидерство', 'совесть', 'человек')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.052429Z","iopub.execute_input":"2022-02-04T11:36:51.053199Z","iopub.status.idle":"2022-02-04T11:36:51.064538Z","shell.execute_reply.started":"2022-02-04T11:36:51.053155Z","shell.execute_reply":"2022-02-04T11:36:51.063492Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"'еслиd' in embeddings.word2id","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:05:17.432871Z","iopub.execute_input":"2022-02-04T12:05:17.433165Z","iopub.status.idle":"2022-02-04T12:05:17.438726Z","shell.execute_reply.started":"2022-02-04T12:05:17.433127Z","shell.execute_reply":"2022-02-04T12:05:17.438085Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"dir(embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:03:35.192825Z","iopub.execute_input":"2022-02-04T12:03:35.193387Z","iopub.status.idle":"2022-02-04T12:03:35.199339Z","shell.execute_reply.started":"2022-02-04T12:03:35.193351Z","shell.execute_reply":"2022-02-04T12:03:35.198719Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"test_vectors = embeddings.get_vectors(*words_to_plot)\nprint(test_vectors.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.066396Z","iopub.execute_input":"2022-02-04T11:36:51.066823Z","iopub.status.idle":"2022-02-04T11:36:51.071598Z","shell.execute_reply.started":"2022-02-04T11:36:51.066794Z","shell.execute_reply":"2022-02-04T11:36:51.070674Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(test_vectors, test_words, how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:36:51.073064Z","iopub.execute_input":"2022-02-04T11:36:51.073538Z","iopub.status.idle":"2022-02-04T11:36:51.629141Z","shell.execute_reply.started":"2022-02-04T11:36:51.073497Z","shell.execute_reply":"2022-02-04T11:36:51.628564Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## Обучение Word2Vec с помощью Gensim","metadata":{}},{"cell_type":"code","source":"import gensim","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:44:19.927983Z","iopub.execute_input":"2022-02-04T10:44:19.928268Z","iopub.status.idle":"2022-02-04T10:44:20.058428Z","shell.execute_reply.started":"2022-02-04T10:44:19.928235Z","shell.execute_reply":"2022-02-04T10:44:20.057556Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"word2vec = gensim.models.Word2Vec(sentences=train_tokenized, vector_size=100,\n                                  window=5, min_count=1, workers=4,\n                                  sg=1, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:52:43.708833Z","iopub.execute_input":"2022-02-04T10:52:43.709575Z","iopub.status.idle":"2022-02-04T10:52:46.112627Z","shell.execute_reply.started":"2022-02-04T10:52:43.709529Z","shell.execute_reply":"2022-02-04T10:52:46.112008Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(word2vec.wv.most_similar('вклад'))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:07:18.420493Z","iopub.execute_input":"2022-02-04T12:07:18.420776Z","iopub.status.idle":"2022-02-04T12:07:18.448172Z","shell.execute_reply.started":"2022-02-04T12:07:18.420747Z","shell.execute_reply":"2022-02-04T12:07:18.447378Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"word2vec.wv.most_similar('парадигма')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:59:54.407567Z","iopub.execute_input":"2022-02-04T10:59:54.408150Z","iopub.status.idle":"2022-02-04T10:59:54.418853Z","shell.execute_reply.started":"2022-02-04T10:59:54.408109Z","shell.execute_reply":"2022-02-04T10:59:54.417926Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"word2vec.wv.most_similar('синергия')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:00:42.878770Z","iopub.execute_input":"2022-02-04T11:00:42.879070Z","iopub.status.idle":"2022-02-04T11:00:42.888131Z","shell.execute_reply.started":"2022-02-04T11:00:42.879037Z","shell.execute_reply":"2022-02-04T11:00:42.887199Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"word2vec.most_similar(positive=['man', 'queen'], negative=['king'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gensim_words = [w for w in test_words if w in word2vec.wv.key_to_index]\ngensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:17:20.385981Z","iopub.execute_input":"2022-02-04T11:17:20.386242Z","iopub.status.idle":"2022-02-04T11:17:20.414770Z","shell.execute_reply.started":"2022-02-04T11:17:20.386213Z","shell.execute_reply":"2022-02-04T11:17:20.413732Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(gensim_vectors, test_words, how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:17:16.494761Z","iopub.execute_input":"2022-02-04T11:17:16.495525Z","iopub.status.idle":"2022-02-04T11:17:16.513655Z","shell.execute_reply.started":"2022-02-04T11:17:16.495478Z","shell.execute_reply":"2022-02-04T11:17:16.512564Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка предобученного Word2Vec\n\nИсточники готовых векторов:\n\nhttps://rusvectores.org/ru/ - для русского языка\n\nhttps://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:13:24.084884Z","iopub.execute_input":"2022-02-04T11:13:24.085450Z","iopub.status.idle":"2022-02-04T11:13:24.089859Z","shell.execute_reply.started":"2022-02-04T11:13:24.085411Z","shell.execute_reply":"2022-02-04T11:13:24.088969Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"available_models = api.info()['models'].keys()\nprint('\\n'.join(available_models))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:13:24.402510Z","iopub.execute_input":"2022-02-04T11:13:24.402915Z","iopub.status.idle":"2022-02-04T11:13:24.861646Z","shell.execute_reply.started":"2022-02-04T11:13:24.402886Z","shell.execute_reply":"2022-02-04T11:13:24.860640Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pretrained = api.load('word2vec-ruscorpora-300')  # > 1.5 GB!","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:13:27.401907Z","iopub.execute_input":"2022-02-04T11:13:27.402181Z","iopub.status.idle":"2022-02-04T11:13:56.575192Z","shell.execute_reply.started":"2022-02-04T11:13:27.402152Z","shell.execute_reply":"2022-02-04T11:13:56.574376Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pretrained.most_similar('служение_NOUN')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:27:59.529267Z","iopub.execute_input":"2022-02-04T11:27:59.529550Z","iopub.status.idle":"2022-02-04T11:27:59.553409Z","shell.execute_reply.started":"2022-02-04T11:27:59.529522Z","shell.execute_reply":"2022-02-04T11:27:59.552575Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"pretrained.most_similar(positive=['man', 'queen'], negative=['king'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_words = [w for w in test_words if w in pretrained.key_to_index]\npretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:18:58.893509Z","iopub.execute_input":"2022-02-04T11:18:58.893834Z","iopub.status.idle":"2022-02-04T11:18:58.996961Z","shell.execute_reply.started":"2022-02-04T11:18:58.893790Z","shell.execute_reply":"2022-02-04T11:18:58.996031Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"[pretrained[w] for w in pretrained_words]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:19:49.115815Z","iopub.execute_input":"2022-02-04T11:19:49.116115Z","iopub.status.idle":"2022-02-04T11:19:49.122207Z","shell.execute_reply.started":"2022-02-04T11:19:49.116083Z","shell.execute_reply":"2022-02-04T11:19:49.121375Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)","metadata":{},"execution_count":null,"outputs":[]}]}