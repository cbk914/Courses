{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T14:07:46.851931Z","iopub.execute_input":"2022-02-08T14:07:46.852278Z","iopub.status.idle":"2022-02-08T14:07:46.874544Z","shell.execute_reply.started":"2022-02-08T14:07:46.852243Z","shell.execute_reply":"2022-02-08T14:07:46.873926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Итоговое задание семинара (3.3.Final)","metadata":{}},{"cell_type":"raw","source":"Формулировка из курса:\nВ качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы лучше понять свойства эмбеддингов и попробовать улучшить их качество. Что можно попробовать сделать:\n\n    - поиграться с параметрами - количеством отрицательных слов, размером батча, скоростью обучения, размером окна\n    - убрать разбиение текстов на предложения и увеличить окно\n    - изменить токенизацию, например, разобравшись с библиотекой SpaCy и подключив лемматизацию и POS-теггинг, чтобы строить эмбеддинги не для словоформ, а для лемм (например, chicked_NOUN)\n    - реализовать FastText и сравнить, как отличаются списки похожих документов, получаемых с помощью Word2Vec и FastText\n    - усложнить алгоритм оценки вероятности совместной встречаемости слов, например, заменив скалярное произведение на нейросеть с парой слоёв\n\nТакже мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы заставить Word2Vec выучить что-то интересное и полезное.\n\nОпишите то, что у Вас получилось, в ответе к этому шагу.\n\nБалл за этот шаг зачитывается автоматически, вне зависимости от текста, который Вы впишете в поле ответа :). Но то, насколько этот семинар и это задание будет полезным для Вас, вполне зависит - так что мы предлагаем Вам поисследовать возможности линейных моделей и подробно описать свой опыт. К тому же, после нажатия кнопки \"Отправить\" Вы получите доступ к ответам других участников и сможете обменяться своими находками.","metadata":{}},{"cell_type":"raw","source":"Мое индивидуальное задание:\n+ 1) сформировать датасет из выбранной мной русскоязычной книги (в формате txt)\n    + загрузить/сформировать из txt файла книги первоначальный корпус, где книга = корпус, строка (это в txt формате является обзатцем) = документ\n    + провести дальнейшую разбивку на предложения (на основе знаков препинания, ., ..., !, ? и т.п.), по итогу, книга = корпус, предложение = документ\n2) Обучить ембеддинги word2vec:\n    + через самописные функции из библиотеки dlnlputils\n    + через gensim\n3) Покрутить (с упором на принципы, ценности, общий окрас/особенности книги), выполнять циклично для разных вариантов моделей (далее), настроек, подходов к построению и т.п.:\n    + статистика по словам\n    + похожие слова\n    + сложение/вычитание\n    + карта схожести различных слов (особенно которые относятся к принципам)\n    + постараться определиться с основными метриками получаемых эмбеддингов (loss, применимо ли здесь в каком то виде accuracy и т.п.)\n    + сравнить косинусное расстояние одних и тех же слов у разных моделей\n    + добавить в список для сравнения слова антонимы (типа злость, ложь и т.п.)\n4) Модификации основной модели (из библиотеки dlnlputils):\n    + поиграться с параметрами - количеством отрицательных слов, размером батча, скоростью обучения, размером окна (обязательно задать параметры одной из моделей, аналогичные тем, что применяются в бейзлайне gensim)\n    + убрать разбиение текстов на предложения и увеличить окно\n    + изменить токенизацию, например, разобравшись с библиотекой SpaCy и подключив лемматизацию и POS-теггинг, чтобы строить эмбеддинги не для словоформ, а для лемм (например, chicked_NOUN)\n    + реализовать FastText и сравнить, как отличаются списки похожих документов, получаемых с помощью Word2Vec и FastText (реализовать 2 варианта, свой и gensim)\n    - усложнить алгоритм оценки вероятности совместной встречаемости слов, например, заменив скалярное произведение на нейросеть с парой слоёв\n    ","metadata":{}},{"cell_type":"markdown","source":"### Код из библиотеки dlnlputils репозитория https://github.com/Samsung-IT-Academy/stepik-dl-nlp","metadata":{}},{"cell_type":"code","source":"#########stepik-dl-nlp/dlnlputils/data/base.py#########\n\nimport collections\nimport re\n\nimport numpy as np\n\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef tokenize_corpus_verbose(texts, tokenizer=tokenize_text_simple_regex, verbose_chunk=1000, **tokenizer_kwargs):\n    tokenize_texts = []\n    for i, text in enumerate(texts):\n        tokenize_texts.append(tokenizer(text, **tokenizer_kwargs))\n        if i % verbose_chunk == 0:\n            print('Complete: {}/{}'.format(i,len(texts)))\n    return tokenize_texts\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # посчитать количество документов, в которых употребляется каждое слово\n    # а также общее количество документов\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # убрать слишком редкие и слишком частые слова\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # отсортировать слова по убыванию частоты\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # нумеруем слова\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # нормируем частоты слов\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq\n\n\n\n#########stepik-dl-nlp/dlnlputils/data/bag_of_words.py#########\n\nimport numpy as np\nimport scipy.sparse\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n    #modified by me \n    #add 'lftidf', 'tflidf', 'ltflidf', 'ltf', 'lidf'\n    \n    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'tflidf', 'tflidf_v2', 'ltf', 'tfpmi'}\n\n    # считаем количество употреблений каждого слова в каждом документе\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in word2id:\n                result[text_i, word2id[token]] += 1\n\n    # получаем бинарные вектора \"встречается или нет\"\n    if mode == 'bin':\n        result = (result > 0).astype('float32')\n\n    # получаем вектора относительных частот слова в документе\n    elif mode == 'tf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n\n    # полностью убираем информацию о количестве употреблений слова в данном документе,\n    # но оставляем информацию о частотности слова в корпусе в целом\n    elif mode == 'idf':\n        result = (result > 0).astype('float32').multiply(1 / word2freq)\n\n    # учитываем всю информацию, которая у нас есть:\n    # частоту слова в документе и частоту слова в корпусе\n    elif mode == 'tfidf':\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n\n    elif mode == 'ltf': # lTF=ln⁡(TF+1)\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n \n    elif mode == 'lidf': # lIDF=ln⁡(n/IDF+1)\n        result = (result > 0).astype('float32').multiply(len(tokenized_texts) / word2freq)\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n\n        \n    elif mode == 'ltfidf': # lTFIDF=ln⁡(TF+1)⋅IDF\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n        result = result.multiply(1 / word2freq) # разделить каждый столбец на вес слова\n        \n\n    elif mode == 'tflidf': # lTFIDF=TF⋅ln⁡(1/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(1 / word2freq + 1)) # разделить каждый столбец на вес слова\n\n    elif mode == 'tflidf_v2': # lTFIDF=TF⋅ln⁡(n/IDF+1)\n        result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n        result = result.multiply(1/result.sum(1)) # разделить каждую строку на её длину\n        result = result.multiply(np.log(len(tokenized_texts) / word2freq + 1)) # разделить каждый столбец на вес слова\n        \n    elif mode == 'tfpmi': # TFPMI=TF⋅PMI\n        result = result.tocsr()\n        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n        result = result.multiply(word2freq)  # домножить каждую строку на word2freq (это массив PMI Scores)\n\n    if scale:\n        result = result.tocsc()\n        result -= result.min()\n        result /= (result.max() + 1e-6)\n\n    return result.tocsr()\n\n\nclass SparseFeaturesDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \n    \n#########stepik-dl-nlp/dlnlputils/pipeline.py#########\n\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\n\ndef init_random_seed(value=0):\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.cuda.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n\n\ndef copy_data_to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, (list, tuple)):\n        return [copy_data_to_device(elem, device) for elem in data]\n    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n\n\ndef print_grad_stats(model):\n    mean = 0\n    std = 0\n    norm = 1e-5\n    for param in model.parameters():\n        grad = getattr(param, 'grad', None)\n        if grad is not None:\n            mean += grad.data.abs().mean()\n            std += grad.data.std()\n            norm += 1\n    mean /= norm\n    std /= norm\n    print(f'Mean grad {mean}, std {std}, n {norm}')\n\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0,\n                    best_acc_type = 'loss',\n                    test_dataset = None,\n                    experiment_name = 'NoName',\n                    no_calculate_accuracy = False):\n    \"\"\"\n    v2.1\n    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n    :param model: torch.nn.Module - обучаемая модель\n    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n    :param criterion: функция потерь для настройки модели\n    :param lr: скорость обучения\n    :param epoch_n: максимальное количество эпох\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n        отсутствие улучшения модели, чтобы обучение продолжалось.\n    :param l2_reg_alpha: коэффициент L2-регуляризации\n    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n        (по умолчанию torch.utils.data.DataLoader)\n    :return: кортеж из двух элементов:\n        - среднее значение функции потерь на валидации на лучшей эпохе\n        - лучшая модель\n    \"\"\"\n    \n    '''\n    modified by wisoffe\n    best_acc_type: 'loss' or 'acc'\n    experiment_name: \n    '''\n    assert best_acc_type in {'loss', 'acc'}\n    \n    train_start_time = datetime.datetime.now()\n    print(\"############## Start experiment with name: {} ##############\".format(experiment_name))\n    \n    #statistics history\n    history = {'acc': {'train': [0.0],\n                       'val': [0.0]},\n               'loss': {'train': [float('inf')],\n                       'val': [float('inf')]}}\n    \n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    device = torch.device(device)\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n    \n    if best_acc_type == 'loss': #отбираем модель по минимальному loss\n        best_val_metric = float('inf')\n    elif best_acc_type == 'acc': #отбираем модель по максимальному accuracy\n        best_val_metric = float('-inf')\n        \n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n    \n    \n    for epoch_i in range(1, epoch_n + 1):\n        try:\n            #####train phase######\n            epoch_start = datetime.datetime.now()\n            train_accuracy_epoch = [] #for statistics\n            train_loss_epoch = [] #for statistics\n            \n            model.train()\n            \n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    print('Threshold max_batches_per_epoch_train exceeded!')\n                    break\n\n                batch_x = copy_data_to_device(batch_x, device)\n                batch_y = copy_data_to_device(batch_y, device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                train_loss_epoch.append(float(loss))\n                \n                if not no_calculate_accuracy:\n                    train_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                    #train_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                else: train_accuracy_epoch.append(0.)\n                    \n            \n            #####validation phase######\n            model.eval()\n\n            val_accuracy_epoch = [] #for statistics\n            val_loss_epoch = [] #for statistics\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        print('Threshold max_batches_per_epoch_val exceeded!')\n                        break\n\n                    batch_x = copy_data_to_device(batch_x, device)\n                    batch_y = copy_data_to_device(batch_y, device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n                    \n                    if not no_calculate_accuracy:\n                        val_accuracy_epoch.append(float((pred.argmax(dim=1) == batch_y.data).float().mean().data))\n                        #val_accuracy_epoch.append(float((pred.detach().cpu().numpy().argmax(-1) == batch_y.detach().cpu().numpy()).mean()))\n                    else:\n                        val_accuracy_epoch.append(0.)\n                    val_loss_epoch.append(float(loss))\n\n            \n            ########ending of epoch#########\n            \n            history['acc']['train'].append(sum(train_accuracy_epoch) / len(train_accuracy_epoch))\n            history['loss']['train'].append(sum(train_loss_epoch) / len(train_loss_epoch))  \n\n            history['acc']['val'].append(sum(val_accuracy_epoch) / len(val_accuracy_epoch))\n            history['loss']['val'].append(sum(val_loss_epoch) / len(val_loss_epoch))\n            \n            \n            #save best model\n            best_model_saved = False\n            if (best_acc_type == 'loss' and history['loss']['val'][-1] < best_val_metric) or \\\n                    (best_acc_type == 'acc' and history['acc']['val'][-1] > best_val_metric):\n                #отбираем модель по минимальному loss или максимальному accuracy\n                best_epoch_i = epoch_i\n                best_val_metric = history[best_acc_type]['val'][-1]\n                best_model = copy.deepcopy(model)\n                best_model_saved = True\n            #check for break training\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(history['loss']['val'][-1])\n            \n            #output statistics\n            \n            print('Epoch = {:>3},   ACC: val = {:.3f}, train = {:.3f}    LOSS: val = {:.3f}, train = {:.3f}   SAVE: {}, Time: {:0.2f}s'\\\n                  .format(epoch_i,\n                          history['acc']['val'][-1], \n                          history['acc']['train'][-1],\n                          history['loss']['val'][-1],\n                          history['loss']['train'][-1],\n                          best_model_saved,\n                          (datetime.datetime.now() - epoch_start).total_seconds()),\n                  flush=True)\n\n        except KeyboardInterrupt:\n            print('Досрочно остановлено пользователем')\n            break\n        except Exception as ex:\n            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n            \n    print(' ')\n    print(\"BEST MODEL: ACC: val = {:.3f}, train = {:.3f}, LOSS: val = {:.3f}, train = {:.3f}, on epoch = {}, metric type = {}, Full train time = {:0.2f}s\"\\\n                  .format(history['acc']['val'][best_epoch_i], \n                          history['acc']['train'][best_epoch_i],\n                          history['loss']['val'][best_epoch_i],\n                          history['loss']['train'][best_epoch_i],\n                          best_epoch_i,\n                          best_acc_type,\n                          (datetime.datetime.now() - train_start_time).total_seconds()))\n    print(\"************** End experiment with name: {} **************\".format(experiment_name))\n    print(' ')\n    history['BEST'] = {}\n    history['BEST']['epoch'] = best_epoch_i\n    history['BEST']['dict_size'] = batch_x.shape[-1]\n    \n    \n    #calculate and save final metrics best_model on train/val/test datasets\n    if test_dataset is not None:\n        history['BEST']['acc'] = {}\n        history['BEST']['loss'] = {}\n        \n        #save validation metrics (no calculate again)\n        history['BEST']['acc']['val'] = history['acc']['val'][best_epoch_i]\n        history['BEST']['loss']['val'] = history['loss']['val'][best_epoch_i]\n        \n        #calculate and save train metrics\n        train_pred = predict_with_model(best_model, train_dataset, return_labels=True)\n        history['BEST']['loss']['train'] = float(F.cross_entropy(torch.from_numpy(train_pred[0]),\n                             torch.from_numpy(train_pred[1]).long()))\n        history['BEST']['acc']['train'] = accuracy_score(train_pred[1], train_pred[0].argmax(-1))\n        \n        #calculate and save test metrics\n        test_pred = predict_with_model(best_model, test_dataset, return_labels=True)\n        history['BEST']['loss']['test'] = float(F.cross_entropy(torch.from_numpy(test_pred[0]),\n                             torch.from_numpy(test_pred[1]).long()))\n        history['BEST']['acc']['test'] = accuracy_score(test_pred[1], test_pred[0].argmax(-1))    \n    \n    \n    return history, best_model\n\n\ndef predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n    \"\"\"\n    :param model: torch.nn.Module - обученная модель\n    :param dataset: torch.utils.data.Dataset - данные для применения модели\n    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n    :return: numpy.array размерности len(dataset) x *\n    \"\"\"\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    results_by_batch = []\n\n    device = torch.device(device)\n    model.to(device)\n    model.eval()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    labels = []\n    with torch.no_grad():\n        import tqdm\n        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n            batch_x = copy_data_to_device(batch_x, device)\n\n            if return_labels:\n                labels.append(batch_y.numpy())\n\n            batch_pred = model(batch_x)\n            results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n    if return_labels:\n        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n    else:\n        return np.concatenate(results_by_batch, 0)\n\n\n#########stepik-dl-nlp/dlnlputils/nnets.py#########\n\nfrom torch.utils.data import Dataset\n\n\ndef ensure_length(txt, out_len, pad_value):\n    if len(txt) < out_len:\n        txt = list(txt) + [pad_value] * (out_len - len(txt))\n    else:\n        txt = txt[:out_len]\n    return txt\n\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, texts, targets, out_len=100, pad_value=0):\n        self.texts = texts\n        self.targets = targets\n        self.out_len = out_len\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n\n#########stepik-dl-nlp/dlnlputils/embeddings.py#########\n\nclass Embeddings:\n    def __init__(self, embeddings, word2id):\n        self.embeddings = embeddings\n        self.embeddings /= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n        self.word2id = word2id\n        self.id2word = {i: w for w, i in word2id.items()}\n\n    def most_similar(self, positive=None, negative=None, topk=10, with_mean = False):\n        #modified by wis, converted to gensim syntax\n        \n        if positive is not None:\n            if type(positive) != list:\n                positive = [positive]\n            pos_vec = [self.get_vector(word) for word in positive]\n            pos_len = len(positive)\n        else:\n            pos_vec = 0\n            pos_len = 1\n            \n        if negative is not None:\n            if type(negative) != list:\n                negative = [negative]\n            neg_vec = [self.get_vector(word) for word in negative]\n            neg_len = len(negative)\n        else:\n            neg_vec = 0\n            neg_len = 1\n        \n        if with_mean:\n            result_vec = np.array(pos_vec).sum(0) / pos_len - np.array(neg_vec).sum(0) / neg_len\n        else:\n            result_vec = np.array(pos_vec).sum(0) - np.array(neg_vec).sum(0)\n        \n        return self.most_similar_by_vector(result_vec, topk=topk)\n    \n    def most_similar_legacy(self, word, topk=10):\n        return self.most_similar_by_vector(self.get_vector(word), topk=topk)\n\n    def analogy(self, a1, b1, a2, topk=10):\n        a1_v = self.get_vector(a1)\n        b1_v = self.get_vector(b1)\n        a2_v = self.get_vector(a2)\n        query = b1_v - a1_v + a2_v\n        return self.most_similar_by_vector(query, topk=topk)\n\n    def most_similar_by_vector(self, query_vector, topk=10):\n        similarities = (self.embeddings * query_vector).sum(-1)\n        best_indices = np.argpartition(-similarities, topk, axis=0)[:topk]\n        result = [(self.id2word[i], similarities[i]) for i in best_indices]\n        result.sort(key=lambda pair: -pair[1])\n        return result\n\n    def get_vector(self, word):\n        if word not in self.word2id:\n            raise ValueError('Неизвестное слово \"{}\"'.format(word))\n        return self.embeddings[self.word2id[word]]\n\n    def get_vectors(self, *words):\n        word_ids = [self.word2id[i] for i in words]\n        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n        return vectors\n\n#########stepik-dl-nlp/dlnlputils/visualization.py#########\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\n\ndef plot_vectors(vectors, labels, how='tsne', ax=None, xy_lim=None):\n    if how == 'tsne':\n        projections = TSNE().fit_transform(vectors)\n    elif how == 'svd':\n        projections = TruncatedSVD().fit_transform(vectors)\n\n    x = projections[:, 0]\n    y = projections[:, 1]\n    if xy_lim is not None:\n        ax.set_xlim(xy_lim)\n        ax.set_ylim(xy_lim)\n    ax.scatter(x, y)\n    for cur_x, cur_y, cur_label in zip(x, y, labels):\n        ax.annotate(cur_label, (cur_x, cur_y))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:11.463551Z","iopub.execute_input":"2022-02-15T10:32:11.463761Z","iopub.status.idle":"2022-02-15T10:32:11.582136Z","shell.execute_reply.started":"2022-02-15T10:32:11.463736Z","shell.execute_reply":"2022-02-15T10:32:11.581341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Мои наработки","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport spacy\n!python -m spacy download ru_core_news_md\nspacy_nlp = spacy.load('ru_core_news_md', disable=['parser', 'ner'])\n\ndef tokenize_text_spacy_lemmatize(txt, spacy_nlp, min_token_size=4, with_pos = True, remove_stopwords = False):\n    doc = spacy_nlp(txt)\n    \n    if remove_stopwords:\n        lemmatized_doc = [token for token in doc if (len(token) >= min_token_size) and (not token.is_stop)]\n    else:\n        lemmatized_doc = [token for token in doc if len(token) >= min_token_size]\n    \n    if with_pos:\n        return ['_'.join([token.lemma_, token.pos_]) for token in lemmatized_doc]\n    else:\n        return [token.lemma_ for token in lemmatized_doc]\n\ndef tokenize_corpus_convert(tokenized_corpus, converter, addition = False):\n    '''\n    Convert each token in tokenized_corpus by converter\n    \n    Sample (PorterStemmer):\n    import nltk\n    ps = nltk.stemmer.PorterStemmer()\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=ps.stem)\n    \n    Sample (SnowballStemmer):\n    import nltk\n    sno = nltk.stem.SnowballStemmer('english')\n    tokenized_stemmed_corpus = tokenize_corpus_convert(tokenized_corpus, converter=sno.stem)\n    \n    Sample (WordNetLemmatizer):\n    import nltk\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    tokenized_lemmas_corpus = tokenize_corpus_convert(tokenized_corpus, converter=lemma.lemmatize)\n    '''\n    output = []\n    if not addition: #возвращаем только преобразованные токены\n        for doc in tokenized_corpus:\n            output.append([converter(token) for token in doc])\n    else: #возвращаем списк из исходных токенов, дополненных списком преобразованных\n        for doc in tokenized_corpus:\n            output.append(doc + [converter(token) for token in doc])        \n    return output\n\ndef show_experiments_stats(histories, figsize = (16.0, 6.0), show_plots = True, only_BEST_MODEL_CALC = False):\n    matplotlib.rcParams['figure.figsize'] = figsize\n    \n    for experiment_id in histories.keys():\n        print('{:-<100}'.format(experiment_id))\n        \n        if not only_BEST_MODEL_CALC:\n            epoch_max_acc = np.array(histories[experiment_id]['acc']['val']).argmax()\n            print('Max val acc on:    Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_max_acc, \n                          histories[experiment_id]['acc']['val'][epoch_max_acc], \n                          histories[experiment_id]['acc']['train'][epoch_max_acc],\n                          histories[experiment_id]['loss']['val'][epoch_max_acc],\n                          histories[experiment_id]['loss']['train'][epoch_max_acc]))\n            epoch_min_loss = np.array(histories[experiment_id]['loss']['val']).argmin()\n            print('Min val loss on:   Epoch = {:>3},   ACCURACY: val  = {:.3f}, train = {:.3f},   LOSS: val  = {:.3f}, train = {:.3f}'\\\n                  .format(epoch_min_loss, \n                          histories[experiment_id]['acc']['val'][epoch_min_loss], \n                          histories[experiment_id]['acc']['train'][epoch_min_loss],\n                          histories[experiment_id]['loss']['val'][epoch_min_loss],\n                          histories[experiment_id]['loss']['train'][epoch_min_loss]))\n        \n        if 'acc' in histories[experiment_id]['BEST']:\n            print(\"BEST MODEL CALC:   Epoch = {:>3},   ACCURACY: test = {:.3f}, train = {:.3f},   LOSS: test = {:.3f}, train = {:.3f}  DICT SIZE = {}\"\\\n                  .format(histories[experiment_id]['BEST']['epoch'], \n                          histories[experiment_id]['BEST']['acc']['test'],\n                          histories[experiment_id]['BEST']['acc']['train'],\n                          histories[experiment_id]['BEST']['loss']['test'],\n                          histories[experiment_id]['BEST']['loss']['train'],\n                          histories[experiment_id]['BEST']['dict_size']))\n    \n    \n    if show_plots:\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n        plt.legend()\n        plt.title('Validation Accuracy (Val only)')\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['acc']['val'], label=experiment_id + ' val')\n            plt.plot(histories[experiment_id]['acc']['train'], label=experiment_id + ' train')\n        plt.legend()\n        plt.title('Validation Accuracy (Val/Train)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n        plt.legend()\n        plt.title('Validation Loss (Val only)');\n        plt.show()\n\n        for experiment_id in histories.keys():\n            plt.plot(histories[experiment_id]['loss']['val'], label=experiment_id  + ' val')\n            plt.plot(histories[experiment_id]['loss']['train'], label=experiment_id  + ' train')\n        plt.legend()\n        plt.title('Validation Loss (Val/Train)');\n        plt.show()\n\ndef run_most_sumilars(func_most_similars, words_list, verbose = True, **kwargs):\n    most_similars = {word: func_most_similars(word, **kwargs) for word in words_list}\n    if verbose:\n        for word, similars in most_similars.items():\n            print('{}:'.format(word))\n            print('\\n'.join(map(str,similars)))\n            print(' ')\n    return most_similars\n        \n\n#https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/4529901\nimport pickle\ndef save_object(obj, filename):\n    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n\ndef load_object(filename):\n    with open(filename, 'rb') as inp:\n        return pickle.load(inp)\n\n# sample usage\n#company1 = [1,2,3,4,5]\n#save_object(company1, '/kaggle/working/company1.pkl')\n#del company\n#company1 = load_object(filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:11.584528Z","iopub.execute_input":"2022-02-15T10:32:11.584794Z","iopub.status.idle":"2022-02-15T10:32:31.318388Z","shell.execute_reply.started":"2022-02-15T10:32:11.584766Z","shell.execute_reply":"2022-02-15T10:32:31.317381Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"WORDS_TO_SIMILAR = ['вклад', 'парадигма', 'лидерство', 'доверие', 'служение', 'семья', 'муж', 'мужчина', 'любовь', 'ненависть']\nWORDS_ANALOGY = [\n                {'positive': ['человек', 'лидер'], 'negative': ['совесть']}\n                ]\nWORDS_TO_PLOT = ['идея', 'один', 'между', 'который', 'отношения', 'могут', 'можно', \n                 'однако', 'человек', 'день', 'время', 'делать', 'должен', 'больше', \n                 'меньше', 'личность', 'характер', 'каждый', 'ненависть', 'лидерство',\n                 'принцип', 'совесть', 'парадигма', 'доверие', 'верность', 'мужество', \n                 'ответственность', 'скромность', 'терпение', 'простота', 'трудолюбие', \n                 'справедливость', 'служение', 'вклад', 'качество', 'любовь', 'зависимость', \n                 'взаимозависимость', 'забота', 'поддержка', 'простота', 'цельность',\n                 'честность', 'добросовестность', 'семья', 'муж', 'жена', 'супруг', 'мужчина']\n#Здесь речь шла о таких свойствах, как цельность личности, скромность, верность, умеренность, \n#мужество, справедливость, терпеливость, трудолюбие, простота,\n\nhistories = {}","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:31.319826Z","iopub.execute_input":"2022-02-15T10:32:31.320117Z","iopub.status.idle":"2022-02-15T10:32:31.329359Z","shell.execute_reply.started":"2022-02-15T10:32:31.320069Z","shell.execute_reply":"2022-02-15T10:32:31.328459Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Baseline","metadata":{}},{"cell_type":"markdown","source":"### Загрузка данных и подготовка корпуса","metadata":{}},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:16.287204Z","iopub.execute_input":"2022-02-15T11:09:16.287487Z","iopub.status.idle":"2022-02-15T11:09:16.300505Z","shell.execute_reply.started":"2022-02-15T11:09:16.287459Z","shell.execute_reply":"2022-02-15T11:09:16.299311Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\nBOOK_LANGUAGE='russian'\ncorpus_paragraphs = []\nfor _, _, filenames in os.walk(BOOK_DIRNAME):\n    for filename in filenames:\n        #print(os.path.join(BOOK_DIRNAME, filename))\n        with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n            corpus_paragraphs.extend([line.strip() for line in file])\n            \ncorpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\nprint(len(corpus_paragraphs))\ncorpus_paragraphs[200:205]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:16.633797Z","iopub.execute_input":"2022-02-15T11:09:16.634312Z","iopub.status.idle":"2022-02-15T11:09:16.759583Z","shell.execute_reply.started":"2022-02-15T11:09:16.634276Z","shell.execute_reply":"2022-02-15T11:09:16.758382Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#convert corpus of paragraphs to corpus of sentences\ncorpus_sentences = []\nfor paragraph in corpus_paragraphs:\n    corpus_sentences.extend(sent_tokenize(paragraph, language=BOOK_LANGUAGE))\n\nprint(len(corpus_sentences))\ncorpus_sentences[200:205]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:17.032950Z","iopub.execute_input":"2022-02-15T11:09:17.034230Z","iopub.status.idle":"2022-02-15T11:09:21.643117Z","shell.execute_reply.started":"2022-02-15T11:09:17.034156Z","shell.execute_reply":"2022-02-15T11:09:21.642307Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"random.shuffle(corpus_sentences)\ncorpus_sentences[:5]\n\nTRAIN_VAL_SPLIT = int(len(corpus_sentences) * 0.8)\ntrain_source = corpus_sentences[:TRAIN_VAL_SPLIT]\ntest_source = corpus_sentences[TRAIN_VAL_SPLIT:]\nprint(\"Обучающая выборка\", len(train_source))\nprint(\"Тестовая выборка\", len(test_source))\nprint()\nprint('\\n'.join(train_source[:5]))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:21.644604Z","iopub.execute_input":"2022-02-15T11:09:21.645198Z","iopub.status.idle":"2022-02-15T11:09:21.741406Z","shell.execute_reply.started":"2022-02-15T11:09:21.645153Z","shell.execute_reply":"2022-02-15T11:09:21.740764Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# токенизируем\ntrain_tokenized = tokenize_corpus(train_source, min_token_size = 3)\ntest_tokenized = tokenize_corpus(test_source, min_token_size = 3)\nprint('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:21.742488Z","iopub.execute_input":"2022-02-15T11:09:21.742818Z","iopub.status.idle":"2022-02-15T11:09:23.815314Z","shell.execute_reply.started":"2022-02-15T11:09:21.742789Z","shell.execute_reply":"2022-02-15T11:09:23.814404Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# строим словарь\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=2, pad_word='<PAD>')\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:23.817316Z","iopub.execute_input":"2022-02-15T11:09:23.817580Z","iopub.status.idle":"2022-02-15T11:09:24.221610Z","shell.execute_reply.started":"2022-02-15T11:09:23.817549Z","shell.execute_reply":"2022-02-15T11:09:24.220640Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\nprint('\\n'.join(' '.join(str(t) for t in sent)\n                for sent in train_token_ids[:10]))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:24.223092Z","iopub.execute_input":"2022-02-15T11:09:24.223400Z","iopub.status.idle":"2022-02-15T11:09:24.562163Z","shell.execute_reply.started":"2022-02-15T11:09:24.223357Z","shell.execute_reply":"2022-02-15T11:09:24.561337Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in train_token_ids], bins=20);\nplt.title('Гистограмма длин предложений');","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:24.563343Z","iopub.execute_input":"2022-02-15T11:09:24.563555Z","iopub.status.idle":"2022-02-15T11:09:25.154379Z","shell.execute_reply.started":"2022-02-15T11:09:24.563529Z","shell.execute_reply":"2022-02-15T11:09:25.153462Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in train_token_ids], bins=20);\nplt.yscale('log')\nplt.title('Гистограмма длин предложений');","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:25.155988Z","iopub.execute_input":"2022-02-15T11:09:25.156302Z","iopub.status.idle":"2022-02-15T11:09:26.270893Z","shell.execute_reply.started":"2022-02-15T11:09:25.156271Z","shell.execute_reply":"2022-02-15T11:09:26.270255Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"MAX_SENTENCE_LEN = 35\nsum(np.array([len(s) for s in train_token_ids]) <= MAX_SENTENCE_LEN), sum(np.array([len(s) for s in train_token_ids]) > MAX_SENTENCE_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:26.271976Z","iopub.execute_input":"2022-02-15T11:09:26.272448Z","iopub.status.idle":"2022-02-15T11:09:26.709321Z","shell.execute_reply.started":"2022-02-15T11:09:26.272393Z","shell.execute_reply":"2022-02-15T11:09:26.708543Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_dataset = PaddedSequenceDataset(train_token_ids,\n                                      np.zeros(len(train_token_ids)),\n                                      out_len=MAX_SENTENCE_LEN)\ntest_dataset = PaddedSequenceDataset(test_token_ids,\n                                     np.zeros(len(test_token_ids)),\n                                     out_len=MAX_SENTENCE_LEN)\nprint(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:26.710439Z","iopub.execute_input":"2022-02-15T11:09:26.710676Z","iopub.status.idle":"2022-02-15T11:09:26.727484Z","shell.execute_reply.started":"2022-02-15T11:09:26.710646Z","shell.execute_reply":"2022-02-15T11:09:26.726510Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## Алгоритм обучения - Skip Gram Negative Sampling\n\n**Skip Gram** - предсказываем соседние слова по центральному слову\n\n**Negative Sampling** - аппроксимация softmax\n\n$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n\n$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n\n$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n    \n$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$","metadata":{}},{"cell_type":"code","source":"def make_diag_mask(size, radius):\n    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n    idxs = torch.arange(size)\n    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n    return mask\n\nmake_diag_mask(10, 5)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:09:26.729558Z","iopub.execute_input":"2022-02-15T11:09:26.730318Z","iopub.status.idle":"2022-02-15T11:09:26.757156Z","shell.execute_reply.started":"2022-02-15T11:09:26.730278Z","shell.execute_reply":"2022-02-15T11:09:26.756411Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n\n* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n\n$$ \\\\ $$\n\n* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n\n$$ \\\\ $$\n\n$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$","metadata":{}},{"cell_type":"code","source":"class SkipGramNegativeSamplingTrainer(nn.Module):\n    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.negative_samples_n = negative_samples_n\n\n        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.center_emb.weight.data[0] = 0\n\n        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.context_emb.weight.data[0] = 0\n\n        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n    \n    def forward(self, sentences):\n        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n        batch_size = sentences.shape[0]\n        \n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding'и (это центральные слова)\n        #Итого(для batch=1): мы получаем тензор предложения фиксированной длины, где каждое слово \n        #заменено на embedding из центральных слов, все отсутсвующие слова (нет в словаре или \n        #закончилось реальное предложение), заменяются на embedding из 0\n        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n\n        \n        ### оценить сходство с настоящими соседними словами\n        \n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding'и, (это контекстные слова)\n        #дополнительно транспонируем для целей последующего тензорного (матричного) умножения \n        #Итого(для batch=1): мы получаем тензор предложения фиксированной длины, где каждое слово \n        #заменено на embedding из контекстных слов, все отсутсвующие слова (нет в словаре или \n        #закончилось реальное предложение), заменяются на embedding из 0\n        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n        \n        #перемножение тензоров, по сути скалярное произведение эмбеддингов, \n        #Важно отметить, что изначально я проедполагал, что эта операция равносильна нахождению косинусных расстояний, \n        #т.к. на основе анализа итоговых эмбеддингов, сделал неверный вывод, что длина каждого из векторов уже здесь = 1 \n        #(т.е. они сразу нормализуются в пределах каждого embedding (например внутри класса torch.nn.Embedding), \n        #но это не так, нормализация происходит уже после полного обучения модели, через передачу весов в конструктор \n        #созданного вручную класса Embedding)\n        #Итого(для batch=1): мы получаем матрицу MaxSentLength x MaxSentLength, скалярных произведений, \n        #между векторами каждого центрального слова и каждого контекстного слова (значения [-inf; inf])\n        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n        \n        #преобразуем в \"условные вероятности\" через взятие сигмоиды, т.е. получаем как бы \n        #\"условные вероятности\" встретить пары слов вместе, по факту для каждой пары, скалярное произведение, \n        #обернутое в сигмоиду и как следствие в диапазон значений (0; 1)\n        positive_probs = torch.sigmoid(positive_sims)\n\n        \n        ### увеличить оценку вероятности встретить эти пары слов вместе\n        \n        #переводим тензор self.positive_sim_mask на тот же девайс, на котором positive_sims\n        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n        \n        #.expand_as - Expand this tensor to the same size as other. \n        #self.expand_as(other) is equivalent to self.expand(other.size())\n        #positive_probs * positive_mask - мы оставляем только позиции пересечения центральных слов в контекстными,\n        #все остальные позиции зануляются\n        #подсчитываем бинарную кросс энтропию вычисленных \"условных вероятностей\" (сигмоид) и целевых = 1 для всех\n        #пересечений центральных и контекстных слов, все остальные позиции в обоих матрицах = 0 \n        #Примечание: т.к. по умолчанию BCEloss в реализации torch высчитывает итоговое значение как 'mean', \n        #а не 'sum' из всех полученных, то количество 0 так же влияет на итоговое значение, имеет ли это какой \n        #то эффект, и измениться ли что то, если выставить reduction='sum', не очевидно и нужно проверять на практике\n        #Примечание: для всех позиций, которые занулены, их эмбеддинги соответсвуют эмбеддинг-вектору с idx=0, для\n        #для которого мы при создании мы указали паддинг nn.Embedding(..., padding_idx=0), это означает, что эти веса\n        #фиксированы, и не подлежат изменению через градиентных шаг\n        #\n        #Итого: важно понимать, что если бы оптимизировали только данную loss функцию, без отрицательных примеров, \n        #которые идут ниже, то, все сводилось бы к тому, что минимальное значение loss было бы, если бы мы все \n        #вектора (и центральных слов и контекстных) устремили бы в бесконечность, в одном направлении (например всем\n        #их весам присвоили бы значение inf или любые подобные варианты)\n        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n                                               positive_mask.expand_as(positive_probs))\n\n        \n        ### выбрать случайные \"отрицательные\" слова\n        negative_words = torch.randint(1, self.vocab_size,\n                                       size=(batch_size, self.negative_samples_n),\n                                       device=sentences.device)  # Batch x NegSamplesN\n        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n        \n        ### уменьшить оценку вероятность встретить эти пары слов вместе\n        #Важно отметить, что BCEWithLogitsLoss расвносильна последовательному применению Sigmoid -> BCELoss\n        #но в реализации torch она является более численно стабильной, чем раздельное применение\n        #Итого: здесь все целевые (target) значения = 0, и если бы мы минимизировали только эту loss функцию, то минимальное\n        #ее значение было бы, если бы мы устремили все вектора центральных слов в бесконечность одного направления, \n        #а вектора контекстных слов в бесконечность противоположного направления\n        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n                                                           negative_sims.new_zeros(negative_sims.shape))\n\n        return positive_loss + negative_loss\n\n\ndef no_loss(pred, target):\n    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-12T11:19:36.546328Z","iopub.execute_input":"2022-02-12T11:19:36.546712Z","iopub.status.idle":"2022-02-12T11:19:36.566787Z","shell.execute_reply.started":"2022-02-12T11:19:36.546678Z","shell.execute_reply":"2022-02-12T11:19:36.565689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n#                                           radius=5, negative_samples_n=25)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.852347Z","iopub.execute_input":"2022-02-08T14:07:58.853003Z","iopub.status.idle":"2022-02-08T14:07:58.86729Z","shell.execute_reply.started":"2022-02-08T14:07:58.852944Z","shell.execute_reply":"2022-02-08T14:07:58.866287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_history, best_model = train_eval_loop(trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=1e-2,\n#                                             epoch_n=10,\n#                                             batch_size=64,\n#                                             device='cpu',\n#                                             early_stopping_patience=4,\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True),\n#                                             no_calculate_accuracy = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.868586Z","iopub.execute_input":"2022-02-08T14:07:58.870208Z","iopub.status.idle":"2022-02-08T14:07:58.884083Z","shell.execute_reply.started":"2022-02-08T14:07:58.870146Z","shell.execute_reply":"2022-02-08T14:07:58.883244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# torch.save(trainer.state_dict(), './sgns.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.885526Z","iopub.execute_input":"2022-02-08T14:07:58.886364Z","iopub.status.idle":"2022-02-08T14:07:58.897232Z","shell.execute_reply.started":"2022-02-08T14:07:58.886326Z","shell.execute_reply":"2022-02-08T14:07:58.895995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# trainer.load_state_dict(torch.load('./sgns.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.898551Z","iopub.execute_input":"2022-02-08T14:07:58.89878Z","iopub.status.idle":"2022-02-08T14:07:58.909422Z","shell.execute_reply.started":"2022-02-08T14:07:58.898745Z","shell.execute_reply":"2022-02-08T14:07:58.908384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Исследуем характеристики полученных векторов","metadata":{}},{"cell_type":"code","source":"# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.911313Z","iopub.execute_input":"2022-02-08T14:07:58.911842Z","iopub.status.idle":"2022-02-08T14:07:58.92291Z","shell.execute_reply.started":"2022-02-08T14:07:58.911809Z","shell.execute_reply":"2022-02-08T14:07:58.921943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR);","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.924412Z","iopub.execute_input":"2022-02-08T14:07:58.925097Z","iopub.status.idle":"2022-02-08T14:07:58.934789Z","shell.execute_reply.started":"2022-02-08T14:07:58.925062Z","shell.execute_reply":"2022-02-08T14:07:58.933462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(WORDS_ANALOGY[0])\n# embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.936489Z","iopub.execute_input":"2022-02-08T14:07:58.936721Z","iopub.status.idle":"2022-02-08T14:07:58.947407Z","shell.execute_reply.started":"2022-02-08T14:07:58.936692Z","shell.execute_reply":"2022-02-08T14:07:58.946549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #решение семантической пропорции — то есть задача аналогии\n# #word2 - word1 + word3\n# embeddings.analogy('совесть', 'лидер', 'человек')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.948803Z","iopub.execute_input":"2022-02-08T14:07:58.949073Z","iopub.status.idle":"2022-02-08T14:07:58.959146Z","shell.execute_reply.started":"2022-02-08T14:07:58.949042Z","shell.execute_reply":"2022-02-08T14:07:58.958137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(test_vectors, WORDS_TO_PLOT, how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.960481Z","iopub.execute_input":"2022-02-08T14:07:58.960722Z","iopub.status.idle":"2022-02-08T14:07:58.973877Z","shell.execute_reply.started":"2022-02-08T14:07:58.960691Z","shell.execute_reply":"2022-02-08T14:07:58.972671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение Word2Vec с помощью Gensim","metadata":{}},{"cell_type":"code","source":"exp_name = 'base_gensim'\n\nhyps = {\n    'min_token_size': 3,\n    'min_count': 2,\n    'emb_size': 100,\n    'rwindow': 5,\n}\n\nhistories[exp_name] = {\n    'hyps': hyps\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.975974Z","iopub.execute_input":"2022-02-08T14:07:58.976328Z","iopub.status.idle":"2022-02-08T14:07:58.987033Z","shell.execute_reply.started":"2022-02-08T14:07:58.976295Z","shell.execute_reply":"2022-02-08T14:07:58.985881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:58.988735Z","iopub.execute_input":"2022-02-08T14:07:58.989102Z","iopub.status.idle":"2022-02-08T14:07:58.998079Z","shell.execute_reply.started":"2022-02-08T14:07:58.989059Z","shell.execute_reply":"2022-02-08T14:07:58.997286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec = gensim.models.Word2Vec(sentences=train_tokenized, vector_size=100,\n                                  window=5, min_count=2, workers=4,\n                                  sg=1, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:07:59.003045Z","iopub.execute_input":"2022-02-08T14:07:59.003635Z","iopub.status.idle":"2022-02-08T14:08:18.501802Z","shell.execute_reply.started":"2022-02-08T14:07:59.003594Z","shell.execute_reply":"2022-02-08T14:08:18.500861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories[exp_name]['most_similars'] = run_most_sumilars(word2vec.wv.most_similar,WORDS_TO_SIMILAR)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:18.502996Z","iopub.execute_input":"2022-02-08T14:08:18.503233Z","iopub.status.idle":"2022-02-08T14:08:18.530142Z","shell.execute_reply.started":"2022-02-08T14:08:18.503204Z","shell.execute_reply":"2022-02-08T14:08:18.529288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(WORDS_ANALOGY[0])\nword2vec.wv.most_similar(**WORDS_ANALOGY[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:18.531896Z","iopub.execute_input":"2022-02-08T14:08:18.532509Z","iopub.status.idle":"2022-02-08T14:08:18.548548Z","shell.execute_reply.started":"2022-02-08T14:08:18.532464Z","shell.execute_reply":"2022-02-08T14:08:18.54447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gensim_words = [w for w in WORDS_TO_PLOT if w in word2vec.wv.key_to_index]\ngensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])\nhistories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(gensim_words, gensim_vectors)}\nfig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(gensim_vectors, gensim_words, how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:18.550417Z","iopub.execute_input":"2022-02-08T14:08:18.551161Z","iopub.status.idle":"2022-02-08T14:08:19.247464Z","shell.execute_reply.started":"2022-02-08T14:08:18.551114Z","shell.execute_reply":"2022-02-08T14:08:19.246623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка предобученного Word2Vec\n\nИсточники готовых векторов:\n\nhttps://rusvectores.org/ru/ - для русского языка\n\nhttps://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков","metadata":{}},{"cell_type":"code","source":"# import gensim.downloader as api","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.249087Z","iopub.execute_input":"2022-02-08T14:08:19.249552Z","iopub.status.idle":"2022-02-08T14:08:19.253463Z","shell.execute_reply.started":"2022-02-08T14:08:19.24951Z","shell.execute_reply":"2022-02-08T14:08:19.252673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# available_models = api.info()['models'].keys()\n# print('\\n'.join(available_models))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.254923Z","iopub.execute_input":"2022-02-08T14:08:19.255397Z","iopub.status.idle":"2022-02-08T14:08:19.266153Z","shell.execute_reply.started":"2022-02-08T14:08:19.255357Z","shell.execute_reply":"2022-02-08T14:08:19.265312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained = api.load('word2vec-ruscorpora-300')  # > 1.5 GB!","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.267566Z","iopub.execute_input":"2022-02-08T14:08:19.268051Z","iopub.status.idle":"2022-02-08T14:08:19.277609Z","shell.execute_reply.started":"2022-02-08T14:08:19.268009Z","shell.execute_reply":"2022-02-08T14:08:19.276624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained.most_similar('служение_NOUN')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.279222Z","iopub.execute_input":"2022-02-08T14:08:19.279532Z","iopub.status.idle":"2022-02-08T14:08:19.289518Z","shell.execute_reply.started":"2022-02-08T14:08:19.27949Z","shell.execute_reply":"2022-02-08T14:08:19.288708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained.most_similar(positive=['man', 'queen'], negative=['king'])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.291265Z","iopub.execute_input":"2022-02-08T14:08:19.291569Z","iopub.status.idle":"2022-02-08T14:08:19.302664Z","shell.execute_reply.started":"2022-02-08T14:08:19.291527Z","shell.execute_reply":"2022-02-08T14:08:19.301466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained_words = [w for w in test_words if w in pretrained.key_to_index]\n# pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.303905Z","iopub.execute_input":"2022-02-08T14:08:19.304335Z","iopub.status.idle":"2022-02-08T14:08:19.315882Z","shell.execute_reply.started":"2022-02-08T14:08:19.304304Z","shell.execute_reply":"2022-02-08T14:08:19.315183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [pretrained[w] for w in pretrained_words]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.317469Z","iopub.execute_input":"2022-02-08T14:08:19.318018Z","iopub.status.idle":"2022-02-08T14:08:19.329281Z","shell.execute_reply.started":"2022-02-08T14:08:19.317956Z","shell.execute_reply":"2022-02-08T14:08:19.328331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:08:19.330668Z","iopub.execute_input":"2022-02-08T14:08:19.331107Z","iopub.status.idle":"2022-02-08T14:08:19.342083Z","shell.execute_reply.started":"2022-02-08T14:08:19.331075Z","shell.execute_reply":"2022-02-08T14:08:19.341391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Эксперименты с различными параметрами","metadata":{}},{"cell_type":"raw","source":"ИТОГИ:\nПоказали себя лучше остальных:\nbatch=2048\nlr=1e-3\nrwindow=10\n\nПоказали себя хуже остальных:\nbatch=10000\nlr=1e-1\nrwindow=1\nneg_sampl=5_batch=10000\n\nА вот микс из 3-х лучших параметров дал просто отвратительные результаты:\nbatch=2048_lr=1e-3_rwindow=10","metadata":{}},{"cell_type":"code","source":"#hyperparameters\nexp_name = 'newbaseline'\nhyps = {\n    'min_token_size': 3,\n    'min_count': 2,\n    'max_doc_freq': 0.9, \n\n    'MAX_SENTENCE_LEN': 35,\n    'emb_size': 100,\n    'rwindow': 5,\n    \n    'neg_sampl_n': 25,\n    'lr': 1e-2,\n    'epoch_n': 50,\n    'batch': 2048,\n    'device': 'cpu',\n    'stop_pat': 4,\n    'lr_sched_pat': 1\n}\n    \nhistories[exp_name] = {\n    'hyps': hyps\n}\n\n# токенизируем\ntrain_tokenized = tokenize_corpus(train_source, min_token_size=hyps['min_token_size'])\ntest_tokenized = tokenize_corpus(test_source, min_token_size=hyps['min_token_size'])\nprint('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# строим словарь\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n                                             max_doc_freq=hyps['max_doc_freq'], \n                                             min_count=hyps['min_count'], pad_word='<PAD>')\nhistories[exp_name]['vocab_size'] = len(vocabulary)\n\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])\n\n# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# plt.hist([len(s) for s in train_token_ids], bins=20);\n# plt.title('Гистограмма длин предложений');\n\nprint('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n                                                sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n                                                sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\ntrain_dataset = PaddedSequenceDataset(train_token_ids,\n                                      np.zeros(len(train_token_ids)),\n                                      out_len=hyps['MAX_SENTENCE_LEN'])\ntest_dataset = PaddedSequenceDataset(test_token_ids,\n                                     np.zeros(len(test_token_ids)),\n                                     out_len=hyps['MAX_SENTENCE_LEN'])\n#print(train_dataset[0])\n\n### Обучение\ntrainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n                                          radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\nhistories[exp_name]['train_history'], best_model = train_eval_loop(\n                                            trainer,\n                                            train_dataset,\n                                            test_dataset,\n                                            no_loss,\n                                            lr=hyps['lr'],\n                                            epoch_n=hyps['epoch_n'],\n                                            batch_size=hyps['batch'],\n                                            device=hyps['device'],\n                                            early_stopping_patience=hyps['stop_pat'],\n                                            max_batches_per_epoch_train=10000,\n                                            max_batches_per_epoch_val=len(test_dataset),\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n                                                                                                                       patience=hyps['lr_sched_pat'], \n                                                                                                                       verbose=True),\n                                            no_calculate_accuracy = True)\n\n### Исследуем характеристики полученных векторов\nembeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\nprint('MOST_SIMILAR:')\nhistories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\nprint('WORDS_ANALOGY:')\nprint(WORDS_ANALOGY[0])\nprint('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\nprint('\\nWORDS_ON_PLOT:')\nwords_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\ntest_vectors = embeddings.get_vectors(*words_to_plot)\nprint(test_vectors.shape)\n\nhistories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\nfig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n             histories[exp_name]['test_vectors'].keys(), \n             how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:13:42.831888Z","iopub.execute_input":"2022-02-12T12:13:42.832335Z","iopub.status.idle":"2022-02-12T12:20:39.843345Z","shell.execute_reply.started":"2022-02-12T12:13:42.832295Z","shell.execute_reply":"2022-02-12T12:20:39.842449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'batch=2048'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем\n# train_tokenized = tokenize_corpus(train_source, min_token_size=hyps['min_token_size'])\n# test_tokenized = tokenize_corpus(test_source, min_token_size=hyps['min_token_size'])\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'batch=2048_rwindow=10'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 10,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем\n# train_tokenized = tokenize_corpus(train_source, min_token_size=hyps['min_token_size'])\n# test_tokenized = tokenize_corpus(test_source, min_token_size=hyps['min_token_size'])\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - убрать разбиение текстов на предложения и увеличить окно","metadata":{"execution":{"iopub.status.busy":"2022-02-09T13:07:50.538044Z","iopub.execute_input":"2022-02-09T13:07:50.538347Z","iopub.status.idle":"2022-02-09T13:07:50.542869Z","shell.execute_reply.started":"2022-02-09T13:07:50.538318Z","shell.execute_reply":"2022-02-09T13:07:50.542113Z"}}},{"cell_type":"raw","source":"ИТОГИ:\nПробовал\nno_sentenses_batch=2048\nno_sentenses_rwindow=8_batch=2048\nno_sentenses_rwindow=12_batch=2048\nno_sentenses_rwindow=15_MAX_SEN_LEN=70_batch=2048\n\nРезультаты двоякие, в чем то лучше, в чем то хуже, наиболее лучшими мне показались при окне 5 и 8 (но это не точно, все же сложно сравнивать)","metadata":{}},{"cell_type":"code","source":"# def make_diag_mask(size, radius, with_padding = False):\n#     \"\"\"Квадратная матрица размера Size x (Size - 2*radius)с двумя полосами ширины radius вдоль главной диагонали\n#        upd(wis) добавил возможность указать padding для целей задачи, когда мы не бьем текст по предложениям\"\"\"\n#     padding = radius if with_padding else 0\n#     idxs_col = torch.arange(size)\n#     idxs_row = torch.arange(size - padding * 2) + padding\n#     abs_idx_diff = (idxs_col.unsqueeze(0) - idxs_row.unsqueeze(1)).abs()\n#     mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n#     return mask\n\n# make_diag_mask(18, 3, True)\n\n\n# class SkipGramNegativeSamplingTrainer(nn.Module):\n#     #Modified for work with paddings\n#     def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5, with_padding=False):\n#         super().__init__()\n#         self.vocab_size = vocab_size\n#         self.negative_samples_n = negative_samples_n\n\n#         self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n#         self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n#         self.center_emb.weight.data[0] = 0\n\n#         self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n#         self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n#         self.context_emb.weight.data[0] = 0\n        \n#         self.positive_sim_mask = make_diag_mask(sentence_len, radius, with_padding=with_padding)\n        \n#         self.with_padding = with_padding\n#         self.padding = radius if with_padding else 0\n    \n#     def forward(self, sentences):\n#         \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n#         batch_size = sentences.shape[0]\n        \n#         #если мы используем padding, то нам не нужно брать в качестве центральных слов те, которые отстоят\n#         #от краев (начала/конца) на размер этого паддинга, т.е. вырезаем центр тензора без паддинга\n#         if self.with_padding:\n#             #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n#             #в котором индексы заменены на соответсвующие им embedding'и\n#             center_embeddings = self.center_emb(sentences[:,self.padding:-self.padding])  # Batch x (MaxSentLength-2*padding) x EmbSize\n#         else:\n#             center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n        \n\n#         # оценить сходство с настоящими соседними словами\n#         positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n#         positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x (MaxSentLength-2*padding) x (MaxSentLength-2*padding)\n#         positive_probs = torch.sigmoid(positive_sims)\n\n#         # увеличить оценку вероятности встретить эти пары слов вместе\n#         positive_mask = self.positive_sim_mask.to(positive_sims.device)\n#         positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n#                                                positive_mask.expand_as(positive_probs))\n\n#         # выбрать случайные \"отрицательные\" слова\n#         negative_words = torch.randint(1, self.vocab_size,\n#                                        size=(batch_size, self.negative_samples_n),\n#                                        device=sentences.device)  # Batch x NegSamplesN\n#         negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n#         negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x (MaxSentLength-2*padding) x NegSamplesN\n        \n#         # уменьшить оценку вероятность встретить эти пары слов вместе\n#         negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n#                                                            negative_sims.new_zeros(negative_sims.shape))\n\n#         return positive_loss + negative_loss\n\n\n# def no_loss(pred, target):\n#     \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n#     return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:01.769564Z","iopub.execute_input":"2022-02-10T14:53:01.769987Z","iopub.status.idle":"2022-02-10T14:53:01.793259Z","shell.execute_reply.started":"2022-02-10T14:53:01.769947Z","shell.execute_reply":"2022-02-10T14:53:01.79244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'no_sentenses_batch=2048'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n#     'padding': True,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\n# BOOK_LANGUAGE='russian'\n# corpus_paragraphs = []\n# for _, _, filenames in os.walk(BOOK_DIRNAME):\n#     for filename in filenames:\n#         #print(os.path.join(BOOK_DIRNAME, filename))\n#         with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n#             corpus_paragraphs.extend([line.strip() for line in file])\n            \n# corpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\n# print(len(corpus_paragraphs))\n# corpus_paragraphs[200:205]\n\n# #Для текущего эксперимента склеиваем все параграфы в единый текст.\n# plain_corpus = ' '.join(corpus_paragraphs)\n# #plain_corpus[1000:2000]\n\n# #convert plain corpus to corpus of blocks with padding\n# plain_corpus_tokenised = tokenize_corpus([plain_corpus], min_token_size=hyps['min_token_size'])[0]\n# print(len(plain_corpus_tokenised))\n\n# #partition into blocks\n# block_len = hyps['MAX_SENTENCE_LEN'] - 2 * hyps['rwindow']\n\n# corpus_blocks = []\n# corpus_blocks.append(plain_corpus_tokenised[:hyps['rwindow']]) #only for padding\n\n# for i in range(hyps['rwindow'], len(plain_corpus_tokenised) - block_len - hyps['rwindow']*2 + 1, block_len):\n#     corpus_blocks.append(plain_corpus_tokenised[i:i+block_len])\n\n# corpus_blocks.append(plain_corpus_tokenised[i:i+hyps['rwindow']]) #only for padding\n\n# print(len(corpus_blocks) - 2)\n\n# #add paddings\n# padding = hyps['rwindow']\n# for i in range(1, len(corpus_blocks) - 2):\n#     corpus_blocks[i] = corpus_blocks[i-1][-padding:] + corpus_blocks[i] + corpus_blocks[i+1][:padding]\n# corpus_blocks = corpus_blocks[1:-2]\n# #print(corpus_blocks[10:14])\n\n# #split to train/test\n# random.shuffle(corpus_blocks)\n\n# TRAIN_VAL_SPLIT = int(len(corpus_blocks) * 0.8)\n# train_tokenized = corpus_blocks[:TRAIN_VAL_SPLIT]\n# test_tokenized = corpus_blocks[TRAIN_VAL_SPLIT:]\n# print(\"Обучающая выборка\", len(train_tokenized))\n# print(\"Тестовая выборка\", len(test_tokenized))\n\n# # строим словарь, используем для построения train_tokenized с удаленными паддингами\n# vocabulary, word_doc_freq = build_vocabulary([block[padding:-padding] for block in train_tokenized], \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# print(train_dataset[0], len(train_dataset[0][0]))\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'], with_padding=True)\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:25.490018Z","iopub.execute_input":"2022-02-10T14:53:25.491242Z","iopub.status.idle":"2022-02-10T14:53:28.456342Z","shell.execute_reply.started":"2022-02-10T14:53:25.491142Z","shell.execute_reply":"2022-02-10T14:53:28.455048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'no_sentenses_rwindow=8_batch=2048'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 8,\n#     'padding': True,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\n# BOOK_LANGUAGE='russian'\n# corpus_paragraphs = []\n# for _, _, filenames in os.walk(BOOK_DIRNAME):\n#     for filename in filenames:\n#         #print(os.path.join(BOOK_DIRNAME, filename))\n#         with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n#             corpus_paragraphs.extend([line.strip() for line in file])\n            \n# corpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\n# print(len(corpus_paragraphs))\n# corpus_paragraphs[200:205]\n\n# #Для текущего эксперимента склеиваем все параграфы в единый текст.\n# plain_corpus = ' '.join(corpus_paragraphs)\n# #plain_corpus[1000:2000]\n\n# #convert plain corpus to corpus of blocks with padding\n# plain_corpus_tokenised = tokenize_corpus([plain_corpus], min_token_size=hyps['min_token_size'])[0]\n# print(len(plain_corpus_tokenised))\n\n# #partition into blocks\n# block_len = hyps['MAX_SENTENCE_LEN'] - 2 * hyps['rwindow']\n\n# corpus_blocks = []\n# corpus_blocks.append(plain_corpus_tokenised[:hyps['rwindow']]) #only for padding\n\n# for i in range(hyps['rwindow'], len(plain_corpus_tokenised) - block_len - hyps['rwindow']*2 + 1, block_len):\n#     corpus_blocks.append(plain_corpus_tokenised[i:i+block_len])\n\n# corpus_blocks.append(plain_corpus_tokenised[i:i+hyps['rwindow']]) #only for padding\n\n# print(len(corpus_blocks) - 2)\n\n# #add paddings\n# padding = hyps['rwindow']\n# for i in range(1, len(corpus_blocks) - 2):\n#     corpus_blocks[i] = corpus_blocks[i-1][-padding:] + corpus_blocks[i] + corpus_blocks[i+1][:padding]\n# corpus_blocks = corpus_blocks[1:-2]\n# #print(corpus_blocks[10:14])\n\n# #split to train/test\n# random.shuffle(corpus_blocks)\n\n# TRAIN_VAL_SPLIT = int(len(corpus_blocks) * 0.8)\n# train_tokenized = corpus_blocks[:TRAIN_VAL_SPLIT]\n# test_tokenized = corpus_blocks[TRAIN_VAL_SPLIT:]\n# print(\"Обучающая выборка\", len(train_tokenized))\n# print(\"Тестовая выборка\", len(test_tokenized))\n\n# # строим словарь, используем для построения train_tokenized с удаленными паддингами\n# vocabulary, word_doc_freq = build_vocabulary([block[padding:-padding] for block in train_tokenized], \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# print(train_dataset[0], len(train_dataset[0][0]))\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'], with_padding=True)\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:25.490018Z","iopub.execute_input":"2022-02-10T14:53:25.491242Z","iopub.status.idle":"2022-02-10T14:53:28.456342Z","shell.execute_reply.started":"2022-02-10T14:53:25.491142Z","shell.execute_reply":"2022-02-10T14:53:28.455048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'no_sentenses_rwindow=12_batch=2048'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 12,\n#     'padding': True,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\n# BOOK_LANGUAGE='russian'\n# corpus_paragraphs = []\n# for _, _, filenames in os.walk(BOOK_DIRNAME):\n#     for filename in filenames:\n#         #print(os.path.join(BOOK_DIRNAME, filename))\n#         with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n#             corpus_paragraphs.extend([line.strip() for line in file])\n            \n# corpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\n# print(len(corpus_paragraphs))\n# corpus_paragraphs[200:205]\n\n# #Для текущего эксперимента склеиваем все параграфы в единый текст.\n# plain_corpus = ' '.join(corpus_paragraphs)\n# #plain_corpus[1000:2000]\n\n# #convert plain corpus to corpus of blocks with padding\n# plain_corpus_tokenised = tokenize_corpus([plain_corpus], min_token_size=hyps['min_token_size'])[0]\n# print(len(plain_corpus_tokenised))\n\n# #partition into blocks\n# block_len = hyps['MAX_SENTENCE_LEN'] - 2 * hyps['rwindow']\n\n# corpus_blocks = []\n# corpus_blocks.append(plain_corpus_tokenised[:hyps['rwindow']]) #only for padding\n\n# for i in range(hyps['rwindow'], len(plain_corpus_tokenised) - block_len - hyps['rwindow']*2 + 1, block_len):\n#     corpus_blocks.append(plain_corpus_tokenised[i:i+block_len])\n\n# corpus_blocks.append(plain_corpus_tokenised[i:i+hyps['rwindow']]) #only for padding\n\n# print(len(corpus_blocks) - 2)\n\n# #add paddings\n# padding = hyps['rwindow']\n# for i in range(1, len(corpus_blocks) - 2):\n#     corpus_blocks[i] = corpus_blocks[i-1][-padding:] + corpus_blocks[i] + corpus_blocks[i+1][:padding]\n# corpus_blocks = corpus_blocks[1:-2]\n# #print(corpus_blocks[10:14])\n\n# #split to train/test\n# random.shuffle(corpus_blocks)\n\n# TRAIN_VAL_SPLIT = int(len(corpus_blocks) * 0.8)\n# train_tokenized = corpus_blocks[:TRAIN_VAL_SPLIT]\n# test_tokenized = corpus_blocks[TRAIN_VAL_SPLIT:]\n# print(\"Обучающая выборка\", len(train_tokenized))\n# print(\"Тестовая выборка\", len(test_tokenized))\n\n# # строим словарь, используем для построения train_tokenized с удаленными паддингами\n# vocabulary, word_doc_freq = build_vocabulary([block[padding:-padding] for block in train_tokenized], \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# print(train_dataset[0], len(train_dataset[0][0]))\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'], with_padding=True)\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:25.490018Z","iopub.execute_input":"2022-02-10T14:53:25.491242Z","iopub.status.idle":"2022-02-10T14:53:28.456342Z","shell.execute_reply.started":"2022-02-10T14:53:25.491142Z","shell.execute_reply":"2022-02-10T14:53:28.455048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'no_sentenses_rwindow=15_MAX_SEN_LEN=70_batch=2048'\n# hyps = {\n#     'min_token_size': 4,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 12,\n#     'padding': True,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 10,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n    \n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\n# BOOK_LANGUAGE='russian'\n# corpus_paragraphs = []\n# for _, _, filenames in os.walk(BOOK_DIRNAME):\n#     for filename in filenames:\n#         #print(os.path.join(BOOK_DIRNAME, filename))\n#         with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n#             corpus_paragraphs.extend([line.strip() for line in file])\n            \n# corpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\n# print(len(corpus_paragraphs))\n# corpus_paragraphs[200:205]\n\n# #Для текущего эксперимента склеиваем все параграфы в единый текст.\n# plain_corpus = ' '.join(corpus_paragraphs)\n# #plain_corpus[1000:2000]\n\n# #convert plain corpus to corpus of blocks with padding\n# plain_corpus_tokenised = tokenize_corpus([plain_corpus], min_token_size=hyps['min_token_size'])[0]\n# print(len(plain_corpus_tokenised))\n\n# #partition into blocks\n# block_len = hyps['MAX_SENTENCE_LEN'] - 2 * hyps['rwindow']\n\n# corpus_blocks = []\n# corpus_blocks.append(plain_corpus_tokenised[:hyps['rwindow']]) #only for padding\n\n# for i in range(hyps['rwindow'], len(plain_corpus_tokenised) - block_len - hyps['rwindow']*2 + 1, block_len):\n#     corpus_blocks.append(plain_corpus_tokenised[i:i+block_len])\n\n# corpus_blocks.append(plain_corpus_tokenised[i:i+hyps['rwindow']]) #only for padding\n\n# print(len(corpus_blocks) - 2)\n\n# #add paddings\n# padding = hyps['rwindow']\n# for i in range(1, len(corpus_blocks) - 2):\n#     corpus_blocks[i] = corpus_blocks[i-1][-padding:] + corpus_blocks[i] + corpus_blocks[i+1][:padding]\n# corpus_blocks = corpus_blocks[1:-2]\n# #print(corpus_blocks[10:14])\n\n# #split to train/test\n# random.shuffle(corpus_blocks)\n\n# TRAIN_VAL_SPLIT = int(len(corpus_blocks) * 0.8)\n# train_tokenized = corpus_blocks[:TRAIN_VAL_SPLIT]\n# test_tokenized = corpus_blocks[TRAIN_VAL_SPLIT:]\n# print(\"Обучающая выборка\", len(train_tokenized))\n# print(\"Тестовая выборка\", len(test_tokenized))\n\n# # строим словарь, используем для построения train_tokenized с удаленными паддингами\n# vocabulary, word_doc_freq = build_vocabulary([block[padding:-padding] for block in train_tokenized], \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# print(train_dataset[0], len(train_dataset[0][0]))\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'], with_padding=True)\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n# histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n# print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n# words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T14:53:25.490018Z","iopub.execute_input":"2022-02-10T14:53:25.491242Z","iopub.status.idle":"2022-02-10T14:53:28.456342Z","shell.execute_reply.started":"2022-02-10T14:53:25.491142Z","shell.execute_reply":"2022-02-10T14:53:28.455048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - изменить токенизацию, например, разобравшись с библиотекой SpaCy и подключив лемматизацию и POS-теггинг, чтобы строить эмбеддинги не для словоформ, а для лемм (например, chicked_NOUN)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_first_lemmas_by_word(word, lemmas2id):\n#         found = False\n#         for lemma in lemmas2id:\n#             if word + '_' in lemma:\n#                 found = True\n#                 break\n#         return lemma if found else None\n# def get_lemmas_by_words(list_words, lemmas2id):\n#     lemmas = [get_first_lemmas_by_word(word, lemmas2id) for word in list_words]\n#     return list(filter(lambda lem: lem != None, lemmas))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T11:18:46.986486Z","iopub.execute_input":"2022-02-11T11:18:46.98686Z","iopub.status.idle":"2022-02-11T11:18:46.995259Z","shell.execute_reply.started":"2022-02-11T11:18:46.986827Z","shell.execute_reply":"2022-02-11T11:18:46.993788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\n# BOOK_LANGUAGE='russian'\n# corpus_paragraphs = []\n# for _, _, filenames in os.walk(BOOK_DIRNAME):\n#     for filename in filenames:\n#         #print(os.path.join(BOOK_DIRNAME, filename))\n#         with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n#             corpus_paragraphs.extend([line.strip() for line in file])\n            \n# corpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\n# print(len(corpus_paragraphs))\n# corpus_paragraphs[200:205]\n\n# #convert corpus of paragraphs to corpus of sentences\n# corpus_sentences = []\n# for paragraph in corpus_paragraphs:\n#     corpus_sentences.extend(sent_tokenize(paragraph, language=BOOK_LANGUAGE))\n\n# print(len(corpus_sentences))\n# corpus_sentences[200:205]\n\n# random.shuffle(corpus_sentences)\n# corpus_sentences[:5]\n\n# TRAIN_VAL_SPLIT = int(len(corpus_sentences) * 0.8)\n# train_source = corpus_sentences[:TRAIN_VAL_SPLIT]\n# test_source = corpus_sentences[TRAIN_VAL_SPLIT:]\n# print(\"Обучающая выборка\", len(train_source))\n# print(\"Тестовая выборка\", len(test_source))\n# print()\n# print('\\n'.join(train_source[:5]))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:16:16.946264Z","iopub.execute_input":"2022-02-11T10:16:16.946762Z","iopub.status.idle":"2022-02-11T10:16:20.96628Z","shell.execute_reply.started":"2022-02-11T10:16:16.94668Z","shell.execute_reply":"2022-02-11T10:16:20.964832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_pos'\n# hyps = {\n#     'min_token_size': 3,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': True,\n#     'lem_remove_stopw': False,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_nopos'\n# hyps = {\n#     'min_token_size': 3,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': False,\n#     'lem_remove_stopw': False,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_pos_nostopwords'\n# hyps = {\n#     'min_token_size': 3,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': True,\n#     'lem_remove_stopw': True,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_nopos_nostopwords'\n# hyps = {\n#     'min_token_size': 3,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': False,\n#     'lem_remove_stopw': True,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_pos_min_token_size=2'\n# hyps = {\n#     'min_token_size': 2,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': True,\n#     'lem_remove_stopw': False,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #hyperparameters\n# exp_name = 'lemmas_pos_nostopwords_min_token_size=2'\n# hyps = {\n#     'min_token_size': 2,\n#     'min_count': 2,\n#     'max_doc_freq': 0.9, \n#     'lem_with_pos': True,\n#     'lem_remove_stopw': True,\n\n#     'MAX_SENTENCE_LEN': 35,\n#     'emb_size': 100,\n#     'rwindow': 5,\n    \n#     'neg_sampl_n': 25,\n#     'lr': 1e-2,\n#     'epoch_n': 50,\n#     'batch': 2048,\n#     'device': 'cpu',\n#     'stop_pat': 4,\n#     'lr_sched_pat': 1\n# }\n\n# histories[exp_name] = {\n#     'hyps': hyps\n# }\n\n# # токенизируем (с лемматизацией)\n# train_tokenized = tokenize_corpus_verbose(train_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n# test_tokenized = tokenize_corpus_verbose(test_source, tokenizer=tokenize_text_spacy_lemmatize, \n#                                           spacy_nlp=spacy_nlp, verbose_chunk=5000, \n#                                           min_token_size=hyps['min_token_size'], \n#                                           with_pos = hyps['lem_with_pos'], \n#                                           remove_stopwords = hyps['lem_remove_stopw'])\n\n# print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# # строим словарь\n# vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n#                                              max_doc_freq=hyps['max_doc_freq'], \n#                                              min_count=hyps['min_count'], pad_word='<PAD>')\n# histories[exp_name]['vocab_size'] = len(vocabulary)\n\n# print(\"Размер словаря\", len(vocabulary))\n# print(list(vocabulary.items())[:10])\n\n# # отображаем в номера токенов\n# train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n# test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n# # plt.hist([len(s) for s in train_token_ids], bins=20);\n# # plt.title('Гистограмма длин предложений');\n\n# print('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n#                                                 sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n#                                                 sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n# train_dataset = PaddedSequenceDataset(train_token_ids,\n#                                       np.zeros(len(train_token_ids)),\n#                                       out_len=hyps['MAX_SENTENCE_LEN'])\n# test_dataset = PaddedSequenceDataset(test_token_ids,\n#                                      np.zeros(len(test_token_ids)),\n#                                      out_len=hyps['MAX_SENTENCE_LEN'])\n# #print(train_dataset[0])\n\n# ### Обучение\n# trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n#                                           radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\n# histories[exp_name]['train_history'], best_model = train_eval_loop(\n#                                             trainer,\n#                                             train_dataset,\n#                                             test_dataset,\n#                                             no_loss,\n#                                             lr=hyps['lr'],\n#                                             epoch_n=hyps['epoch_n'],\n#                                             batch_size=hyps['batch'],\n#                                             device=hyps['device'],\n#                                             early_stopping_patience=hyps['stop_pat'],\n#                                             max_batches_per_epoch_train=10000,\n#                                             max_batches_per_epoch_val=len(test_dataset),\n#                                             lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n#                                                                                                                        patience=hyps['lr_sched_pat'], \n#                                                                                                                        verbose=True),\n#                                             no_calculate_accuracy = True)\n\n\n\n# ### Исследуем характеристики полученных векторов\n# embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)\n\n# print('MOST_SIMILAR:')\n\n# if hyps['lem_with_pos']:\n#     lemmas_to_similar = get_lemmas_by_words(WORDS_TO_SIMILAR, embeddings.word2id)\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,lemmas_to_similar)\n# else:\n#     histories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR)\n\n# print('WORDS_ANALOGY:')\n# print(WORDS_ANALOGY[0])\n\n# if hyps['lem_with_pos']:\n#     lemmas_analogy = []\n#     for analogy_query in WORDS_ANALOGY:\n#         lemmas_analogy.append(\n#         {\n#             'positive': get_lemmas_by_words(analogy_query['positive'], embeddings.word2id),\n#             'negative': get_lemmas_by_words(analogy_query['negative'], embeddings.word2id)\n#         })\n#     print('\\n'.join(map(str,embeddings.most_similar(**lemmas_analogy[0], topk=10, with_mean=True))))\n# else:\n#     print('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\n# print('\\nWORDS_ON_PLOT:')\n\n# if hyps['lem_with_pos']:\n#     words_to_plot = get_lemmas_by_words(WORDS_TO_PLOT, embeddings.word2id)\n# else:\n#     words_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\n# test_vectors = embeddings.get_vectors(*words_to_plot)\n# print(test_vectors.shape)\n\n# histories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(words_to_plot, test_vectors)}\n\n# fig, ax = plt.subplots()\n# fig.set_size_inches((10, 10))\n# plot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n#              histories[exp_name]['test_vectors'].keys(), \n#              how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:28:28.545077Z","iopub.execute_input":"2022-02-11T10:28:28.545415Z","iopub.status.idle":"2022-02-11T10:38:59.482087Z","shell.execute_reply.started":"2022-02-11T10:28:28.545375Z","shell.execute_reply":"2022-02-11T10:38:59.480474Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - реализовать FastText и сравнить, как отличаются списки похожих документов, получаемых с помощью Word2Vec и FastText","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:40:42.164435Z","iopub.execute_input":"2022-02-11T12:40:42.164805Z","iopub.status.idle":"2022-02-11T12:40:42.169691Z","shell.execute_reply.started":"2022-02-11T12:40:42.164768Z","shell.execute_reply":"2022-02-11T12:40:42.168335Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Общий алгоритм обучения FastText Skip Gram Negative Sampling выглядит следующим образом:\n\n    - Очистить и токенизировать обучающую коллекцию документов\n    - Построить словарь - подсчитать частоты всех целых токенов и N-грамм заданной длины (например, от 3 до 6 символов). При построении словаря раз в заданное число шагов прореживать словарь - удалить из словаря токены, набравшие с предыдущего прореживания меньше всего употреблений (или меньше заданного порога).\n    - Проход по корпусу скользящим окном заданной ширины, для каждой позиции окна выполнять шаги 4-7.\n    - Для текущего словоупотребления в центре окна выделить его N-граммы, содержащиеся в словаре (то есть только достаточно частотные N-граммы)\n    - Вычислить вектор центрального токена, усреднив вектора целого токена (если он есть в словаре) и всех N-грамм, выделенных на шаге 4.\n    - Выбрать случайным образом отрицательные слова (сделать negative sampling).\n    - Обновить следущие вектора так, чтобы улучшить оценку правдоподобия:\n        -- N-грамм, участвовавших в получении вектора центрального токена,\n        -- контекстные вектора всех токенов в окне, кроме центрального,\n        -- контекстные вектора отрицательных слов.\n    - Повторять шаги 3-7 заданное число раз или до сходимости.\n\nВнимание! FastText учитывает само центральное слово как n-грамму, только если оно достаточно частотное.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sym_ngrams(token, size=(3, 6)):\n    ngram = [token]\n    leng = len(token)\n    if leng > size[0]:\n        for l in range(size[0], size[1]+1):\n            ngram.extend([token[i:i+l] for i in range(0, leng - l + 1)])\n    return ngram\n\ndef convert_corpus_to_sym_ngramms(corpus_tokenized, size=(3, 6)):\n    #convert corpus_tokenized to corpus_sym_ngramms\n        return [sum([get_sym_ngrams(token, size=size) for token in sent], []) for sent in corpus_tokenized]\n    \ndef get_ngrams_idx(token, vocabulary_ngram, size=(3, 6)):\n    if token in vocabulary_ngram:\n        ngrams_idx = [vocabulary_ngram[token]]\n    else:\n        ngrams_idx = []\n    leng = len(token)\n    if leng > size[0]:\n        for l in range(size[0], size[1]+1):\n            ngrams_idx.extend([vocabulary_ngram[token[i:i+l]] for i in range(0, leng - l + 1) if token[i:i+l] in vocabulary_ngram])\n    return ngrams_idx if ngrams_idx != [] else [0]\n\ndef get_corresp_voc_vocngram(vocabulary, vocabulary_ngram, size=(3, 6)):\n    corresp = []\n    for word, word_idx in vocabulary.items():\n        corresp.append(get_ngrams_idx(word,vocabulary_ngram, size=size))\n    corresp[0] = [0]\n    return corresp\n\ndef convert_vocab_ngrams(corresp_vocngram, center_ngram_emb):\n    center_embeddings = center_ngram_emb(torch.LongTensor(corresp_vocngram)[:,1:])  # VocabSize x MaxNgrams x MaxEmbSize\n    center_embeddings = (center_embeddings.sum(-2)/center_embeddings.count_nonzero(-2)).nan_to_num() # VocabSize x MaxEmbSize\n    return center_embeddings.data.detach().numpy()\n\n\nclass PaddedSequenceNgramsDataset(Dataset):\n    def __init__(self, texts, corresp_vocngram, targets, sent_len=100, pad_value=0):\n        self.texts = texts\n        self.corresp_vocngram = corresp_vocngram\n        self.targets = targets\n        self.sent_len = sent_len\n        self.max_ngrams_count = len(corresp_vocngram[0])\n        self.pad_value = pad_value\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = ensure_length(txt, self.sent_len, self.pad_value)\n        txt = np.array([corresp_vocngram[idx] for idx in txt])\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:31.915834Z","iopub.execute_input":"2022-02-15T10:32:31.916071Z","iopub.status.idle":"2022-02-15T10:32:31.935019Z","shell.execute_reply.started":"2022-02-15T10:32:31.916042Z","shell.execute_reply":"2022-02-15T10:32:31.934315Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SkipGramNegativeSamplingTrainer(nn.Module):\n    #класс модифицирован для FastText\n    def __init__(self, vocab_size, vocab_ngram_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.vocab_ngram_size = vocab_ngram_size\n        self.negative_samples_n = negative_samples_n\n\n        self.center_ngram_emb = nn.Embedding(self.vocab_ngram_size, emb_size, padding_idx=0)\n        self.center_ngram_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.center_ngram_emb.weight.data[0] = 0\n\n        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.context_emb.weight.data[0] = 0\n\n        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n    \n    \n    def forward(self, sentences):\n        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n        batch_size = sentences.shape[0]\n\n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding', равные среднему из всех ембеддингов,\n        #входящих в токен n-gramm (это описание сразу 2-х строк ниже), среднее считается без учета 0\n        center_embeddings = self.center_ngram_emb(sentences[:,:,1:])  # Batch x MaxSentLength x MaxNgrams x MaxEmbSize\n        center_embeddings = (center_embeddings.sum(-2)/center_embeddings.count_nonzero(-2)).nan_to_num() # Batch x MaxSentLength x MaxEmbSize\n        \n        ### оценить сходство с настоящими соседними словами\n        \n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding'и, (это контекстные слова)\n        #дополнительно транспонируем для целей последующего тензорного (матричного) умножения \n        #Итого(для batch=1): мы получаем тензор предложения фиксированной длины, где каждое слово \n        #заменено на embedding из контекстных слов, все отсутсвующие слова (нет в словаре или \n        #закончилось реальное предложение), заменяются на embedding из 0\n        positive_context_embs = self.context_emb(sentences[:,:,0]).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n        \n        #перемножение тензоров, по сути скалярное произведение эмбеддингов, \n        #Важно отметить, что изначально я проедполагал, что эта операция равносильна нахождению косинусных расстояний, \n        #т.к. на основе анализа итоговых эмбеддингов, сделал неверный вывод, что длина каждого из векторов уже здесь = 1 \n        #(т.е. они сразу нормализуются в пределах каждого embedding (например внутри класса torch.nn.Embedding), \n        #но это не так, нормализация происходит уже после полного обучения модели, через передачу весов в конструктор \n        #созданного вручную класса Embedding)\n        #Итого(для batch=1): мы получаем матрицу MaxSentLength x MaxSentLength, скалярных произведений, \n        #между векторами каждого центрального слова и каждого контекстного слова (значения [-inf; inf])\n        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n        \n        #преобразуем в \"условные вероятности\" через взятие сигмоиды, т.е. получаем как бы \n        #\"условные вероятности\" встретить пары слов вместе, по факту для каждой пары, скалярное произведение, \n        #обернутое в сигмоиду и как следствие в диапазон значений (0; 1)\n        positive_probs = torch.sigmoid(positive_sims)\n        \n        ### увеличить оценку вероятности встретить эти пары слов вместе\n        \n        #переводим тензор self.positive_sim_mask на тот же девайс, на котором positive_sims\n        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n        \n        #.expand_as - Expand this tensor to the same size as other. \n        #self.expand_as(other) is equivalent to self.expand(other.size())\n        #positive_probs * positive_mask - мы оставляем только позиции пересечения центральных слов в контекстными,\n        #все остальные позиции зануляются\n        #подсчитываем бинарную кросс энтропию вычисленных \"условных вероятностей\" (сигмоид) и целевых = 1 для всех\n        #пересечений центральных и контекстных слов, все остальные позиции в обоих матрицах = 0 \n        #Примечание: т.к. по умолчанию BCEloss в реализации torch высчитывает итоговое значение как 'mean', \n        #а не 'sum' из всех полученных, то количество 0 так же влияет на итоговое значение, имеет ли это какой \n        #то эффект, и измениться ли что то, если выставить reduction='sum', не очевидно и нужно проверять на практике\n        #Примечание: для всех позиций, которые занулены, их эмбеддинги соответсвуют эмбеддинг-вектору с idx=0, для\n        #для которого мы при создании мы указали паддинг nn.Embedding(..., padding_idx=0), это означает, что эти веса\n        #фиксированы, и не подлежат изменению через градиентных шаг\n        #\n        #Итого: важно понимать, что если бы оптимизировали только данную loss функцию, без отрицательных примеров, \n        #которые идут ниже, то, все сводилось бы к тому, что минимальное значение loss было бы, если бы мы все \n        #вектора (и центральных слов и контекстных) устремили бы в бесконечность, в одном направлении (например всем\n        #их весам присвоили бы значение inf или любые подобные варианты)\n        \n        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n                                               positive_mask.expand_as(positive_probs))\n\n        \n        ### выбрать случайные \"отрицательные\" слова\n        negative_words = torch.randint(1, self.vocab_size,\n                                       size=(batch_size, self.negative_samples_n),\n                                       device=sentences.device)  # Batch x NegSamplesN\n        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n        \n        ### уменьшить оценку вероятность встретить эти пары слов вместе\n        #Важно отметить, что BCEWithLogitsLoss расвносильна последовательному применению Sigmoid -> BCELoss\n        #но в реализации torch она является более численно стабильной, чем раздельное применение\n        #Итого: здесь все целевые (target) значения = 0, и если бы мы минимизировали только эту loss функцию, то минимальное\n        #ее значение было бы, если бы мы устремили все вектора центральных слов в бесконечность одного направления, \n        #а вектора контекстных слов в бесконечность противоположного направления\n        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n                                                           negative_sims.new_zeros(negative_sims.shape))\n        return positive_loss + negative_loss\n\n\ndef no_loss(pred, target):\n    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:31.936595Z","iopub.execute_input":"2022-02-15T10:32:31.937121Z","iopub.status.idle":"2022-02-15T10:32:31.960721Z","shell.execute_reply.started":"2022-02-15T10:32:31.937060Z","shell.execute_reply":"2022-02-15T10:32:31.959962Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"BOOK_DIRNAME = '/kaggle/input/my-private-datasets/CVBooks/rus/'\nBOOK_LANGUAGE='russian'\ncorpus_paragraphs = []\nfor _, _, filenames in os.walk(BOOK_DIRNAME):\n    for filename in filenames:\n        #print(os.path.join(BOOK_DIRNAME, filename))\n        with open(os.path.join(BOOK_DIRNAME, filename)) as file:\n            corpus_paragraphs.extend([line.strip() for line in file])\n            \ncorpus_paragraphs = list(filter(None, corpus_paragraphs)) #remove empty strings\nprint(len(corpus_paragraphs))\ncorpus_paragraphs[200:205]\n\n#convert corpus of paragraphs to corpus of sentences\ncorpus_sentences = []\nfor paragraph in corpus_paragraphs:\n    corpus_sentences.extend(sent_tokenize(paragraph, language=BOOK_LANGUAGE))\n\nprint(len(corpus_sentences))\ncorpus_sentences[200:205]\n\nrandom.shuffle(corpus_sentences)\ncorpus_sentences[:5]\n\nTRAIN_VAL_SPLIT = int(len(corpus_sentences) * 0.8)\ntrain_source = corpus_sentences[:TRAIN_VAL_SPLIT]\ntest_source = corpus_sentences[TRAIN_VAL_SPLIT:]\nprint(\"Обучающая выборка\", len(train_source))\nprint(\"Тестовая выборка\", len(test_source))\nprint()\nprint('\\n'.join(train_source[:5]))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:31.961998Z","iopub.execute_input":"2022-02-15T10:32:31.962722Z","iopub.status.idle":"2022-02-15T10:32:37.111257Z","shell.execute_reply.started":"2022-02-15T10:32:31.962643Z","shell.execute_reply":"2022-02-15T10:32:37.110256Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#hyperparameters\nexp_name = 'fasttext'\nhyps = {\n    'min_token_size': 3,\n    'min_count': 2,\n    'max_doc_freq': 0.9,\n    \n    'ngrams_size': (3, 6),\n    'min_count_ngram': 5,\n    'max_doc_freq_ngram': 0.9,\n\n    'MAX_SENTENCE_LEN': 35,\n    'emb_size': 100,\n    'rwindow': 5,\n    \n    'neg_sampl_n': 25,\n    'lr': 1e-2,\n    'epoch_n': 30,\n    'batch': 2048,\n    'device': 'cpu',\n    'stop_pat': 4,\n    'lr_sched_pat': 1\n}\n    \nhistories[exp_name] = {\n    'hyps': hyps\n}\n\n# токенизируем \ntrain_tokenized = tokenize_corpus(train_source, min_token_size=hyps['min_token_size'])\ntest_tokenized = tokenize_corpus(test_source, min_token_size=hyps['min_token_size'])\nprint('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# строим словарь (основной словарь, только токены, без N-грамм)\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n                                             max_doc_freq=hyps['max_doc_freq'], \n                                             min_count=hyps['min_count'], pad_word='<PAD>')\nhistories[exp_name]['vocab_size'] = len(vocabulary)\n\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])\n\n# конвертируем корпус из токенов, в корпус их н-грамм символов, заданной размерности\ntrain_tokenized_ngrams = convert_corpus_to_sym_ngramms(train_tokenized, size=hyps['ngrams_size'])\ntest_tokenized_ngrams = convert_corpus_to_sym_ngramms(test_tokenized, size=hyps['ngrams_size'])\n\n# строим словарь (основной словарь, только токены, без N-грамм)\nvocabulary_ngram, word_doc_freq_ngram = build_vocabulary(train_tokenized_ngrams, \n                                             max_doc_freq=hyps['max_doc_freq_ngram'], \n                                             min_count=hyps['min_count_ngram'], pad_word='<PAD>')\nhistories[exp_name]['vocab_ngram_size'] = len(vocabulary)\n\nprint(\"Размер словаря n-грамм\", len(vocabulary_ngram))\nprint(list(vocabulary_ngram.items())[:10])\n\n\n# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n#составляем соответсвия для каждого idx токена из основного словаря, набору idx из словаря n-грамм  \ncorresp_vocngram = get_corresp_voc_vocngram(vocabulary, vocabulary_ngram, size=(3, 6))\nmax_ngrams_count = np.array(list(map(len, corresp_vocngram))).max()\ncorresp_vocngram = np.array([[word_id] + ngrams_idx + [0]*(max_ngrams_count - len(ngrams_idx)) for word_id, ngrams_idx in enumerate(corresp_vocngram)])\n\n# plt.hist([len(s) for s in train_token_ids], bins=20);\n# plt.title('Гистограмма длин предложений');\n\nprint('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n                                                sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n                                                sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n\ntrain_dataset = PaddedSequenceNgramsDataset(train_token_ids, corresp_vocngram,\n                                            np.zeros(len(train_token_ids)),\n                                            sent_len=hyps['MAX_SENTENCE_LEN'])\ntest_dataset = PaddedSequenceNgramsDataset(test_token_ids, corresp_vocngram,\n                                           np.zeros(len(test_token_ids)),\n                                           sent_len=hyps['MAX_SENTENCE_LEN'])\n\nprint(train_dataset[0])\nprint(train_dataset[0][0].shape)\n\n### Обучение\ntrainer = SkipGramNegativeSamplingTrainer(len(vocabulary), len(vocabulary_ngram), \n                                          hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n                                          radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\nhistories[exp_name]['train_history'], best_model = train_eval_loop(\n                                            trainer,\n                                            train_dataset,\n                                            test_dataset,\n                                            no_loss,\n                                            lr=hyps['lr'],\n                                            epoch_n=hyps['epoch_n'],\n                                            batch_size=hyps['batch'],\n                                            device=hyps['device'],\n                                            early_stopping_patience=hyps['stop_pat'],\n                                            max_batches_per_epoch_train=10000,\n                                            max_batches_per_epoch_val=len(test_dataset),\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n                                                                                                                       patience=hyps['lr_sched_pat'], \n                                                                                                                       verbose=True),\n                                            no_calculate_accuracy = True)\n\n#что бы не переписывать функции ниже, конвертируем эмбеддинги n-грамм в эмбединги токенов \n#(каждый эмбединг токена, соответсвует среднему из эмбеддингов составляющих его н-грамм)\nfinal_embeddings = convert_vocab_ngrams(corresp_vocngram, trainer.center_ngram_emb)\n\n### Исследуем характеристики полученных векторов\nembeddings = Embeddings(final_embeddings, vocabulary)\n\nprint('MOST_SIMILAR:')\nhistories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR, topk=20)\n\nprint('WORDS_ANALOGY:')\nprint(WORDS_ANALOGY[0])\nprint('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\nprint('\\nWORDS_ON_PLOT:')\nwords_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\ntest_vectors = embeddings.get_vectors(*words_to_plot)\nprint(test_vectors.shape)\n\nhistories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\nfig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n             histories[exp_name]['test_vectors'].keys(), \n             how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:37.114050Z","iopub.execute_input":"2022-02-15T10:32:37.114609Z","iopub.status.idle":"2022-02-15T10:32:53.087618Z","shell.execute_reply.started":"2022-02-15T10:32:37.114569Z","shell.execute_reply":"2022-02-15T10:32:53.086725Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_vocab_ngrams(corresp_vocngram, center_ngram_emb):\n    center_embeddings = center_ngram_emb(torch.LongTensor(corresp_vocngram)[:,1:])  # VocabSize x MaxNgrams x MaxEmbSize\n    center_embeddings = center_embeddings.sum(-2) # VocabSize x MaxEmbSize\n    return center_embeddings.data.detach().numpy()\n                         \nclass SkipGramNegativeSamplingTrainer(nn.Module):\n    #класс модифицирован для FastText (ВМЕСТО среднего, берется СУММА эмбеддингов н-грамм)\n    def __init__(self, vocab_size, vocab_ngram_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.vocab_ngram_size = vocab_ngram_size\n        self.negative_samples_n = negative_samples_n\n\n        self.center_ngram_emb = nn.Embedding(self.vocab_ngram_size, emb_size, padding_idx=0)\n        self.center_ngram_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.center_ngram_emb.weight.data[0] = 0\n\n        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n        self.context_emb.weight.data[0] = 0\n\n        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n    \n    \n    def forward(self, sentences):\n        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n        batch_size = sentences.shape[0]\n\n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding', равные сумме из всех ембеддингов,\n        #входящих в токен n-gramm (это описание сразу 2-х строк ниже), среднее считается без учета 0\n        center_embeddings = self.center_ngram_emb(sentences[:,:,1:])  # Batch x MaxSentLength x MaxNgrams x MaxEmbSize\n        center_embeddings = center_embeddings.sum(-2) # Batch x MaxSentLength x MaxEmbSize\n        \n        ### оценить сходство с настоящими соседними словами\n        \n        #получает на вход LongTensor с idx (т.е. индексами токенов), возвращает тензор + 1 измерения\n        #в котором индексы заменены на соответсвующие им embedding'и, (это контекстные слова)\n        #дополнительно транспонируем для целей последующего тензорного (матричного) умножения \n        #Итого(для batch=1): мы получаем тензор предложения фиксированной длины, где каждое слово \n        #заменено на embedding из контекстных слов, все отсутсвующие слова (нет в словаре или \n        #закончилось реальное предложение), заменяются на embedding из 0\n        positive_context_embs = self.context_emb(sentences[:,:,0]).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n        \n        #перемножение тензоров, по сути скалярное произведение эмбеддингов, \n        #Важно отметить, что изначально я проедполагал, что эта операция равносильна нахождению косинусных расстояний, \n        #т.к. на основе анализа итоговых эмбеддингов, сделал неверный вывод, что длина каждого из векторов уже здесь = 1 \n        #(т.е. они сразу нормализуются в пределах каждого embedding (например внутри класса torch.nn.Embedding), \n        #но это не так, нормализация происходит уже после полного обучения модели, через передачу весов в конструктор \n        #созданного вручную класса Embedding)\n        #Итого(для batch=1): мы получаем матрицу MaxSentLength x MaxSentLength, скалярных произведений, \n        #между векторами каждого центрального слова и каждого контекстного слова (значения [-inf; inf])\n        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n        \n        #преобразуем в \"условные вероятности\" через взятие сигмоиды, т.е. получаем как бы \n        #\"условные вероятности\" встретить пары слов вместе, по факту для каждой пары, скалярное произведение, \n        #обернутое в сигмоиду и как следствие в диапазон значений (0; 1)\n        positive_probs = torch.sigmoid(positive_sims)\n        \n        ### увеличить оценку вероятности встретить эти пары слов вместе\n        \n        #переводим тензор self.positive_sim_mask на тот же девайс, на котором positive_sims\n        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n        \n        #.expand_as - Expand this tensor to the same size as other. \n        #self.expand_as(other) is equivalent to self.expand(other.size())\n        #positive_probs * positive_mask - мы оставляем только позиции пересечения центральных слов в контекстными,\n        #все остальные позиции зануляются\n        #подсчитываем бинарную кросс энтропию вычисленных \"условных вероятностей\" (сигмоид) и целевых = 1 для всех\n        #пересечений центральных и контекстных слов, все остальные позиции в обоих матрицах = 0 \n        #Примечание: т.к. по умолчанию BCEloss в реализации torch высчитывает итоговое значение как 'mean', \n        #а не 'sum' из всех полученных, то количество 0 так же влияет на итоговое значение, имеет ли это какой \n        #то эффект, и измениться ли что то, если выставить reduction='sum', не очевидно и нужно проверять на практике\n        #Примечание: для всех позиций, которые занулены, их эмбеддинги соответсвуют эмбеддинг-вектору с idx=0, для\n        #для которого мы при создании мы указали паддинг nn.Embedding(..., padding_idx=0), это означает, что эти веса\n        #фиксированы, и не подлежат изменению через градиентных шаг\n        #\n        #Итого: важно понимать, что если бы оптимизировали только данную loss функцию, без отрицательных примеров, \n        #которые идут ниже, то, все сводилось бы к тому, что минимальное значение loss было бы, если бы мы все \n        #вектора (и центральных слов и контекстных) устремили бы в бесконечность, в одном направлении (например всем\n        #их весам присвоили бы значение inf или любые подобные варианты)\n        \n        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n                                               positive_mask.expand_as(positive_probs))\n\n        \n        ### выбрать случайные \"отрицательные\" слова\n        negative_words = torch.randint(1, self.vocab_size,\n                                       size=(batch_size, self.negative_samples_n),\n                                       device=sentences.device)  # Batch x NegSamplesN\n        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n        \n        ### уменьшить оценку вероятность встретить эти пары слов вместе\n        #Важно отметить, что BCEWithLogitsLoss расвносильна последовательному применению Sigmoid -> BCELoss\n        #но в реализации torch она является более численно стабильной, чем раздельное применение\n        #Итого: здесь все целевые (target) значения = 0, и если бы мы минимизировали только эту loss функцию, то минимальное\n        #ее значение было бы, если бы мы устремили все вектора центральных слов в бесконечность одного направления, \n        #а вектора контекстных слов в бесконечность противоположного направления\n        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n                                                           negative_sims.new_zeros(negative_sims.shape))\n        return positive_loss + negative_loss\n\n\ndef no_loss(pred, target):\n    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:15:33.713119Z","iopub.execute_input":"2022-02-15T12:15:33.714007Z","iopub.status.idle":"2022-02-15T12:15:33.738539Z","shell.execute_reply.started":"2022-02-15T12:15:33.713965Z","shell.execute_reply":"2022-02-15T12:15:33.737541Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"#hyperparameters\n\n#FastText (ВМЕСТО среднего, берется СУММА эмбеддингов н-грамм)\nexp_name = 'fasttext_SUM'\nhyps = {\n    'min_token_size': 3,\n    'min_count': 2,\n    'max_doc_freq': 0.9,\n    \n    'ngrams_size': (3, 6),\n    'min_count_ngram': 5,\n    'max_doc_freq_ngram': 0.9,\n\n    'MAX_SENTENCE_LEN': 35,\n    'emb_size': 100,\n    'rwindow': 5,\n    \n    'neg_sampl_n': 25,\n    'lr': 1e-2,\n    'epoch_n': 30,\n    'batch': 2048,\n    'device': 'cpu',\n    'stop_pat': 4,\n    'lr_sched_pat': 1\n}\n    \nhistories[exp_name] = {\n    'hyps': hyps\n}\n\n# токенизируем \ntrain_tokenized = tokenize_corpus(train_source, min_token_size=hyps['min_token_size'])\ntest_tokenized = tokenize_corpus(test_source, min_token_size=hyps['min_token_size'])\nprint('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))\n\n# строим словарь (основной словарь, только токены, без N-грамм)\nvocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n                                             max_doc_freq=hyps['max_doc_freq'], \n                                             min_count=hyps['min_count'], pad_word='<PAD>')\nhistories[exp_name]['vocab_size'] = len(vocabulary)\n\nprint(\"Размер словаря\", len(vocabulary))\nprint(list(vocabulary.items())[:10])\n\n# конвертируем корпус из токенов, в корпус их н-грамм символов, заданной размерности\ntrain_tokenized_ngrams = convert_corpus_to_sym_ngramms(train_tokenized, size=hyps['ngrams_size'])\ntest_tokenized_ngrams = convert_corpus_to_sym_ngramms(test_tokenized, size=hyps['ngrams_size'])\n\n# строим словарь (основной словарь, только токены, без N-грамм)\nvocabulary_ngram, word_doc_freq_ngram = build_vocabulary(train_tokenized_ngrams, \n                                             max_doc_freq=hyps['max_doc_freq_ngram'], \n                                             min_count=hyps['min_count_ngram'], pad_word='<PAD>')\nhistories[exp_name]['vocab_ngram_size'] = len(vocabulary)\n\nprint(\"Размер словаря n-грамм\", len(vocabulary_ngram))\nprint(list(vocabulary_ngram.items())[:10])\n\n\n# отображаем в номера токенов\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\n\n#составляем соответсвия для каждого idx токена из основного словаря, набору idx из словаря n-грамм  \ncorresp_vocngram = get_corresp_voc_vocngram(vocabulary, vocabulary_ngram, size=(3, 6))\nmax_ngrams_count = np.array(list(map(len, corresp_vocngram))).max()\ncorresp_vocngram = np.array([[word_id] + ngrams_idx + [0]*(max_ngrams_count - len(ngrams_idx)) for word_id, ngrams_idx in enumerate(corresp_vocngram)])\n\n# plt.hist([len(s) for s in train_token_ids], bins=20);\n# plt.title('Гистограмма длин предложений');\n\nprint('MAX_SENTENCE_LEN = {}, Sentences counts: (<=) = {}, (>) = {}'.format(hyps['MAX_SENTENCE_LEN'],\n                                                sum(np.array([len(s) for s in train_token_ids]) <= hyps['MAX_SENTENCE_LEN']),\n                                                sum(np.array([len(s) for s in train_token_ids]) > hyps['MAX_SENTENCE_LEN'])))\n\n\ntrain_dataset = PaddedSequenceNgramsDataset(train_token_ids, corresp_vocngram,\n                                            np.zeros(len(train_token_ids)),\n                                            sent_len=hyps['MAX_SENTENCE_LEN'])\ntest_dataset = PaddedSequenceNgramsDataset(test_token_ids, corresp_vocngram,\n                                           np.zeros(len(test_token_ids)),\n                                           sent_len=hyps['MAX_SENTENCE_LEN'])\n\nprint(train_dataset[0])\nprint(train_dataset[0][0].shape)\n\n### Обучение\ntrainer = SkipGramNegativeSamplingTrainer(len(vocabulary), len(vocabulary_ngram), \n                                          hyps['emb_size'], hyps['MAX_SENTENCE_LEN'],\n                                          radius=hyps['rwindow'], negative_samples_n=hyps['neg_sampl_n'])\n\nhistories[exp_name]['train_history'], best_model = train_eval_loop(\n                                            trainer,\n                                            train_dataset,\n                                            test_dataset,\n                                            no_loss,\n                                            lr=hyps['lr'],\n                                            epoch_n=hyps['epoch_n'],\n                                            batch_size=hyps['batch'],\n                                            device=hyps['device'],\n                                            early_stopping_patience=hyps['stop_pat'],\n                                            max_batches_per_epoch_train=10000,\n                                            max_batches_per_epoch_val=len(test_dataset),\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \n                                                                                                                       patience=hyps['lr_sched_pat'], \n                                                                                                                       verbose=True),\n                                            no_calculate_accuracy = True)\n\n#что бы не переписывать функции ниже, конвертируем эмбеддинги n-грамм в эмбединги токенов \n#(каждый эмбединг токена, соответсвует сумме из эмбеддингов составляющих его н-грамм)\nfinal_embeddings = convert_vocab_ngrams(corresp_vocngram, trainer.center_ngram_emb)\n\n### Исследуем характеристики полученных векторов\nembeddings = Embeddings(final_embeddings, vocabulary)\n\nprint('MOST_SIMILAR:')\nhistories[exp_name]['most_similars'] = run_most_sumilars(embeddings.most_similar,WORDS_TO_SIMILAR, topk=20)\n\nprint('WORDS_ANALOGY:')\nprint(WORDS_ANALOGY[0])\nprint('\\n'.join(map(str,embeddings.most_similar(**WORDS_ANALOGY[0], topk=10, with_mean=True))))\n\nprint('\\nWORDS_ON_PLOT:')\nwords_to_plot = [w for w in WORDS_TO_PLOT if w in embeddings.word2id]\ntest_vectors = embeddings.get_vectors(*words_to_plot)\nprint(test_vectors.shape)\n\nhistories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(WORDS_TO_PLOT, test_vectors)}\n\nfig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(np.array(list(histories[exp_name]['test_vectors'].values())), \n             histories[exp_name]['test_vectors'].keys(), \n             how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:32:37.114050Z","iopub.execute_input":"2022-02-15T10:32:37.114609Z","iopub.status.idle":"2022-02-15T10:32:53.087618Z","shell.execute_reply.started":"2022-02-15T10:32:37.114569Z","shell.execute_reply":"2022-02-15T10:32:53.086725Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:52:25.109421Z","iopub.execute_input":"2022-02-15T10:52:25.109775Z","iopub.status.idle":"2022-02-15T10:52:25.123090Z","shell.execute_reply.started":"2022-02-15T10:52:25.109741Z","shell.execute_reply":"2022-02-15T10:52:25.122271Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Реализация FastText через gensim","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim","metadata":{"execution":{"iopub.status.busy":"2022-02-15T10:53:13.522355Z","iopub.execute_input":"2022-02-15T10:53:13.522832Z","iopub.status.idle":"2022-02-15T10:53:13.663710Z","shell.execute_reply.started":"2022-02-15T10:53:13.522798Z","shell.execute_reply":"2022-02-15T10:53:13.662637Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"exp_name = 'FastText_gensim'\n\nhyps = {\n    'min_token_size': 3,\n    'min_count': 2,\n    'emb_size': 100,\n    'rwindow': 5,\n    'ngrams_size': (3, 6)\n}\n\nhistories[exp_name] = {\n    'hyps': hyps\n}\n\nfasttext = gensim.models.FastText(sentences=train_tokenized, vector_size=hyps['emb_size'], \n                                  window=hyps['rwindow'], min_count=hyps['min_count'], \n                                  min_n=hyps['ngrams_size'][0], max_n=hyps['ngrams_size'][1],\n                                  epochs=10)\n\nhistories[exp_name]['most_similars'] = run_most_sumilars(fasttext.wv.most_similar,WORDS_TO_SIMILAR, topn=20)\nprint(histories[exp_name]['most_similars'])\n\nprint(WORDS_ANALOGY[0])\nfasttext.wv.most_similar(**WORDS_ANALOGY[0])\n\ngensim_words = [w for w in WORDS_TO_PLOT if w in fasttext.wv.key_to_index]\ngensim_vectors = np.stack([fasttext.wv[w] for w in gensim_words])\nhistories[exp_name]['test_vectors'] = {word:vec for word,vec in zip(gensim_words, gensim_vectors)}\nfig, ax = plt.subplots()\nfig.set_size_inches((10, 10))\nplot_vectors(gensim_vectors, gensim_words, how='svd', ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Эксперимент - усложнить алгоритм оценки вероятности совместной встречаемости слов, например, заменив скалярное произведение на нейросеть с парой слоёв ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Итоги экспериментов","metadata":{}},{"cell_type":"code","source":"#show experiments min loss\ntrain_history = {}\nfor exp_name, exp in histories.items():\n    if 'train_history' in exp:\n        train_history[exp_name] = exp['train_history']\nshow_experiments_stats(train_history, show_plots = False, only_BEST_MODEL_CALC = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:39:00.158423Z","iopub.execute_input":"2022-02-08T14:39:00.158753Z","iopub.status.idle":"2022-02-08T14:39:00.166312Z","shell.execute_reply.started":"2022-02-08T14:39:00.158718Z","shell.execute_reply":"2022-02-08T14:39:00.16536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mutual distance (Cosine Similarity)\n\nimport seaborn as sns; sns.set_theme()\n\nfigsize = (14, 10)\n\nprint('Experiments:')\nprint('\\n'.join(map(str,histories.keys())))\nfor word in WORDS_TO_SIMILAR[:3]:\n    list_test_vectors = []\n    for exp_name, exp_hist in histories.items():\n        if ('lem_with_pos' in exp_hist['hyps']) and exp_hist['hyps']['lem_with_pos']:\n            lemma = get_first_lemmas_by_word(word, exp_hist['test_vectors'])\n            list_test_vectors.append(exp_hist['test_vectors'][lemma])\n        else:\n            list_test_vectors.append(exp_hist['test_vectors'][word])\n        \n    test_vectors = np.stack(list_test_vectors)\n    similarity_corr = test_vectors @ test_vectors.T\n    mask = np.zeros_like(similarity_corr)\n    mask[np.triu_indices_from(mask)] = True\n    with sns.axes_style(\"white\"):\n        f, ax = plt.subplots(figsize=figsize)\n        ax.set_title(word)\n        ax = sns.heatmap(similarity_corr, mask=mask, vmin=-1, vmax=1, annot=True, square=True, linewidths=.5, cmap=\"YlGnBu\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T11:43:26.555794Z","iopub.execute_input":"2022-02-11T11:43:26.556123Z","iopub.status.idle":"2022-02-11T11:43:27.430547Z","shell.execute_reply.started":"2022-02-11T11:43:26.556083Z","shell.execute_reply":"2022-02-11T11:43:27.429836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mutual distance (SVD plots)\n\nfrom sklearn.decomposition import TruncatedSVD\n        \nsize_inches = (10, 10)\nprint('Experiments:')\nprint('\\n'.join(map(str,histories.keys())))\n\nfor word in WORDS_TO_SIMILAR[:3]:\n    print('WORD: {}'.format(word))\n    list_test_vectors = []\n    for exp_name, exp_hist in histories.items():\n        if ('lem_with_pos' in exp_hist['hyps']) and exp_hist['hyps']['lem_with_pos']:\n            lemma = get_first_lemmas_by_word(word, exp_hist['test_vectors'])\n            list_test_vectors.append(exp_hist['test_vectors'][lemma])\n        else:\n            list_test_vectors.append(exp_hist['test_vectors'][word])\n\n    if len(list_test_vectors) > 1:\n        test_vectors = np.stack(list_test_vectors)\n        print(TruncatedSVD().fit_transform(test_vectors))\n\n        fig, ax = plt.subplots()\n        fig.set_size_inches(size_inches)\n        ax.set_title(word)\n        plot_vectors(test_vectors, \n                     histories.keys(), \n                     how='svd', ax=ax, xy_lim=(-1., 1.))\n    else:\n        print('Only one experiment')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:01:43.664318Z","iopub.execute_input":"2022-02-11T12:01:43.664675Z","iopub.status.idle":"2022-02-11T12:01:43.678043Z","shell.execute_reply.started":"2022-02-11T12:01:43.664639Z","shell.execute_reply":"2022-02-11T12:01:43.67702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mutual distance (SVD plots) in one plot\n\nfrom sklearn.decomposition import TruncatedSVD\n        \nsize_inches = (16, 16)\n\nprint('Experiments:')\nprint('\\n'.join(map(str,histories.keys())))\n\nlist_test_vectors = []\nlist_test_vectors_names = []\n\nfor word in WORDS_TO_SIMILAR[:3]:\n    for exp_name, exp_hist in histories.items():\n        if ('lem_with_pos' in exp_hist['hyps']) and exp_hist['hyps']['lem_with_pos']:\n            lemma = get_first_lemmas_by_word(word, exp_hist['test_vectors'])\n            list_test_vectors.append(exp_hist['test_vectors'][lemma])\n        else:\n            list_test_vectors.append(exp_hist['test_vectors'][word])\n        list_test_vectors_names.append('{}-{}'.format(word, exp_name))\n        \ntest_vectors = np.stack(list_test_vectors)\n    \nfig, ax = plt.subplots()\nfig.set_size_inches(size_inches)\nplot_vectors(test_vectors, \n                 list_test_vectors_names, \n                 how='svd', ax=ax, xy_lim=(-1., 1.))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:02:38.72887Z","iopub.execute_input":"2022-02-11T12:02:38.729843Z","iopub.status.idle":"2022-02-11T12:02:39.12129Z","shell.execute_reply.started":"2022-02-11T12:02:38.729796Z","shell.execute_reply":"2022-02-11T12:02:39.120022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compare most_similars\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nlist_compare_similars = []\nfor word in WORDS_TO_SIMILAR:\n    list_similar = []\n    for exp_name, exp_hist in histories.items():\n        if ('lem_with_pos' in exp_hist['hyps']) and exp_hist['hyps']['lem_with_pos']:\n            lemma = get_first_lemmas_by_word(word, exp_hist['test_vectors'])\n            list_similar.append(exp_hist['most_similars'][lemma])\n        else:\n            list_similar.append(exp_hist['most_similars'][word])\n    \n    MultiIndex = pd.MultiIndex.from_product(([word], [i for i in range(10)]), names=[\"word\", \"n\"])\n    list_compare_similars.append(pd.DataFrame(list_similar,index=histories.keys(),columns=MultiIndex).T)\npd.concat(list_compare_similars)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:04:57.43758Z","iopub.execute_input":"2022-02-11T12:04:57.438559Z","iopub.status.idle":"2022-02-11T12:04:57.537816Z","shell.execute_reply.started":"2022-02-11T12:04:57.438499Z","shell.execute_reply":"2022-02-11T12:04:57.537148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare plots (SVD)\nsize_inches = (10, 10)\nfor exp_name, exp_hist in histories.items():\n    fig, ax = plt.subplots()\n    fig.set_size_inches(size_inches)\n    ax.set_title('Experiment: {}'.format(exp_name))\n    plot_vectors(np.array(list(exp_hist['test_vectors'].values())), \n                 exp_hist['test_vectors'].keys(), \n                 how='svd', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:07:15.229793Z","iopub.execute_input":"2022-02-11T12:07:15.230191Z","iopub.status.idle":"2022-02-11T12:07:15.866229Z","shell.execute_reply.started":"2022-02-11T12:07:15.230157Z","shell.execute_reply":"2022-02-11T12:07:15.865546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare plots (SVD) with xy_lim\nsize_inches = (10, 10)\nfor exp_name, exp_hist in histories.items():\n    print('Experiment: {}'.format(exp_name))\n    fig, ax = plt.subplots()\n    fig.set_size_inches(size_inches)\n    ax.set_title('Experiment: {}'.format(exp_name))\n    plot_vectors(np.array(list(exp_hist['test_vectors'].values())), \n                 exp_hist['test_vectors'].keys(), \n                 how='svd', ax=ax, xy_lim=(-1., 1.))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:07:21.879453Z","iopub.execute_input":"2022-02-11T12:07:21.880102Z","iopub.status.idle":"2022-02-11T12:07:22.566354Z","shell.execute_reply.started":"2022-02-11T12:07:21.880055Z","shell.execute_reply":"2022-02-11T12:07:22.5655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Compare plots (TSNE)\n# size_inches = (10, 10)\n# for exp_name, exp_hist in histories.items():\n#     print('Experiment: {}'.format(exp_name))\n#     fig, ax = plt.subplots()\n#     fig.set_size_inches(size_inches)\n#     plot_vectors(np.array(list(exp_hist['test_vectors'].values())), \n#                  exp_hist['test_vectors'].keys(), \n#                  how='tsne', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:10:06.316572Z","iopub.execute_input":"2022-02-08T14:10:06.317143Z","iopub.status.idle":"2022-02-08T14:10:06.321987Z","shell.execute_reply.started":"2022-02-08T14:10:06.317097Z","shell.execute_reply":"2022-02-08T14:10:06.320999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:38:38.525939Z","iopub.execute_input":"2022-02-08T14:38:38.526267Z","iopub.status.idle":"2022-02-08T14:38:38.53383Z","shell.execute_reply.started":"2022-02-08T14:38:38.526235Z","shell.execute_reply":"2022-02-08T14:38:38.533204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:37:33.514Z","iopub.execute_input":"2022-02-08T14:37:33.515138Z","iopub.status.idle":"2022-02-08T14:37:33.521473Z","shell.execute_reply.started":"2022-02-08T14:37:33.515081Z","shell.execute_reply":"2022-02-08T14:37:33.520209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}