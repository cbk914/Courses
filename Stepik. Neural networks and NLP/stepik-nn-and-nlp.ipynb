{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-19T08:14:11.304063Z","iopub.execute_input":"2022-01-19T08:14:11.304723Z","iopub.status.idle":"2022-01-19T08:14:11.334342Z","shell.execute_reply.started":"2022-01-19T08:14:11.304626Z","shell.execute_reply":"2022-01-19T08:14:11.333555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 2. Векторная модель текста и классификация длинных текстов ###","metadata":{}},{"cell_type":"markdown","source":"**2.3.1**","metadata":{"execution":{"iopub.status.busy":"2022-01-19T08:22:03.646101Z","iopub.execute_input":"2022-01-19T08:22:03.646851Z","iopub.status.idle":"2022-01-19T08:22:03.652155Z","shell.execute_reply.started":"2022-01-19T08:22:03.646811Z","shell.execute_reply":"2022-01-19T08:22:03.651287Z"}}},{"cell_type":"code","source":"from sympy import diff, symbols, log, exp\nx, y, w, b = symbols('x y w b')\ny_hat = 1/(1+exp(-w*x-b))\neq = -y*log(y_hat)-(1-y)*log(1-y_hat)\nprint(diff(eq, w).simplify())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T08:42:09.936928Z","iopub.execute_input":"2022-01-19T08:42:09.93727Z","iopub.status.idle":"2022-01-19T08:42:10.010771Z","shell.execute_reply.started":"2022-01-19T08:42:09.93723Z","shell.execute_reply":"2022-01-19T08:42:10.010046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sympy.parsing.sympy_parser\n\nsample_expr_str = 'x*(-y*exp(-b - w*x) - y + 1)/(exp(-b - w*x) + 1)'\nsample_expr = sympy.parsing.sympy_parser.parse_expr(sample_expr_str)\nsample_value = sample_expr.evalf(subs=dict(x=0.5, y=1, w=4, b=1))\nprint(sample_value)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T08:44:38.143925Z","iopub.execute_input":"2022-01-19T08:44:38.144588Z","iopub.status.idle":"2022-01-19T08:44:38.17401Z","shell.execute_reply.started":"2022-01-19T08:44:38.144554Z","shell.execute_reply":"2022-01-19T08:44:38.17313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3.2**","metadata":{}},{"cell_type":"code","source":"from sympy import diff, symbols, log, exp\nx, y, w, b = symbols('x y w b')\ny_hat = 1/(1+exp(-w*x-b))\neq = -y*log(y_hat)-(1-y)*log(1-y_hat)\nprint(diff(eq, b).simplify())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T08:45:09.221546Z","iopub.execute_input":"2022-01-19T08:45:09.222109Z","iopub.status.idle":"2022-01-19T08:45:09.347839Z","shell.execute_reply.started":"2022-01-19T08:45:09.222073Z","shell.execute_reply":"2022-01-19T08:45:09.346901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3.3**","metadata":{}},{"cell_type":"code","source":"from sympy import diff, symbols, log, exp\nx, y, w, b, c = symbols('x y w b, c')\ny_hat = 1/(1+exp(-w*x-b))\nreg = c * (w**2 + b**2)\neq = -y*log(y_hat)-(1-y)*log(1-y_hat) + reg\nprint(diff(eq, w).simplify())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T08:50:56.646459Z","iopub.execute_input":"2022-01-19T08:50:56.646732Z","iopub.status.idle":"2022-01-19T08:50:56.83698Z","shell.execute_reply.started":"2022-01-19T08:50:56.646704Z","shell.execute_reply":"2022-01-19T08:50:56.836378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3.6**","metadata":{}},{"cell_type":"code","source":"import sys\nimport numpy as np\n\ndef parse_array(s):\n    return np.array([int(s.strip()) for s in s.strip().split(' ')])\n\ndef read_array(inp):\n    return parse_array(inp)\n\ndef calculate_pmi(a, b):\n    #p(a,b) - вероятность того, что оба события совершились одновременно (и там и там единица).\n    #p(a) , p(b) - вероятности реализации случайных событий.\n    #pmi(a,b) = log(p(a,b)/(p(a)*p(b)))\n    p_a_b = np.mean(a * b)\n    p_a = np.mean(a)\n    p_b = np.mean(b)\n    return np.log(p_a_b / (p_a * p_b))  \n\na = read_array('1 0 0 1 1 0')\nb = read_array('1 0 0 0 1 0')\npmi_value = calculate_pmi(a, b)\n\nprint('{:.6f}'.format(pmi_value))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:07:34.354385Z","iopub.execute_input":"2022-01-19T11:07:34.354864Z","iopub.status.idle":"2022-01-19T11:07:34.364725Z","shell.execute_reply.started":"2022-01-19T11:07:34.354829Z","shell.execute_reply":"2022-01-19T11:07:34.363583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3.7**","metadata":{}},{"cell_type":"raw","source":"Найдите количество слов, которые встречаются менее, чем в 10 из 10000 документов, если предполагать, что вероятность встретить слово в документе распределена по Ципфу с параметром s=2, количество слов в словаре N=1000. Ранги нумеруются с 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\ns = 2\nP = 10 / 10000 #заданная плотность распределения\nZ = np.array([i**(-2) for i in range(1, 1001)]).sum()\nrank = (1 / (Z * 10 / 10000))**(1/s)\nnum = int(1000 - rank) + 1\nnum","metadata":{"execution":{"iopub.status.busy":"2022-01-19T11:36:26.353728Z","iopub.execute_input":"2022-01-19T11:36:26.353998Z","iopub.status.idle":"2022-01-19T11:36:26.362292Z","shell.execute_reply.started":"2022-01-19T11:36:26.353971Z","shell.execute_reply":"2022-01-19T11:36:26.361611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.4.3**","metadata":{}},{"cell_type":"raw","source":"Детали \"конструктора\", из которых можно собрать решение задачи:\n    [а-яё]+          // ненулевая последовательность любых букв русского алфавита.\n    -?\\d*[.,]?\\d+   // возможно знак, возможная целая часть числа, возможно, десятичный знак и остальная часть числа.\n    \\S                // любой символ, кроме разделителей (пробелов, переносов строк)\n    |     //  | отвечает за выбор из двух паттернов, например: [а-яё]+|\\d последовательность букв или одна цифра.","metadata":{}},{"cell_type":"code","source":"import re\nimport sys\n\ntext = [\n'Контактный телефон: 123123.',\n'Что-нибудь надо придумать.',\n'Значение числа Е=2.7182.',\n'Демон123, как тебя зовут в реале?',\n'-1-.15=-1.15',\n'- 1 - .15 = -1.15',\n'Какого ;%:?* тут происходит?'\n]\n\n# модифицируйте это регулярное выражение\nparts = [\n    r'[а-яё]+',\n    r'-?\\d*[.,]?\\d+',\n    r'\\S'\n]\n#re.I -  re.IGNORECASE - Perform case-insensitive matching\n#Флаг, избавляет нас от необходимости указывать заглавные буквы алфавита \n#при выделении слов и достаточно указать лишь диапазон строчных букв.\nTOKENIZE_RE = re.compile(r'|'.join(parts), re.I) \n\n\ndef tokenize(txt):\n    return TOKENIZE_RE.findall(txt)\n\n\nfor line in text:\n    print(' '.join(tokenize(line.strip().lower())))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T12:54:59.818176Z","iopub.execute_input":"2022-01-19T12:54:59.818515Z","iopub.status.idle":"2022-01-19T12:54:59.829103Z","shell.execute_reply.started":"2022-01-19T12:54:59.818482Z","shell.execute_reply":"2022-01-19T12:54:59.827971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.4.5**","metadata":{}},{"cell_type":"code","source":"import re\nimport sys\nimport collections\nimport numpy as np\n\n\n\ndef tokenize_text(text, lower=True):\n    parts = [\n        r'[\\w\\d]+'\n    ]\n    TOKENIZE_RE = re.compile(r'|'.join(parts), re.I)\n    if lower:\n        return TOKENIZE_RE.findall(text.lower())\n    else:\n        return TOKENIZE_RE.findall(text)\n\ndef tokenize_corpus(texts, lower=True):\n    return [tokenize_text(text, lower=lower) for text in texts]\n\ndef build_vocabulary(tokenize_texts):\n    word_counts = collections.defaultdict(int)\n    \n    for text in tokenize_texts:\n        unique_tokens = set(text)\n        for token in unique_tokens:\n            word_counts[token] += 1\n    doc_count = len(tokenize_texts)\n    \n    word_counts_sorted = sorted(word_counts.items(), reverse=False, key=lambda kv:(kv[1], kv[0]))\n    \n    word2id = {word: i for i, (word, _) in enumerate(word_counts_sorted)}\n    word2freq = np.array([word_count/doc_count for (_, word_count) in word_counts_sorted], dtype='float32')\n    \n    return word2id, word2freq\n\ntexts = [\n\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n\"Казнить, нельзя помиловать. Нельзя освободить.\",\n\"Нельзя не помиловать.\",\n\"Обязательно освободить.\"\n]\n\ntokenized_texts = tokenize_corpus(texts)\nvoc_word2id, voc_word2freq = build_vocabulary(tokenized_texts)\n\nprint(' '.join(voc_word2id.keys()))\nprint(' '.join(map(str, voc_word2id.values())))\nprint(' '.join(map(str, voc_word2freq)))","metadata":{"execution":{"iopub.status.busy":"2022-01-24T06:13:00.91721Z","iopub.execute_input":"2022-01-24T06:13:00.91786Z","iopub.status.idle":"2022-01-24T06:13:00.954925Z","shell.execute_reply.started":"2022-01-24T06:13:00.917745Z","shell.execute_reply":"2022-01-24T06:13:00.954031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Федор Краснов\n# 2 years ago\n# Привет,\n# для любопытных: с помощью sklearn тоже можно сделать, но есть особенность, которой и делюсь.\n\ncorpus = [\n    'Казнить нельзя, помиловать. Нельзя наказывать.',\n    'Казнить, нельзя помиловать. Нельзя освободить.',\n    'Нельзя не помиловать.',\n    'Обязательно освободить.']\n\n## Получение df\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(smooth_idf=False, use_idf=True)\nvectorizer.fit_transform(corpus)\nword_doc_freq = 1/np.exp(vectorizer.idf_ - 1) ## ВОТ ОСОБЕННОСТЬ ПЕРЕВОДА ИЗ IDF В DF\n\n# Словарь тут: \n# vectorizer.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T13:20:39.318297Z","iopub.execute_input":"2022-01-20T13:20:39.318614Z","iopub.status.idle":"2022-01-20T13:20:39.328132Z","shell.execute_reply.started":"2022-01-20T13:20:39.318581Z","shell.execute_reply":"2022-01-20T13:20:39.327362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.4.7**","metadata":{}},{"cell_type":"code","source":"import scipy.sparse\nimport numpy as np\ndef vectorize_texts_ltfidf(tokenized_texts, voc_word2id, voc_word2freq):\n    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(voc_word2id)), dtype='float32')\n    for text_i, text in enumerate(tokenized_texts):\n        for token in text:\n            if token in voc_word2id.keys():\n                result[text_i, voc_word2id[token]] += 1\n    result = result.tocsr() #переводим матрицу в режим быстрой работы со строками (это очень важно!!)\n    result = result.multiply(1/result.sum(1))\n    result = scipy.sparse.dok_matrix(np.log(result.toarray()+1))\n    result = result.multiply(1 / voc_word2freq)\n    feats_std = result.toarray().std(0, ddof=1)\n    feats_mean = result.toarray().mean(0)\n    return (result.toarray() - feats_mean) / feats_std","metadata":{"execution":{"iopub.status.busy":"2022-01-24T06:19:19.834051Z","iopub.execute_input":"2022-01-24T06:19:19.834395Z","iopub.status.idle":"2022-01-24T06:19:19.927517Z","shell.execute_reply.started":"2022-01-24T06:19:19.834349Z","shell.execute_reply":"2022-01-24T06:19:19.926488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_texts = vectorize_texts_ltfidf(tokenized_texts, voc_word2id, voc_word2freq)\n[print(' '.join(map(str, row))) for row in vectorize_texts];","metadata":{"execution":{"iopub.status.busy":"2022-01-24T06:27:22.78863Z","iopub.execute_input":"2022-01-24T06:27:22.789225Z","iopub.status.idle":"2022-01-24T06:27:22.799828Z","shell.execute_reply.started":"2022-01-24T06:27:22.789175Z","shell.execute_reply":"2022-01-24T06:27:22.799141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Salavat Garifullin\nfrom collections import Counter\nimport pandas as pd\nimport re\n\ncorpus = ['Казнить нельзя, помиловать. Нельзя наказывать.',\n         'Казнить, нельзя помиловать. Нельзя освободить.',\n         'Нельзя не помиловать.',\n         'Обязательно освободить.']\nTOKENIZE_RE = re.compile(r'[\\w\\d]+')\n\ncorpus = pd.Series(corpus).str.lower().apply(TOKENIZE_RE.findall)\ncorpus_counts = corpus.apply(Counter).apply(pd.Series).fillna(0).astype(int)\nword2freq = (corpus_counts > 0).mean(0).sort_index().sort_values()\n\n### ^ код из 5 шага\n\nimport numpy as np\n\nTF = (corpus_counts[word2freq.index].T / corpus.apply(len)).T\nlTFIDF = np.log(TF + 1) / word2freq\nfeature_matrix  = (lTFIDF - lTFIDF.mean()) / lTFIDF.std(0, ddof=1)\n\nfor doc in feature_matrix.round(2).values:\n    print(*doc)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T06:30:53.019345Z","iopub.execute_input":"2022-01-24T06:30:53.019789Z","iopub.status.idle":"2022-01-24T06:30:53.086016Z","shell.execute_reply.started":"2022-01-24T06:30:53.019725Z","shell.execute_reply":"2022-01-24T06:30:53.085122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.4 Final task**","metadata":{"execution":{"iopub.status.busy":"2022-01-24T11:23:45.674636Z","iopub.execute_input":"2022-01-24T11:23:45.675043Z","iopub.status.idle":"2022-01-24T11:23:45.701331Z","shell.execute_reply.started":"2022-01-24T11:23:45.674932Z","shell.execute_reply":"2022-01-24T11:23:45.699599Z"}}},{"cell_type":"code","source":"ДЗ по семинару реализовано в отдельном ноутбуке: stepik-nn-and-nlp 2.4.final (bag-of-words-models).ipynb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Module 3. Базовые нейросетевые методы для работы с текстами ###","metadata":{}},{"cell_type":"markdown","source":"**3.3 Final task**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:54:28.142533Z","iopub.execute_input":"2022-02-15T12:54:28.142933Z","iopub.status.idle":"2022-02-15T12:54:28.171436Z","shell.execute_reply.started":"2022-02-15T12:54:28.14283Z","shell.execute_reply":"2022-02-15T12:54:28.170082Z"}}},{"cell_type":"code","source":"ДЗ по семинару реализовано в отдельном ноутбуке: stepik-nn-and-nlp 3.3 final (word-embeddings).ipynb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.1**","metadata":{}},{"cell_type":"raw","source":"Вы обучаете Word2Vec Skip Gram Negative Sampling с окном заданной ширины. Например, окно размерности 5 подразумевает, что положительными примерами считаются слова, отстоящие от центрального слова не более чем на 2 позиции влево или вправо. Центральное слово не учитывается как контекстное слово.\n\nНапишите функцию, которая генерирует обучающие примеры из текста. Каждый обучающий пример должен иметь вид кортежа из трёх элементов (CenterWord,CtxWord,Label)(CenterWord, CtxWord, Label)(CenterWord,CtxWord,Label), где CenterWord∈NCenterWord \\in \\mathbb{N}CenterWord∈N - идентификатор токена в центре окна, CtxWord∈NCtxWord \\in \\mathbb{N}CtxWord∈N - идентификатор соседнего токена, Label∈{0,1}Label \\in \\{0, 1\\}Label∈{0,1} - 1 если CtxWordCtxWordCtxWord настоящий и 0, если это отрицательный пример.\n\nФункция должна возвращать список обучающих примеров.\n\nАргумент ns_rate задаёт количество отрицательных примеров, которое нужно сгенерировать на каждый положительный пример. При семплировании отрицательных слов обычно не проверяют, встречается ли это слово в окне. Таким образом, среди отрицательных примеров могут появиться положительные.\n\nВходной текст уже токенизирован, токены заменены на их идентификаторы.\n\nТесты генерируются случайно, ограничения:\n\n    len(text) < 20\n    window_size <= 11, нечётное\n    vocab_size < 100\n    ns_rate < 3\n\nСлова имеют идентификаторы 0..vocab_size - 1 (как возвращает np.random.randint).\n\nОбратите также внимание на то, что -3 // 2 != -(3 // 2).","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n    \"\"\"\n    text - list of integer numbers - ids of tokens in text\n    window_size - odd integer - width of window\n    vocab_size - positive integer - number of tokens in vocabulary\n    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n\n    returns list of training samples (CenterWord, CtxWord, Label)\n    \"\"\"\n    rwindow = window_size // 2\n    output = []\n    for i in range(len(text)):\n        pos_sample = [*text[max(i-rwindow,0):i], *text[i+1:i+rwindow+1]]\n        output.extend([[text[i], pos_id, 1] for pos_id in pos_sample])\n        output.extend([[text[i], neg_id, 0] for neg_id in np.random.randint(vocab_size, size=ns_rate*len(pos_sample))])\n    return output\n\ntext = np.random.randint(100,size=100)\nwindow_size = 7\nvocab_size = 100\nns_rate = 3\n\nresult = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n\n#write_array(np.array(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.2**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы обучаете Word2Vec Skip Gram Negative Sampling.\n\nНапишите функцию, обновляющую веса модели при получении одного обучающего примера в формате (CenterWord,CtxWord,Label)(CenterWord, CtxWord, Label)(CenterWord,CtxWord,Label).\n\nПри обучении предсказания модели вычисляются по формуле P(CtxWord∣CenterWord)=σ(WCenterWord,:⋅DCtxWord,:)P(CtxWord | CenterWord) = \\sigma(W_{CenterWord, :} \\cdot D_{CtxWord,:})P(CtxWord∣CenterWord)=σ(WCenterWord,:​⋅DCtxWord,:​).\n\nФункция потерь - бинарная кросс-энтропия.","metadata":{}},{"cell_type":"code","source":"def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n    \"\"\"\n    center_embeddings - VocabSize x EmbSize\n    context_embeddings - VocabSize x EmbSize\n    center_word - int - identifier of center word\n    context_word - int - identifier of context word\n    label - 1 if context_word is real, 0 if it is negative\n    learning_rate - float > 0 - size of gradient step\n    \"\"\"\n    #здесь для истории, заменил свой код (которым решил задание), \n    #кодом Эдуарда Смирнова, т.к. он более читабельный (сама суть аналогична)\n    w = center_embeddings[center_word]\n    d = context_embeddings[context_word]\n\n    sigma = 1/(1+np.exp(-w@d))\n    grad_w = (sigma-label) * d\n    grad_d = (sigma-label) * w\n\n    w -= learning_rate * grad_w\n    d -= learning_rate * grad_d","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.3**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы обучаете FastText SkipGram Negative Sampling с окном заданной ширины (аналогично первой задаче). Общий алгоритм обучения также описывался ранее. Единственное существенное отличие от Word2Vec - использование N-грамм для получения вектора центрального слова.\n\nНапишите функцию, которая генерирует обучающие примеры из текста. Каждый обучающий пример должен иметь вид кортежа из трёх элементов (CenterSubwords,CtxWord,Label)(CenterSubwords, CtxWord, Label)(CenterSubwords,CtxWord,Label), где CenterSubwordsCenterSubwordsCenterSubwords - список идентификаторов N-грамм, входящих в токен в центре окна (включая идентификатор самого токена), CtxWord∈NCtxWord \\in \\mathbb{N}CtxWord∈N - идентификатор соседнего токена, Label∈{0,1}Label \\in \\{0, 1\\}Label∈{0,1} - 1 если CtxWordCtxWordCtxWord настоящий и 0, если это отрицательный пример.\n\nФункция должна возвращать список обучающих примеров.\n\nАргумент ns_rate задаёт количество отрицательных примеров, которое нужно сгенерировать на каждый положительный пример.\n\nВходной текст уже токенизирован, токены заменены на их идентификаторы.\n\nДля получения списка N-грамм, входящих в токен, используется заранее построенный словарь (он передаётся в функцию, которую Вам нужно написать). В этом задании следует считать, что сам токен всегда есть в словаре и поэтому нужно обновлять вектор для него (хотя при обучении FastText на настоящих данных это может быть не так). Все n-граммы имеют номера больше или равные vocab_size.\n\nТесты генерируются случайно, ограничения:\n\n    len(text) < 20\n    vocab_size < 10\n    window_size <= 11, нечётное\n    ns_rate < 3\n    ngrams_n < 20\n\nСлова имеют идентификаторы 0..vocab_size - 1 (как возвращает np.random.randint). N-граммы имеют идентификаторы vocab_size .. vocab_size + ngrams_n - 1.","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef read_list():\n    return ast.literal_eval(sys.stdin.readline())\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords):\n    \"\"\"\n    text - list of integer numbers - ids of tokens in text\n    window_size - odd integer - width of window\n    vocab_size - positive integer - number of tokens in vocabulary\n    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n    token2subwords - list of lists of int - i-th sublist contains list of identifiers of n-grams for token #i (list of subword units)\n\n    returns list of training samples (CenterSubwords, CtxWord, Label)\n    \"\"\"\n    rwindow = window_size // 2\n    output = []\n    for i in range(len(text)):\n        pos_sample = [*text[max(i-rwindow,0):i], *text[i+1:i+rwindow+1]]\n        output.extend([[[text[i], *token2subwords[text[i]]], pos_id, 1] for pos_id in pos_sample])\n        output.extend([[[text[i], *token2subwords[text[i]]], neg_id, 0] for neg_id in np.random.randint(vocab_size, size=ns_rate*len(pos_sample))])\n    return output\n\n\ntext = [1, 2, 0, 1, 4, 0, 4, 1, 5, 4, 5, 4, 5, 1]\nwindow_size = 3\nvocab_size = 6\nns_rate = 2\ntoken2subwords = [[17], [10, 12], [20, 20], [7, 13], [], [7, 11]]\n\nresult = generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords)\n\nprint(repr(result))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:21:34.732456Z","iopub.execute_input":"2022-02-16T07:21:34.733032Z","iopub.status.idle":"2022-02-16T07:21:34.772864Z","shell.execute_reply.started":"2022-02-16T07:21:34.732933Z","shell.execute_reply":"2022-02-16T07:21:34.772274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.4**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы обучаете FastText SkipGram Negative Sampling.\n\nНапишите функцию, обновляющую веса модели при получении одного обучающего примера в формате (CenterSubwords,CtxWord,Label)(CenterSubwords, CtxWord, Label)(CenterSubwords,CtxWord,Label).\n\nФункция потерь - бинарная кросс-энтропия.","metadata":{}},{"cell_type":"code","source":"def update_ft_weights(center_embeddings, context_embeddings, center_subwords, context_word, label, learning_rate):\n    \"\"\"\n    center_embeddings - VocabSize x EmbSize\n    context_embeddings - VocabSize x EmbSize\n    center_subwords - list of ints - list of identifiers of n-grams contained in center word\n    context_word - int - identifier of context word\n    label - 1 if context_word is real, 0 if it is negative\n    learning_rate - float > 0 - size of gradient step\n    \"\"\"\n    #заменил для истории на подход от Дани Крутовкий, у меня было чуть длинее, за счет того,\n    #что я забыл, что для выбора нескольких значений сразу можно слайс подавать\n    w_mean = center_embeddings[center_subwords].mean(-2)\n    d = context_embeddings[context_word]\n\n    sigma = 1/(1+np.exp(-w_mean@d))\n    grad_w = (sigma-label) * d\n    grad_d = (sigma-label) * w_mean\n    \n    context_embeddings[context_word] -= learning_rate * grad_d\n    center_embeddings[center_subwords] -= learning_rate * grad_w / len(center_subwords)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.5**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы собираетесь обучать GloVe и строите матрицу совместной встречаемости токенов, то есть матрицу, в ij ячейке которой стоит количество документов, в которых употреблялось и слово i и слово j. На главной диагонали должны стоять нули (то есть не нужно считать количество совместных употреблений слова с самим собой).\n\nНапишите функцию, которая получает на вход список документов и строит такую матрицу. Документы уже токенизированы и токены представлены их числовыми идентификаторами.\n\nДля хранения матрицы используйте scipy.sparse.dok_matrix.","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\nimport scipy.sparse\n\n\ndef read_array():\n    return ast.literal_eval(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\ndef generate_coocurrence_matrix(texts, vocab_size):\n    \"\"\"\n    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n    vocab_size - int - size of vocabulary\n    returns scipy.sparse.dok_matrix\n    \"\"\"\n    co_matrix = scipy.sparse.dok_matrix((vocab_size, vocab_size), dtype=np.float32)\n    for doc in text:\n        tokens = tuple(set(doc))\n        for i in tokens:\n            co_matrix[i,tokens] += 1   \n    co_matrix.setdiag(0)\n    return co_matrix\n\ntext = [[0, 2, 2, 2, 0, 0], [1, 1, 2, 1, 1], [2, 2, 1, 1]]\nvocab_size = 3\n\nresult = generate_coocurrence_matrix(text, vocab_size)\n\nwrite_array(result.toarray())","metadata":{"execution":{"iopub.status.busy":"2022-02-19T06:45:20.674908Z","iopub.execute_input":"2022-02-19T06:45:20.67585Z","iopub.status.idle":"2022-02-19T06:45:20.682832Z","shell.execute_reply.started":"2022-02-19T06:45:20.675779Z","shell.execute_reply":"2022-02-19T06:45:20.682419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\nimport scipy.sparse\n\n\ndef read_array():\n    return ast.literal_eval(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\ndef generate_coocurrence_matrix(texts, vocab_size):\n    \"\"\"\n    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n    vocab_size - int - size of vocabulary\n    returns scipy.sparse.dok_matrix\n    \"\"\"\n    co_matrix = scipy.sparse.dok_matrix((vocab_size, vocab_size), dtype=np.float32)\n    for doc in text:\n        tokens = tuple(set(doc))\n        mesh = np.array(np.meshgrid(tokens, tokens)).reshape(2,-1)\n        co_matrix[mesh[0],mesh[1]] += 1\n    co_matrix.setdiag(0)\n    return co_matrix\n\ntext = [[0, 2, 2, 2, 0, 0], [1, 1, 2, 1, 1], [2, 2, 1, 1]]\nvocab_size = 3\n\nresult = generate_coocurrence_matrix(text, vocab_size)\n\nwrite_array(result.toarray())","metadata":{"execution":{"iopub.status.busy":"2022-02-19T06:40:36.016269Z","iopub.execute_input":"2022-02-19T06:40:36.016722Z","iopub.status.idle":"2022-02-19T06:40:36.024718Z","shell.execute_reply.started":"2022-02-19T06:40:36.016672Z","shell.execute_reply":"2022-02-19T06:40:36.023958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.6**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы обучаете GloVe. Матрица совместной встречаемости X∈R∣Vocab∣×∣Vocab∣X \\in \\mathbb{R}^{|Vocab| \\times |Vocab|}X∈R∣Vocab∣×∣Vocab∣ построена.\n\nНапишите функцию, которая обновляет параметры модели градиентным спуском (делает один градиентный шаг). Обратите внимание, что Вы должны изменить значения массивов w и d (inplace), а не создать новые массивы с новыми значениями.\n\nФункция потерь: GloVeLoss(W,D,X)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T11:09:13.211665Z","iopub.execute_input":"2022-02-16T11:09:13.212007Z","iopub.status.idle":"2022-02-16T11:09:13.223793Z","shell.execute_reply.started":"2022-02-16T11:09:13.211973Z","shell.execute_reply":"2022-02-16T11:09:13.222761Z"}}},{"cell_type":"code","source":"def weight(x, max_x, alpha):\n    mask = (x <= max_x)\n    out = np.ones(x.shape)\n    out[mask] = (x[mask]/max_x)**alpha\n    return out\n\ndef update_glove_weights(x, w, d, alpha, max_x, learning_rate):\n    \"\"\"\n    x - square integer matrix VocabSize x VocabSize - coocurrence matrix\n    w - VocabSize x EmbSize - first word vectors\n    d - VocabSize x EmbSize - second word vectors\n    alpha - float - power in weight smoothing function f\n    max_x - int - maximum coocurrence count in weight smoothing function f\n    learning_rate - positive float - size of gradient step\n    \"\"\"\n    f = weight(x, max_x, alpha)\n    \n    pre = f*2*(w@d.T - np.log1p(x))\n    grad_w = pre @ d\n    grad_d = pre.T @ w\n    \n    w -= learning_rate * grad_w\n    d -= learning_rate * grad_d","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.4.7**","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:13:10.237243Z","iopub.execute_input":"2022-02-15T20:13:10.238127Z","iopub.status.idle":"2022-02-15T20:13:10.266539Z","shell.execute_reply.started":"2022-02-15T20:13:10.238011Z","shell.execute_reply":"2022-02-15T20:13:10.265524Z"}}},{"cell_type":"raw","source":"Вы обучили некоторую модель эмбеддингов и получили матрицу векторов слов.\n\nНапишите функцию, которая для заданного слова получит список наиболее похожих на него слов вместе с оценкой близости. Результат должен быть отсортирован по убыванию оценки сходства (наиболее похожие вначале). Само слово тоже должно присутствовать в выдаче.\n\nДля оценки сходства используйте отрицательное эвклидово расстояние sim(a,b)\n\nПеред вычислением сходства нормируйте векторы по L2-норме.\n\nТесты (включая тест для примера) генерируются случайно.\n\nФункция принимает на вход:\n\n    embeddings - матрица размерности VocabSize x EmbSize - заранее выученная матрица векторов слов (эмбеддингов)\n    query_word_id - целое неотрицательное число - идентификатор (номер) слова-запроса, для которого нужно найти наиболее похожие слова\n    get_n - целое неотрицательное число - наибольшее количество похожих слов, которое нужно вернуть из функции\n\nФункция должна возвращать список из не более чем get_n пар (номер слова, оценка сходства), отсортированных по убыванию оценки сходства со словом-запросом.","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\ndef get_nearest(embeddings, query_word_id, get_n):\n    \"\"\"\n    embeddings - VocabSize x EmbSize - word embeddings\n    query_word_id - integer - id of query word to find most similar to\n    get_n - integer - number of most similar words to retrieve\n\n    returns list of `get_n` tuples (word_id, similarity) sorted by descending order of similarity value\n    \"\"\"\n    norm_emb = (embeddings.T / (np.linalg.norm(embeddings,axis=-1) + 1e-9)).T\n    sims = -np.linalg.norm(norm_emb-norm_emb[query_word_id], axis=-1)\n    return [[arg, sims[arg]] for arg in np.argsort(-sims)[:get_n]]\n\n\nembeddings = np.array([[0.7299015792584768, 0.2915364327741303, 0.5307571134639943, 0.3101345732086396, \n                        0.8327085262119636, 0.39018382511314353, 0.678094726221033, 0.12372148102696612, \n                        0.5966533433209616], [0.5411155947267721, 0.046791742239819856, 0.5358832195593092, \n                        0.09894162419462038, 0.6350557173679914, 0.15126161842015717, 0.11375720216711405, \n                        0.46954553941325416, 0.8281402097264261], [0.5323869209381028, 0.2005012376766715,\n                        0.5925043884236925, 0.4621530177251649, 0.3886830034303448, 0.6403738184472031, \n                        0.23320289120963578, 0.43574647265888766, 0.5305633832484254]])\nquery_word_id = 0\nget_n = 8\n\nresult = get_nearest(embeddings, query_word_id, get_n)\n\nwrite_array(np.array(result))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T13:17:30.162351Z","iopub.execute_input":"2022-02-16T13:17:30.16272Z","iopub.status.idle":"2022-02-16T13:17:30.177059Z","shell.execute_reply.started":"2022-02-16T13:17:30.162683Z","shell.execute_reply":"2022-02-16T13:17:30.175829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.6.2**","metadata":{}},{"cell_type":"raw","source":"Примените свёртку с ядром (−0.5,0,0.5)(-0.5, 0, 0.5)(−0.5,0,0.5) к сигналу (1,1,2,3,3,3,2,1,1)(1, 1, 2, 3, 3, 3, 2, 1, 1)(1,1,2,3,3,3,2,1,1). Ответ запишите в виде последовательности чисел, разделённых пробелами. В качестве десятичного разделителя используйте точку. Входную последовательность не нужно дополнять фиктивными элементами (padding выключен).\n\nШаг свёртки (stride в PyTorch) считаем 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nkernel = np.array((-0.5,0,0.5))\nsignal = np.array((1,1,2,3,3,3,2,1,1), dtype=np.float32)\n\" \".join(map(str,([signal[i:i+len(kernel)]@kernel for i in range(len(signal) - len(kernel)+1)])))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T11:47:38.044826Z","iopub.execute_input":"2022-02-17T11:47:38.045281Z","iopub.status.idle":"2022-02-17T11:47:38.052383Z","shell.execute_reply.started":"2022-02-17T11:47:38.04522Z","shell.execute_reply":"2022-02-17T11:47:38.051792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.6.4**","metadata":{"execution":{"iopub.status.busy":"2022-02-17T11:43:39.189176Z","iopub.execute_input":"2022-02-17T11:43:39.18958Z","iopub.status.idle":"2022-02-17T11:43:39.19449Z","shell.execute_reply.started":"2022-02-17T11:43:39.189541Z","shell.execute_reply":"2022-02-17T11:43:39.193962Z"}}},{"cell_type":"code","source":"import numpy as np\nkernel = np.array([[1,1,0],\n                   [0,1,1]], dtype=np.float32) #Kernel(InChannels,K)\nX = np.array([[1,0],\n              [1,1],\n              [0,0],\n              [0,1],\n              [1,0]],dtype=np.float32) #X(InLen,InChannels)\nbias = 0\nout = [bias + X[i:i+kernel.shape[1],:].flatten()@kernel.T.flatten() for i in range(X.shape[0] - kernel.shape[1]+1)]\n\" \".join(map(str,out))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T08:46:49.033516Z","iopub.execute_input":"2022-02-19T08:46:49.034199Z","iopub.status.idle":"2022-02-19T08:46:49.06911Z","shell.execute_reply.started":"2022-02-19T08:46:49.034091Z","shell.execute_reply":"2022-02-19T08:46:49.068263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nkernel = np.array([[1,1,0],\n                   [0,1,1]], dtype=np.float32) #Kernel(InChannels,K)\nX = np.array([[1,0],\n              [1,1],\n              [0,0],\n              [0,1],\n              [1,0]],dtype=np.float32) #X(InLen,InChannels)\nbias = 0\nout = [bias + np.sum(X[i:i+kernel.shape[1],:]*kernel.T) for i in range(X.shape[0] - kernel.shape[1]+1)]\n\" \".join(map(str,out))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:02:56.045877Z","iopub.execute_input":"2022-02-19T09:02:56.046237Z","iopub.status.idle":"2022-02-19T09:02:56.05707Z","shell.execute_reply.started":"2022-02-19T09:02:56.046199Z","shell.execute_reply":"2022-02-19T09:02:56.056007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.6.5**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nkernel = np.array([[[1,1,0],\n                   [0,1,1]],\n                  [[1,0,0],\n                   [0,0,1]]], dtype=np.float32) #Kernel(OutChannels,InChannels,K)\nX = np.array([[1,0],\n              [1,1],\n              [0,0],\n              [0,1],\n              [1,0]],dtype=np.float32) #X(InLen,InChannels)\nbias = 0\nout = np.array([bias + np.sum(X[i:i+kernel.shape[2],:]*kernel.transpose((0,2,1)), axis=(1,2)) \\\n                for i in range(X.shape[0] - kernel.shape[2]+1)]).reshape(-1,kernel.shape[0])\nprint(\"\\n\".join(map(lambda row: str(row)[1:-1],out)))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T09:37:11.58848Z","iopub.execute_input":"2022-02-19T09:37:11.588927Z","iopub.status.idle":"2022-02-19T09:37:11.597909Z","shell.execute_reply.started":"2022-02-19T09:37:11.588896Z","shell.execute_reply":"2022-02-19T09:37:11.596823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#решение от Роман Григоров\n#Обновил на более оптимальное решение.\n#Разик делаем матричное умножение, потом суммы диагоналей с шагом считаем\nimport numpy as np\n\nkernel = np.array([[[1,1,0],\n                   [0,1,1]],\n                  [[1,0,0],\n                   [0,0,1]]], dtype=np.float32) #Kernel(OutChannels,InChannels,K)\nX = np.array([[1,0],\n              [1,1],\n              [0,0],\n              [0,1],\n              [1,0]],dtype=np.float32) #X(InLen,InChannels)\n\nx_kernel_product = X @ kernel\ny = np.array([np.trace(x_kernel_product, pos, 2) for pos in range(X.shape[0] - kernel.shape[2]+1)])\ny","metadata":{"execution":{"iopub.status.busy":"2022-02-19T10:04:12.169832Z","iopub.execute_input":"2022-02-19T10:04:12.170169Z","iopub.status.idle":"2022-02-19T10:04:12.182463Z","shell.execute_reply.started":"2022-02-19T10:04:12.170135Z","shell.execute_reply":"2022-02-19T10:04:12.181349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.7.Разбор видео**","metadata":{"execution":{"iopub.status.busy":"2022-02-22T08:41:37.12051Z","iopub.execute_input":"2022-02-22T08:41:37.120856Z","iopub.status.idle":"2022-02-22T08:41:37.149363Z","shell.execute_reply.started":"2022-02-22T08:41:37.120772Z","shell.execute_reply":"2022-02-22T08:41:37.148434Z"}}},{"cell_type":"code","source":"import numpy as np\n\nkern_size = 3\nsequence_len = 7\nin_channel = 4\n\nx = np.array([[1,2,3,4,5,6,7],\n              [10,20,30,40,50,60,70],\n              [1,2,3,4,5,6,7],\n              [1,2,3,4,5,6,7]])\nchunks = []\nchunk_size = sequence_len - kern_size + 1\nfor offset in range(kern_size):\n    chunks.append(x[:,offset:offset+chunk_size])\nchunks","metadata":{"execution":{"iopub.status.busy":"2022-02-22T08:02:37.350906Z","iopub.execute_input":"2022-02-22T08:02:37.351352Z","iopub.status.idle":"2022-02-22T08:02:37.360216Z","shell.execute_reply.started":"2022-02-22T08:02:37.351301Z","shell.execute_reply":"2022-02-22T08:02:37.359262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.8.1**","metadata":{}},{"cell_type":"code","source":"def apply_convolution(data, kernel, bias):\n    \"\"\"\n    data - InLen x InChannels\n    kernel - OutChannels x InChannels x KernelSize\n    bias - OutChannels\n\n    returns OutLen x OutChannels\n    \"\"\"\n    return np.array([bias + np.sum(data[i:i+kernel.shape[2],:]*kernel.transpose((0,2,1)), axis=(1,2)) \\\n                for i in range(data.shape[0] - kernel.shape[2]+1)]).reshape(-1,kernel.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T13:12:12.300112Z","iopub.execute_input":"2022-02-27T13:12:12.300487Z","iopub.status.idle":"2022-02-27T13:12:12.331615Z","shell.execute_reply.started":"2022-02-27T13:12:12.300394Z","shell.execute_reply":"2022-02-27T13:12:12.330967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.8.2**","metadata":{}},{"cell_type":"code","source":"def calculate_kernel_grad(x, y, kernel, bias):\n    \"\"\"\n    x - InLen x InChannels\n    y - OutLen x OutChannels\n    kernel - OutChannels x InChannels x KernelSize\n    bias - OutChannels\n\n    returns OutChannels x InChannels x KernelSize\n    \"\"\"\n    k_ones = np.ones(kernel.shape).transpose((0,2,1))\n    return np.sum([x[i:i+k_ones.shape[1],:]*k_ones for i in range(x.shape[0] - k_ones.shape[1]+1)],axis=0).transpose((0,2,1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_kernel_grad(x, y, kernel, bias):\n    \"\"\"\n    x - InLen x InChannels\n    y - OutLen x OutChannels\n    kernel - OutChannels x InChannels x KernelSize\n    bias - OutChannels\n\n    returns OutChannels x InChannels x KernelSize\n    \"\"\"\n    return np.repeat(np.sum([x[i:i+kernel.shape[2],:] for i in range(x.shape[0] - kernel.shape[2]+1)],axis=0)\\\n          .transpose((1,0))[np.newaxis,:,:],kernel.shape[0],axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.8.3**","metadata":{}},{"cell_type":"code","source":"def calculate_conv_x_grad(x, y, kernel, bias):\n    \"\"\"\n    x - InLen x InChannels\n    y - OutLen x OutChannels\n    kernel - OutChannels x InChannels x KernelSize\n    bias - OutChannels\n\n    returns InLen x InChannels\n    \"\"\"\n    stkernel = np.sum(kernel, axis=0).T\n    x_grad = np.zeros(x.shape)\n    for i in range(x.shape[0] - kernel.shape[2]+1):\n        x_grad[i:i+kernel.shape[2],:] += stkernel\n    return x_grad","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.8.4**","metadata":{}},{"cell_type":"code","source":"def calculate_receptive_field(layers):\n    \"\"\"\n    layers - list of LayerInfo\n\n    returns int - receptive field size\n    \"\"\"\n    return 1 + sum(map(lambda x: (x.kernel_size-1)*x.dilation, layers))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.4.4**","metadata":{"execution":{"iopub.status.busy":"2022-03-11T06:56:12.205451Z","iopub.execute_input":"2022-03-11T06:56:12.20594Z","iopub.status.idle":"2022-03-11T06:56:12.211407Z","shell.execute_reply.started":"2022-03-11T06:56:12.205903Z","shell.execute_reply":"2022-03-11T06:56:12.210282Z"}}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nW = torch.tensor([[1.,0.,2.],\n                  [0.,1.,3.],\n                  [1.,3.,0.],\n                  [0.,0.,0.]]) #InLen x EmbSize\nquery = torch.tensor([0.,0.,1.])\n\n#насколько каждое слово (токен) релевантно (похоже на) слову-запросу (на выходе каждого слова скаляр, \n#чем он больше, тем выше релевантность данного слова)\nunnorm_scores = W @ query #InLen\nprint(unnorm_scores)\n\n#нормируем через softmax\natt_scores = F.softmax(unnorm_scores, dim=-1) #InLen\nprint(att_scores)\n\n#каждое измерение эмбеддинга сжимаем до скаляра на основе полученной нормированной релевантности (значимости)\nresult = W.T @ att_scores #EmbSize\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T07:18:28.955968Z","iopub.execute_input":"2022-03-11T07:18:28.956315Z","iopub.status.idle":"2022-03-11T07:18:28.970012Z","shell.execute_reply.started":"2022-03-11T07:18:28.956274Z","shell.execute_reply":"2022-03-11T07:18:28.969095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.5.2**","metadata":{}},{"cell_type":"code","source":"def print_matrix(matrix):\n    print('\\n'.join([' '.join(map(lambda x: '{:.2f}'.format(x), row)) for row in list(matrix)]))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T12:55:15.066848Z","iopub.execute_input":"2022-03-11T12:55:15.067409Z","iopub.status.idle":"2022-03-11T12:55:15.072465Z","shell.execute_reply.started":"2022-03-11T12:55:15.067374Z","shell.execute_reply":"2022-03-11T12:55:15.071783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nW = torch.tensor([[1.,0.],\n                  [0.,1.],\n                  [1.,1.],\n                  [0.,0.]]) #InLen x EmbSize\nlogits = W @ W.T #InLen x InLen\nprint_matrix(logits)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T11:02:36.938419Z","iopub.execute_input":"2022-03-11T11:02:36.938703Z","iopub.status.idle":"2022-03-11T11:02:36.94601Z","shell.execute_reply.started":"2022-03-11T11:02:36.938675Z","shell.execute_reply":"2022-03-11T11:02:36.944779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.5.3**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\natt_scores = F.softmax(logits, dim=-1)\nprint(att_scores)\nresult = att_scores @ W\nprint_matrix(result)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T11:03:57.344563Z","iopub.execute_input":"2022-03-11T11:03:57.344879Z","iopub.status.idle":"2022-03-11T11:03:57.353246Z","shell.execute_reply.started":"2022-03-11T11:03:57.344846Z","shell.execute_reply":"2022-03-11T11:03:57.352296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.5.4**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nInput = torch.tensor([[1.,0.],\n                      [0.,1.],\n                      [1.,1.],\n                      [0.,0.]]) #InLen x EmbSize\nProjK = torch.tensor([[1.,0.],\n                      [0.,0.]]) #EmbSize x EmbSize\nProjQ = torch.tensor([[0.,0.],\n                      [1.,0.]]) #EmbSize x EmbSize\nProjV = torch.tensor([[1.,0.],\n                      [0.,1.]]) #EmbSize x EmbSize\nBiasK = BiasQ = BiasV = torch.tensor([0.,0.])\n\nKeys = Input @ ProjK + BiasK\nQueries = Input @ ProjQ + BiasQ\nValues = Input @ ProjV + BiasV\n\nLogits = Queries @ Keys.T #InLen x InLen\nAttScores = F.softmax(Logits, dim=-1)\nResult = AttScores @ Values\nprint_matrix(Result)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T11:39:34.558063Z","iopub.execute_input":"2022-03-11T11:39:34.558651Z","iopub.status.idle":"2022-03-11T11:39:34.5746Z","shell.execute_reply.started":"2022-03-11T11:39:34.558594Z","shell.execute_reply":"2022-03-11T11:39:34.573448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.5.5**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nInput = torch.tensor([[1.,0.],\n                      [0.,1.],\n                      [1.,1.],\n                      [0.,0.]]) #InLen x EmbSize\n\nProj1K = torch.tensor([[1.,0.],\n                       [0.,0.]]) #\nProj1Q = torch.tensor([[0.,1.],\n                       [1.,0.]]) #\nProj1V = torch.tensor([[1.],\n                       [0.]]) #\n\nProj2K = torch.tensor([[0.,0.],\n                       [1.,0.]]) #\nProj2Q = torch.tensor([[1.,1.],\n                       [1.,1.]]) #\nProj2V = torch.tensor([[0.],\n                       [1.]]) #\n\nBiasK = BiasQ = torch.tensor([0.,0.])\nBiasV = torch.tensor([0.])\n\nKeys1 = Input @ Proj1K + BiasK\nprint(Keys1)\nQueries1 = Input @ Proj1Q + BiasQ\nprint(Queries1)\nValues1 = Input @ Proj1V + BiasV\nprint(Values1)\n\nKeys2 = Input @ Proj2K + BiasK\nQueries2 = Input @ Proj2Q + BiasQ\nValues2 = Input @ Proj2V + BiasV\n\nLogits1 = Queries1 @ Keys1.T #InLen x InLen\nprint(Logits1)\nLogits2 = Queries2 @ Keys2.T #InLen x InLen\n\nAttScores1 = F.softmax(Logits1, dim=-1)\nAttScores2 = F.softmax(Logits2, dim=-1)\n\nResult1 = AttScores1 @ Values1\nprint(Result1)\nResult2 = AttScores2 @ Values2\nResult = torch.cat([Result1, Result2], dim=1)\nprint_matrix(Result)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T13:01:24.755793Z","iopub.execute_input":"2022-03-11T13:01:24.756856Z","iopub.status.idle":"2022-03-11T13:01:24.775664Z","shell.execute_reply.started":"2022-03-11T13:01:24.756801Z","shell.execute_reply":"2022-03-11T13:01:24.774771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.7.1**","metadata":{}},{"cell_type":"code","source":"def max_pooling(features, kernel_size):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    kernel_size - positive integer - size of sliding window\n\n    returns tuple of two matrices of shape OutLen x EmbSize:\n         - output features (main result)\n         - relative indices of maximum elements for each position of sliding window\n    \"\"\"\n    return np.stack((np.max(features[i:i+kernel_size],axis=0) for i in range(features.shape[0] - kernel_size + 1))),\\\n           np.stack((np.argmax(features[i:i+kernel_size],axis=0) for i in range(features.shape[0] - kernel_size + 1)))","metadata":{"execution":{"iopub.status.busy":"2022-03-22T16:29:27.6976Z","iopub.execute_input":"2022-03-22T16:29:27.697895Z","iopub.status.idle":"2022-03-22T16:29:27.704508Z","shell.execute_reply.started":"2022-03-22T16:29:27.697864Z","shell.execute_reply":"2022-03-22T16:29:27.703671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#из лучших решений (не мое)\n'''\nКоммент:\n1. Создаём матрицу по которой посчитанные в один чих max, argmax будут нашими ответами\n2. Делаем чих\nPS: Если столько памяти нет, то добавляем max или argmax в первую строчку решения\nPS: На pytorch можно сделать через expand и вообще не пострадать по памяти\n'''\ndef max_pooling(features, kernel_size):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    kernel_size - positive integer - size of sliding window\n\n    returns tuple of two matrices of shape OutLen x EmbSize:\n         - output features (main result)\n         - relative indices of maximum elements for each position of sliding window\n    \"\"\"\n\n    features_stacked = np.stack((features[i:i+kernel_size, ] for i in range(1 + features.shape[0] - kernel_size)))\n    result = np.max(features_stacked, axis=1)\n    indices = np.argmax(features_stacked, axis=1)\n    return result, indices","metadata":{"execution":{"iopub.status.busy":"2022-03-22T16:08:58.053003Z","iopub.execute_input":"2022-03-22T16:08:58.053334Z","iopub.status.idle":"2022-03-22T16:08:58.08106Z","shell.execute_reply.started":"2022-03-22T16:08:58.053252Z","shell.execute_reply":"2022-03-22T16:08:58.080194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.7.2**","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef max_pooling_dldfeatures(features, kernel_size, indices, dldout):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    kernel_size - positive integer - size of sliding window\n    indices - OutLen x EmbSize - relative indices of maximum elements for each window position\n    dldout - OutLen x EmbSize - partial derivative of loss function with respect to outputs of max_pooling layer\n\n    returns InLen x EmbSize\n    \"\"\"\n    out = np.zeros((features.shape))\n    for i in range(out.shape[0] - kernel_size + 1):\n        indixes = indices[i] * out.shape[1] + np.arange(out.shape[1])\n        out[i:i+kernel_size].flat[indixes] += dldout[i]\n    return out\n\n\nfeatures = np.array([[-1.2420542766989977, -0.045100789663994285, 1.858151857421511, 0.10732741246325356], \n                     [-1.480497780371414, -0.12486054931133332, -0.18422425981847368, -1.4228130362490647], \n                     [-0.8417536968892625, 0.9802583655274091, -0.18413492661665792, -1.5582607186399924], \n                     [1.325799250424393, 0.08149768959330334, -1.454876921308986, 0.1408031456023352], \n                     [0.1637602967235608, -0.21250114632967532, 0.8362859721448469, 0.717774697701287], \n                     [-0.7641399532198978, -2.112568530488304, 0.20121440705964902, 0.015624280892385661], \n                     [1.3862200103422582, 0.6508694196448389, -1.162417318743681, 1.5202488401790915], \n                     [1.3947418297193952, -1.013483406336198, -2.0608332074129545, -1.733019236247151], \n                     [1.0932612618870112, 0.8071262618398916, 0.15924519176972282, -0.6885825807454318]])\nkernel_size = 6\nindices = np.array([[3.0, 2.0, 0.0, 4.0], \n                    [5.0, 1.0, 3.0, 5.0], \n                    [5.0, 0.0, 2.0, 4.0], \n                    [4.0, 5.0, 1.0, 3.0]]).astype('uint32')\ndldout = np.array([[-0.0763791951131031, -0.8729161683329371, 0.7454337173675266, -2.508470377801969], \n                   [-0.9080189656976042, 0.6952579391985969, 0.2829942797947518, 0.35396918585149195], \n                   [0.339009358836277, -1.9496823733556254, -0.11017174942549533, 1.4591363247582954], \n                   [-1.1161622731011638, -2.371055190136431, -1.174384738333498, 0.4672192180206214]])\n\ndldfeatures = max_pooling_dldfeatures(features, kernel_size, indices, dldout)\n\ndldfeatures","metadata":{"execution":{"iopub.status.busy":"2022-03-22T18:39:25.048103Z","iopub.execute_input":"2022-03-22T18:39:25.048478Z","iopub.status.idle":"2022-03-22T18:39:25.073318Z","shell.execute_reply.started":"2022-03-22T18:39:25.048444Z","shell.execute_reply":"2022-03-22T18:39:25.07267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#более интересное решение\ndef max_pooling_dldfeatures(features, kernel_size, indices, dldout):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    kernel_size - positive integer - size of sliding window\n    indices - OutLen x EmbSize - relative indices of maximum elements for each window position\n    dldout - OutLen x EmbSize - partial derivative of loss function with respect to outputs of max_pooling layer\n\n    returns InLen x EmbSize\n    \"\"\"\n    result = np.zeros(features.shape)\n    for i in range(features.shape[0]-kernel_size+1):\n        result[indices[i, :]+i, range(features.shape[1])] += dldout[i,:]\n    return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#и еще более интересное решение\ndef max_pooling_dldfeatures(features, kernel_size, indices, dldout):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    kernel_size - positive integer - size of sliding window\n    indices - OutLen x EmbSize - relative indices of maximum elements for each window position\n    dldout - OutLen x EmbSize - partial derivative of loss function with respect to outputs of max_pooling layer\n\n    returns InLen x EmbSize\n    \"\"\"\n    doutdfeatures = np.zeros(features.shape)\n    out_len = indices.shape[0]\n    for i in range(kernel_size):\n        doutdfeatures[i:i+out_len] += (indices == i) * dldout\n        \n    return doutdfeatures\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.7.3**","metadata":{}},{"cell_type":"code","source":"def softmax(x):\n    \"\"\"\n    x - vector of n elements - input\n\n    returns vector of n elements - softmax output\n    \"\"\"\n    return np.exp(x)/np.sum(np.exp(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.7.4**","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef dsoftmax_dx(x):\n    \"\"\"\n    x - vector of n elements - input\n\n    returns matrix n x n\n    \"\"\"\n    return np.exp(x)/np.sum(np.exp(x)) * (np.diag(np.ones(x.shape[0])) - (np.exp(x)/np.sum(np.exp(x)))).T\n\n\nx = np.array([-0.36170314084137395, 1.531638983431016, -1.7131284538840788, -0.9027503682845508, \n              -0.8591376176087115, 0.16481576122888014, 0.2286590883015934, 0.4686776665093307, \n              -0.4880318948728026, 0.13865483857501165, -1.0740873577508447, -1.1693607815929845, \n              0.6388250392697977])\n\nresult = dsoftmax_dx(x)\n\nwrite_array(result)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T10:33:50.594312Z","iopub.execute_input":"2022-03-23T10:33:50.594708Z","iopub.status.idle":"2022-03-23T10:33:50.603269Z","shell.execute_reply.started":"2022-03-23T10:33:50.594682Z","shell.execute_reply":"2022-03-23T10:33:50.602095Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**4.7.5**","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef attention(features, query):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    query - EmbSize - features of query object\n\n    returns vector of size EmbSize - features, aggregated according to the query\n    \"\"\"\n    #насколько каждое слово (токен) релевантно (похоже на) слову-запросу (на выходе каждого слова скаляр, \n    #чем он больше, тем выше релевантность данного слова)\n    unnorm_scores = features @ query #InLen\n\n    #нормируем через softmax\n    att_scores = np.exp(unnorm_scores)/np.sum(np.exp(unnorm_scores), axis=-1) #InLen\n\n    #каждое измерение эмбеддинга сжимаем до скаляра на основе полученной нормированной релевантности (значимости)\n    return features.T @ att_scores #EmbSize\n\n\nfeatures = np.array([[0.3504305689198156, 0.871844425624726, 0.29345316540775357, 0.49159320438393916, 0.16391992930609034, 0.24589641847050037, 0.34921020303336925, 0.09968814035867879, 0.8652385667745919, 0.00906484385602968, 0.6134586521086117, 0.08104312584086149, 0.643129733435556, 0.6610968673257929, 0.6825169003800382], [0.005641686042561211, 0.3397733866278605, 0.4408793722092307, 0.6618752692611525, 0.4192615374283991, 0.6718589897811911, 0.23503584107912667, 0.9972834040264165, 0.6907780153811639, 0.5160598448726361, 0.4200243418824855, 0.7745997472321381, 0.9124177261957108, 0.627661131744206, 0.7792319239076758], [0.44947044223343224, 0.39851993627332394, 0.27645205987950927, 0.3360502940952873, 0.20207394761469466, 0.27730469648938627, 0.9647449489128369, 0.38480306917172535, 0.7014748335636187, 0.5616919724157547, 0.3082991954077743, 0.43320540280287834, 0.7682716834674514, 0.04669826413239708, 0.7975639937877288], [0.3529089999231677, 0.5085801437940869, 0.6686697864089949, 0.9579051761714787, 0.14893344048972235, 0.6504801381978066, 0.6554087909852483, 0.2182290972559655, 0.8874805349214916, 0.8549111586624765, 0.2256959432511686, 0.7090739886051667, 0.9215898392742404, 0.8361038069049278, 0.9807575571901945], [0.93096165894251, 0.21204415175966806, 0.005393301311816812, 0.6868163541395496, 0.17651743121260177, 0.4276211882347165, 0.7172630747046246, 0.6222321154458413, 0.782866044726867, 0.9401403417712956, 0.6009251310321828, 0.7689712777389628, 0.011137370287858661, 0.6750220130270511, 0.3656918897133191], [0.344423832576648, 0.5200078573131781, 0.08090528060543856, 0.6187002344784092, 0.24428489996011238, 0.18400539459399, 0.40308101020726217, 0.19255989698913867, 0.8010944590934469, 0.20324438899818598, 0.4927144298170735, 0.03783988662477278, 0.7705093963091103, 0.2520865403496373, 0.40266725180440743], [0.6681310493060861, 0.2801674372250399, 0.6224648405839522, 0.6287746784150413, 0.864080498689899, 0.23833127705610258, 0.20311743810136906, 0.6646132937899738, 0.23575457417289924, 0.1869625695994146, 0.7712148738157554, 0.15237041670323637, 0.2763150373902683, 0.46500408886101585, 0.991468614310106], [0.47955269815875845, 0.18371674117676162, 0.4749895427072034, 0.5127159626377625, 0.14327300286458633, 0.5921086963579639, 0.21467664382766927, 0.08984875049424312, 0.5619088573772313, 0.6324525220346037, 0.65145500723789, 0.5118736033583858, 0.3791794826772541, 0.7062193547285907, 0.12888775429739185], [0.8936198110390413, 0.1499351596848777, 0.23230300209801535, 0.6275970485906217, 0.14179412142521963, 0.44590423506527455, 0.6398118989481705, 0.44025473834142315, 0.9690917160909921, 0.7329911430579539, 0.4723409208689966, 0.30051845327308735, 0.6517065287372249, 0.11682964074366375, 0.3356912564688531], [0.5819483213886298, 0.12715730125007862, 0.6624081011547434, 0.27210122174739293, 0.3321414469174103, 0.6738692045564213, 0.49979407643990537, 0.923095453187033, 0.8688108133354637, 0.29672803800781, 0.8422355709858387, 0.22967466587429908, 0.48606633908371566, 0.3302629931498261, 0.9271715208940098], [0.814229644181095, 0.6269508015754929, 0.19067116118180638, 0.6597416333912787, 0.3042396495694798, 0.5349586078017191, 0.9889297007928726, 0.5059647631701082, 0.6586214714303461, 0.19972197385065704, 0.730120041302739, 0.9254129585548022, 0.7774768791337286, 0.5880525770183761, 0.4404909426586451], [0.8393608070151912, 0.551470751477307, 0.3776646281929925, 0.7403545806778788, 0.01464073752506101, 0.49682079457661743, 0.12829037985166736, 0.8323709714882789, 0.4861583628986299, 0.10966510942571872, 0.36384711262095637, 0.008343156485128067, 0.05481969871494197, 0.11036480291979456, 0.3495717657917], [0.575668909069271, 0.1948209406820347, 0.5066632418120769, 0.5610866065811511, 0.7503051258152065, 0.20250301475454058, 0.9387177222181186, 0.4214964558859865, 0.2441688535705906, 0.2852282954051667, 0.7185375048873539, 0.09961745251862686, 0.507873295740294, 0.9713796833363287, 0.8218946227484244], [0.14519733396011691, 0.07264015089790021, 0.7254237309701331, 0.7437297525624584, 0.3465971472185204, 0.6489212261982703, 0.2152569561085349, 0.6476151760429897, 0.2045187871395916, 0.9599712380254137, 0.28554199184758966, 0.7701922251424572, 0.7095119328780166, 0.7579558453415812, 0.4251898428876446]])\nquery = np.array([0.30760147020407946, 0.1528992448227442, 0.9387231083505163, 0.12201982125460176, 0.3159744925438269, 0.555332538642272, 0.8654043562058316, 0.5523485724329922, 0.6405492495162189, 0.8421217300945876, 0.03415012932624606, 0.0914780538557024, 0.745151636966557, 0.9885343010237021, 0.02289480154711454])\n\nresult = attention(features, query)\n\nwrite_array(result)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:31:19.667094Z","iopub.execute_input":"2022-03-23T11:31:19.667640Z","iopub.status.idle":"2022-03-23T11:31:19.693129Z","shell.execute_reply.started":"2022-03-23T11:31:19.667536Z","shell.execute_reply":"2022-03-23T11:31:19.691624Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**4.7.6**","metadata":{}},{"cell_type":"code","source":"import sys\nimport ast\nimport numpy as np\n\n\ndef parse_array(s):\n    return np.array(ast.literal_eval(s))\n\ndef read_array():\n    return parse_array(sys.stdin.readline())\n\ndef write_array(arr):\n    print(repr(arr.tolist()))\n\n\ndef self_attention(features, proj_k, bias_k, proj_q, bias_q, proj_v, bias_v):\n    \"\"\"\n    features - InLen x EmbSize - features of elements of input sequence\n    proj_k - EmbSize x EmbSize - projection matrix to make keys from features\n    bias_k - EmbSize - bias vector to make keys from features\n    proj_q - EmbSize x EmbSize - projection matrix to make queries from features\n    bias_q - EmbSize - bias vector to make queries from features\n    proj_v - EmbSize x EmbSize - projection matrix to make values from features\n    bias_v - EmbSize - bias vector to make values from features\n\n    returns InLen x EmbSize\n    \"\"\"\n    keys = features @ proj_k + bias_k \n    queries = features @ proj_q + bias_q\n    values = features @ proj_v + bias_v\n\n    logits = queries @ keys.T #InLen x InLen\n    att_scores = (np.exp(logits.T)/np.sum(np.exp(logits.T), axis=0)).T #InLen x InLen\n    \n    return att_scores @ values\n\n\nfeatures = np.array([[0.5175129778200084, 0.13021330700949507], \n                     [-0.3578609445921744, -0.07768163060380659], \n                     [-0.046577477754636734, -0.12288550821838619], \n                     [0.4424092505449793, -1.431399548551344], \n                     [0.753992222548331, -1.1210257338970167], \n                     [1.6736061037504428, -1.9789731491226337], \n                     [-1.4985152255486565, -1.6614802556117283], \n                     [-0.610065708073959, -0.8475335063027695], \n                     [-0.1657640783522184, -1.7079776825852762], \n                     [0.7857341373616981, 0.2956255012408635], \n                     [-0.49243028413686984, 0.01065675311085114], \n                     [0.20598401523943788, -0.6339670563637549], \n                     [-0.15698934123474126, 0.9516567843056503], \n                     [-0.08965595798444444, 0.9923765422389716], \n                     [-0.9649404480809814, -0.6203623955866846]])\nproj_k = np.array([[-0.4777373377067222, 1.384780896738418], \n                   [1.5537173245233542, -1.6151073640132454]])\nbias_k = np.array([0.34068677065672126, -1.7225350645946451])\n\nproj_q = np.array([[1.667192089239799, 0.9072106203091014], \n                   [1.0017863742909645, -0.8578876449703756]])\nbias_q = np.array([-0.3811843050970959, -0.15922065479353645])\n\nproj_v = np.array([[2.396375885002737, 0.23995979796695774], \n                   [0.21631803999882093, -0.6475173781192963]])\nbias_v = np.array([1.448781271879823, -0.7144164516316529])\n\nresult = self_attention(features, proj_k, bias_k, proj_q, bias_q, proj_v, bias_v)\n\nresult","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:48:55.857936Z","iopub.execute_input":"2022-03-23T11:48:55.858860Z","iopub.status.idle":"2022-03-23T11:48:55.876945Z","shell.execute_reply.started":"2022-03-23T11:48:55.858830Z","shell.execute_reply":"2022-03-23T11:48:55.875995Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"keys = features @ proj_k + bias_k\nkeys","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:23:22.272220Z","iopub.execute_input":"2022-03-23T11:23:22.272951Z","iopub.status.idle":"2022-03-23T11:23:22.283035Z","shell.execute_reply.started":"2022-03-23T11:23:22.272880Z","shell.execute_reply":"2022-03-23T11:23:22.281965Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"queries = features @ proj_q + bias_q\nqueries","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:23:27.456354Z","iopub.execute_input":"2022-03-23T11:23:27.456863Z","iopub.status.idle":"2022-03-23T11:23:27.467818Z","shell.execute_reply.started":"2022-03-23T11:23:27.456832Z","shell.execute_reply":"2022-03-23T11:23:27.464669Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"values = features @ proj_v + bias_v\nvalues","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:23:41.068353Z","iopub.execute_input":"2022-03-23T11:23:41.069746Z","iopub.status.idle":"2022-03-23T11:23:41.080173Z","shell.execute_reply.started":"2022-03-23T11:23:41.069701Z","shell.execute_reply":"2022-03-23T11:23:41.078613Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"logits = queries @ keys.T #InLen x InLen\nlogits.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:24:01.886108Z","iopub.execute_input":"2022-03-23T11:24:01.886862Z","iopub.status.idle":"2022-03-23T11:24:01.895890Z","shell.execute_reply.started":"2022-03-23T11:24:01.886822Z","shell.execute_reply":"2022-03-23T11:24:01.894762Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"att_scores = np.exp(logits)/np.sum(np.exp(logits), axis=1) #InLen\nsum(att_scores[:,0])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:27:40.894327Z","iopub.execute_input":"2022-03-23T11:27:40.894671Z","iopub.status.idle":"2022-03-23T11:27:40.904105Z","shell.execute_reply.started":"2022-03-23T11:27:40.894638Z","shell.execute_reply":"2022-03-23T11:27:40.902868Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"logits = np.array([[1.,1.,1.],\n                   [-999., 1.,1.],\n                   [-999., -9999.,1.]])\nlogits","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:38:36.864273Z","iopub.execute_input":"2022-03-23T11:38:36.864557Z","iopub.status.idle":"2022-03-23T11:38:36.872471Z","shell.execute_reply.started":"2022-03-23T11:38:36.864530Z","shell.execute_reply":"2022-03-23T11:38:36.871637Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"att_scores = (np.exp(logits.T)/np.sum(np.exp(logits.T), axis=0)).T #InLen\natt_scores","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:41:09.043004Z","iopub.execute_input":"2022-03-23T11:41:09.043282Z","iopub.status.idle":"2022-03-23T11:41:09.051943Z","shell.execute_reply.started":"2022-03-23T11:41:09.043253Z","shell.execute_reply":"2022-03-23T11:41:09.051144Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nInput = torch.tensor([[1.,0.],\n                      [0.,1.],\n                      [1.,1.],\n                      [0.,0.]]) #InLen x EmbSize\nProjK = torch.tensor([[1.,0.],\n                      [0.,0.]]) #EmbSize x EmbSize\nProjQ = torch.tensor([[0.,0.],\n                      [1.,0.]]) #EmbSize x EmbSize\nProjV = torch.tensor([[1.,0.],\n                      [0.,1.]]) #EmbSize x EmbSize\nBiasK = BiasQ = BiasV = torch.tensor([0.,0.])\n\nKeys = Input @ ProjK + BiasK\nQueries = Input @ ProjQ + BiasQ\nValues = Input @ ProjV + BiasV\n\nLogits = Queries @ Keys.T #InLen x InLen\nAttScores = F.softmax(Logits, dim=-1)\nResult = AttScores @ Values\nprint_matrix(Result)","metadata":{},"execution_count":null,"outputs":[]}]}