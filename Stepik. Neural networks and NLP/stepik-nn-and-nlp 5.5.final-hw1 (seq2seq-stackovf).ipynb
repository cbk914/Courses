{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Домашнее задание №1\n\nПри обучении seq2seq модели из семинара качество работы модели оказалось не слишком хорошим. У этого есть несколько причин:\n    - Слишком маленький датасет. На 2000 пар сложно обучить хорошую модель для решения такой сложной задачи.\n    - Модель очень простая, есть смысл попробовать усложнить ее (использовать другие архитектуры, например, Трансформер, разобранный в предыдущих лекциях и семинарах).\n    - Стоит более аккуратно подбирать параметры модели. Обратите внимание на:\n         -- Процесс построения словаря. Может быть, нужно поварьировать параметры min_freq, max_freq.\n         -- Dropout. Есть ли смысл добавлять еще больше dropout в модель?\n         -- Стратегия изменения learning rate в процессе обучения.\n\nВ качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара и улучшить качество работы модели. В репозитории курса выложены два датасета: один из них (conala, 2000 пар на обучение) был разобран видео, второй (StaQC, 50000 пар на обучение) Вам предлагается использовать в домашней работе. Для начала Вы можете проверить, помогает ли улучшить качество работы модели использование большего количества данных без каких-либо дополнительных улучшений.","metadata":{}},{"cell_type":"raw","source":"Итоговые задания для меня:\n    - понять код семинара, где нужно, добавить соответсвующие комментарии\n    - сохранить (через Save Version) текущий baseline (на датасете conala, 2000) со всеми его выводами\n    - запустить baseline на датасете StaQC, 50000, сравить результаты с данными на датасете conala, 2000\n    - оценить качество данных в датасете, если нужно обработать дополнительно (почистить и т.п.)\n    - определиться с метриками и конкретными примерами для дальнейших сравнений результатов экспериментов\n    - поэкспериментировать с baseline_StaQC (но без излишнего увлечения)\n         -- Процесс построения словаря. Может быть, нужно поварьировать параметры min_freq, max_freq.\n         -- Dropout. Есть ли смысл добавлять еще больше dropout в модель?\n         -- Стратегия изменения learning rate в процессе обучения\n    - применить для решения задачи другую архитектуру, а именно Трансформер\n    - поэкспериментировать с Transformer_StaQC\n         -- ...\n         -- ...\n         -- ...\n    - сравнить результаты baseline_StaQC и Transformer_StaQC","metadata":{}},{"cell_type":"markdown","source":"# Генерация кода по вопросам со StackOverflow\n","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle,\n# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n\n# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n# import sys; sys.path.append('./stepik-dl-nlp')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom torchtext.datasets import TranslationDataset, Multi30k\nfrom torchtext.data import Field, BucketIterator\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nimport spacy\n\nimport random\nimport math\nimport time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1234\n\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef tokenize_question(text):\n    \"\"\"\n    Tokenizes question from a string into a list of strings (tokens) and reverses it\n    \"\"\"\n    return list(filter(lambda x: len(x) < 16, re.findall(r\"[\\w']+\", text)[::-1]))\n\ndef tokenize_snippet(text):\n    \"\"\"\n    Tokenizes code snippet into a list of operands\n    \"\"\"\n    return list(filter(lambda x: len(x) < 10, re.findall(r\"[\\w']+|[.,!?;:@~(){}\\[\\]+-/=\\\\\\'\\\"\\`]\", text)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchtext import data, datasets\n\nSRC = data.Field(\n    tokenize = tokenize_question, \n    init_token = '<sos>', \n    eos_token = '<eos>', \n    lower = True,\n    include_lengths = True\n)\n\nTRG = data.Field(\n    tokenize = tokenize_snippet, \n    init_token = '<sos>', \n    eos_token = '<eos>', \n    lower = True\n)\n\nfields = {\n    'intent': ('src', SRC),\n    'snippet': ('trg', TRG)\n}\n\n# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntrain_data, valid_data, test_data = data.TabularDataset.splits(\n                            path = 'datasets/stackoverflow_code_generation/conala/',\n                            train = 'conala-train.csv',\n                            validation = 'conala-valid.csv',\n                            test = 'conala-test.csv',\n                            format = 'csv',\n                            fields = fields\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SRC.build_vocab([train_data.src], max_size=25000, min_freq=3)\nprint(SRC.vocab.freqs.most_common(20))\n\n\nTRG.build_vocab([train_data.trg], min_freq=5)\nprint(TRG.vocab.freqs.most_common(20))\n\nprint(f\"Уникальные токены в словаре интентов: {len(SRC.vocab)}\")\nprint(f\"Уникальные токены в словаре сниппетов: {len(TRG.vocab)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Размер обучающей выборки: {len(train_data.examples)}\")\nprint(f\"Размер валидационной выборки: {len(valid_data.examples)}\")\nprint(f\"Размер тестовой выборки: {len(test_data.examples)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.cuda.set_device(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 2\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data), \n     batch_size = BATCH_SIZE,\n     sort_within_batch = True,\n     sort_key = lambda x : len(x.src),\n     device = device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        \n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n        \n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, src_len):\n        \n        #src = [src sent len, batch size]\n        #src_len = [src sent len]\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        #embedded = [src sent len, batch size, emb dim]\n        \n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n        \n        packed_outputs, hidden = self.rnn(packed_embedded)\n                     \n        #packed_outputs is a packed sequence containing all hidden states\n        #hidden is now from the final non-padded element in the batch\n            \n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n            \n        #outputs is now a non-packed sequence, all hidden states obtained\n        #  when the input is a pad token are all zeros\n            \n        #outputs = [sent len, batch size, hid dim * num directions]\n        #hidden = [n layers * num directions, batch size, hid dim]\n        \n        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n        #outputs are always from the last layer\n        \n        #hidden [-2, :, : ] is the last of the forwards RNN \n        #hidden [-1, :, : ] is the last of the backwards RNN\n        \n        #initial decoder hidden is final hidden state of the forwards and backwards \n        #  encoder RNNs fed through a linear layer\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n        \n        #outputs = [sent len, batch size, enc hid dim * 2]\n        #hidden = [batch size, dec hid dim]\n        \n        return outputs, hidden","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        \n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        \n        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n        \n    def forward(self, hidden, encoder_outputs, mask):\n        \n        #hidden = [batch size, dec hid dim]\n        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n        #mask = [batch size, src sent len]\n        \n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n        \n        #repeat encoder hidden state src_len times\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #hidden = [batch size, src sent len, dec hid dim]\n        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n        \n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n        \n        #energy = [batch size, src sent len, dec hid dim]\n                \n        energy = energy.permute(0, 2, 1)\n        \n        #energy = [batch size, dec hid dim, src sent len]\n        \n        #v = [dec hid dim]\n        \n        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n        \n        #v = [batch size, 1, dec hid dim]\n            \n        attention = torch.bmm(v, energy).squeeze(1)\n        \n        #attention = [batch size, src sent len]\n        \n        attention = attention.masked_fill(mask == 0, -1e10)\n        \n        return F.softmax(attention, dim = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.attention = attention\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n        \n        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs, mask):\n             \n        #input = [batch size]\n        #hidden = [batch size, dec hid dim]\n        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n        #mask = [batch size, src sent len]\n        \n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(input))\n        \n        #embedded = [1, batch size, emb dim]\n        \n        a = self.attention(hidden, encoder_outputs, mask)\n                \n        #a = [batch size, src sent len]\n        \n        a = a.unsqueeze(1)\n        \n        #a = [batch size, 1, src sent len]\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n        \n        weighted = torch.bmm(a, encoder_outputs)\n        \n        #weighted = [batch size, 1, enc hid dim * 2]\n        \n        weighted = weighted.permute(1, 0, 2)\n        \n        #weighted = [1, batch size, enc hid dim * 2]\n        \n        rnn_input = torch.cat((embedded, weighted), dim = 2)\n        \n        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n            \n        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n        \n        #output = [sent len, batch size, dec hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, dec hid dim]\n        \n        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n        #output = [1, batch size, dec hid dim]\n        #hidden = [1, batch size, dec hid dim]\n        #this also means that output == hidden\n        assert (output == hidden).all()\n        \n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n        \n        #output = [bsz, output dim]\n        \n        return output, hidden.squeeze(0), a.squeeze(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.pad_idx = pad_idx\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.device = device\n        \n    def create_mask(self, src):\n        mask = (src != self.pad_idx).permute(1, 0)\n        return mask\n        \n    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src sent len, batch size]\n        #src_len = [batch size]\n        #trg = [trg sent len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n        \n        if trg is None:\n            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n            inference = True\n            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n        else:\n            inference = False\n            \n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #tensor to store attention\n        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n        \n        #encoder_outputs is all hidden states of the input sequence, back and forwards\n        #hidden is the final forward and backward hidden states, passed through a linear layer\n        encoder_outputs, hidden = self.encoder(src, src_len)\n                \n        #first input to the decoder is the <sos> tokens\n        output = trg[0,:]\n        \n        mask = self.create_mask(src)\n                \n        #mask = [batch size, src sent len]\n                \n        for t in range(1, max_len):\n            output, hidden, attention = self.decoder(output, hidden, encoder_outputs, mask)\n            outputs[t] = output\n            attentions[t] = attention\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            output = (trg[t] if teacher_force else top1)\n            if inference and output.item() == self.eos_idx:\n                return outputs[:t], attentions[:t]\n            \n        return outputs, attentions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nENC_HID_DIM = 100\nDEC_HID_DIM = 100\nENC_DROPOUT = 0.8\nDEC_DROPOUT = 0.8\nPAD_IDX = SRC.vocab.stoi['<pad>']\nSOS_IDX = TRG.vocab.stoi['<sos>']\nEOS_IDX = TRG.vocab.stoi['<eos>']\n\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM)\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n            \nmodel.apply(init_weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'Модель содержит {count_parameters(model):,} параметров')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we define our optimizer and criterion. We have already initialized `PAD_IDX` when initializing the model, so we don't need to do it again.","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        \n        src, src_len = batch.src\n        trg = batch.trg\n        \n        optimizer.zero_grad()\n        \n        output, attetion = model(src, src_len, trg, 0.4)\n        \n        #trg = [trg sent len, batch size]\n        #output = [trg sent len, batch size, output dim]\n        \n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg sent len - 1) * batch size]\n        #output = [(trg sent len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n\n            src, src_len = batch.src\n            trg = batch.trg\n\n            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n\n            #trg = [trg sent len, batch size]\n            #output = [trg sent len, batch size, output dim]\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg sent len - 1) * batch size]\n            #output = [(trg sent len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss / len(iterator)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 50\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'conala_model_attention_test.pt')\n    \n    print(f'Эпоха: {epoch+1:02} | Время: {epoch_mins}m {epoch_secs}s')\n    print(f'Перплексия (обучение): {math.exp(train_loss):7.3f}')\n    print(f'Перплексия (валидация): {math.exp(valid_loss):7.3f}')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('conala_model_attention_test.pt'))\n\ntest_loss = evaluate(model, test_iterator, criterion)\n\nprint(f'Перплексия (валидация): {math.exp(test_loss):7.3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание кода по вопросу","metadata":{}},{"cell_type":"code","source":"def translate_sentence(model, sentence):\n    model.eval()\n    tokenized = tokenize_question(sentence) \n    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>']\n    numericalized = [SRC.vocab.stoi[t] for t in tokenized] \n    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n    translation, attention = translation[1:], attention[1:]\n    return translation, attention","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_attention(candidate, translation, attention):\n    \n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    \n    attention = attention.squeeze(1).cpu().detach().numpy()\n    \n    cax = ax.matshow(attention, cmap='bone')\n   \n    ax.tick_params(labelsize=15)\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in tokenize_question(candidate)] + ['<eos>'], \n                       rotation=45)\n    ax.set_yticklabels([''] + translation)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_idx = 2\n\nsrc = ' '.join(vars(train_data.examples[example_idx])['src'])\ntrg = ' '.join(vars(train_data.examples[example_idx])['trg'])\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translation, attention = translate_sentence(model, src)\n\nprint('predicted trg = ', ' '.join(translation))\n\ndisplay_attention(src, translation, attention)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_idx = 8\n\nsrc = ' '.join(vars(valid_data.examples[example_idx])['src'])\ntrg = ' '.join(vars(valid_data.examples[example_idx])['trg'])\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translation, attention = translate_sentence(model, src)\n\nprint('predicted trg = ', ' '.join(translation))\n\ndisplay_attention(src, translation, attention)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_idx = 4\n\nsrc = ' '.join(vars(test_data.examples[example_idx])['src'])\ntrg = ' '.join(vars(test_data.examples[example_idx])['trg'])\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translation, attention = translate_sentence(model, src)\n\nprint('predicted trg = ', ''.join(translation))\n\ndisplay_attention(src, translation, attention)","metadata":{},"execution_count":null,"outputs":[]}]}