{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6647d8d9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-24T13:47:51.345192Z",
     "iopub.status.busy": "2022-10-24T13:47:51.344387Z",
     "iopub.status.idle": "2022-10-24T13:47:51.355987Z",
     "shell.execute_reply": "2022-10-24T13:47:51.354796Z"
    },
    "papermill": {
     "duration": 0.021809,
     "end_time": "2022-10-24T13:47:51.359270",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.337461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa0728",
   "metadata": {
    "papermill": {
     "duration": 0.00301,
     "end_time": "2022-10-24T13:47:51.366040",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.363030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Задача 01 - Word2Vec Model Lite ##\n",
    "\n",
    "Write a program that trains Word2Vec model. Do not use print() instructions in your code, otherwise test procedure will not succeed; the message \"Wrong Answer\" indicates answer format is incorrect (print() in the code, missing words in the dictionary, etc.). The message \"Embeddings are not good enough\" means you're on the right track and you should focus on the model improvement. In this version of the assignment the checks on the embeddings are easier.\n",
    "\n",
    "You may think of the input string as being pre-processed with the following function:\n",
    "\n",
    " '''import re\n",
    "\n",
    "import string\n",
    "\n",
    "def clean(inp: str) -> str:\n",
    "\n",
    "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "\n",
    "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
    "\n",
    "    return inp\n",
    "\n",
    "'''\n",
    "\n",
    "I.e. given the input \"Your string!\" the output will be \"your string \".\n",
    "\n",
    "\n",
    "Input: data (string) - cleaned documents without punctuation in one line\n",
    "Output: w2v_dict (dict: key (string) - a word from vocabulary, value (numpy array) - the word's embedding)\n",
    "\n",
    "Time limit: 25 seconds\n",
    "Memory limit: 128 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ab498d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T13:47:51.374863Z",
     "iopub.status.busy": "2022-10-24T13:47:51.374420Z",
     "iopub.status.idle": "2022-10-24T13:47:51.381586Z",
     "shell.execute_reply": "2022-10-24T13:47:51.380245Z"
    },
    "papermill": {
     "duration": 0.015068,
     "end_time": "2022-10-24T13:47:51.384657",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.369589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean(inp: str) -> str:\n",
    "    \n",
    "    # string.punctuation -> '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    # заменяем вышеперечисленную пункуацию на пробелы\n",
    "    inp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    # заменяем множественные пробелы на один пробел и приводим в нижний регистр\n",
    "    inp = re.sub(r'\\s+', ' ', inp.lower())\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bad6ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T13:47:51.394620Z",
     "iopub.status.busy": "2022-10-24T13:47:51.393799Z",
     "iopub.status.idle": "2022-10-24T13:47:51.407419Z",
     "shell.execute_reply": "2022-10-24T13:47:51.405799Z"
    },
    "papermill": {
     "duration": 0.022605,
     "end_time": "2022-10-24T13:47:51.410755",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.388150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как сказал дэвид лоуренс «любая полуправда в конце концов приходит к самоотрицанию в противоположной полуправде» если я вижу только мысленную карту первой альтернативы — собственную неполную карту то с одной стороны единственный способ решить проблему — убедить вас изменить вашу парадигму или даже заставить вас принять мою точку зрения альтернативную вашей это еще и единственный способ сохранить свою «я концепцию» я должен победить вы — проиграть с другой стороны отбросив собственную карту и следуя вашей — второй альтернативе я сталкиваюсь с той же проблемой вы не можете гарантировать что ваша мысленная карта сколько нибудь полна и я рискую заплатить чудовищную цену руководствуясь ею '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"\\n\".join(['Как сказал Дэвид Лоуренс, «любая полуправда в конце концов приходит к самоотрицанию в противоположной полуправде».',\n",
    " 'Если я вижу только мысленную карту первой альтернативы — собственную неполную карту, то, с одной стороны, единственный способ решить проблему — убедить вас изменить вашу парадигму или даже заставить вас принять мою точку зрения, альтернативную вашей.',\n",
    " 'Это еще и единственный способ сохранить свою «Я-концепцию»: я должен победить, вы — проиграть.',\n",
    " 'С другой стороны, отбросив собственную карту и следуя вашей — второй альтернативе, я сталкиваюсь с той же проблемой.',\n",
    " 'Вы не можете гарантировать, что ваша мысленная карта сколько-нибудь полна, и я рискую заплатить чудовищную цену, руководствуясь ею.'])\n",
    "clean_str = clean(sample)\n",
    "clean_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b6231",
   "metadata": {
    "papermill": {
     "duration": 0.003467,
     "end_time": "2022-10-24T13:47:51.418003",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.414536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Краткие выводы:** Видим что предложенный вариант предобраотки заменяет не всю пунктуаци, как минимум сотаются символы «»— и возможно другие, как следствие, на всякий случай необходимо токенизировать на основе регулярных выражений, соответсвующих возможным словам (возможно входная строка будет на английском языке, и не будет содержать незатронутой пунктуации, но лучше всего дополнительно почистить)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c6d69",
   "metadata": {
    "papermill": {
     "duration": 0.003544,
     "end_time": "2022-10-24T13:47:51.425168",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.421624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Решение ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a7c5e",
   "metadata": {
    "papermill": {
     "duration": 0.00322,
     "end_time": "2022-10-24T13:47:51.431952",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.428732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- попробовать обучать на нормальном тексте (можно на английском) после чего:\n",
    "  - посмотреть как падает лосс\n",
    "  - на практике посмотреть схожесть токенов\n",
    "- попробовать убирать стоп слова (при этом проверить на голых эмбеддингах и легком задании, допустимо ли это) \n",
    "- разные размеры эмбеддингов (50, 100, 200, 300)\n",
    "- инициализация эмбеддингов шумом определенного типа\n",
    "- разный размер окна (какой рекомендуют? какой использовался в предыдущем курсе?) ==> использовался радиут 5, т.е. полный размер 11, при этом обучение проходил полностью от начала и до конца, т.е. позиции когда окно не помещается полностью (остается контект только справа/слева, тоже попадали в обучение)\n",
    "- разный размер негатив сэмплов (какой рекомендуют? какой использовался в предыдущем курсе?)\n",
    "- посмотреть lr который использовался в предыдущем курсе ==> использовался 1e-2, но так же был шедулер)\n",
    "- делать градиентный шаг не сразу, а как бы через батч сайз\n",
    "- на выходе берем контектстные эмбеддинги или центральные\n",
    "- прочитать статьи (которая в чате) и которая рекомендована в лекции\n",
    "- возможно пробежаться по алгоритм, который использовался в предыдущем курсе\n",
    "- если совсем все плохо, и уже будет поджимать время или попытки, попробовать алгоритм из предыдущего курса, а так же написать тому, кто вел переписку в чате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f355d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T13:47:51.442116Z",
     "iopub.status.busy": "2022-10-24T13:47:51.441361Z",
     "iopub.status.idle": "2022-10-24T13:47:53.617510Z",
     "shell.execute_reply": "2022-10-24T13:47:53.616268Z"
    },
    "papermill": {
     "duration": 2.184474,
     "end_time": "2022-10-24T13:47:53.620498",
     "exception": false,
     "start_time": "2022-10-24T13:47:51.436024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, text, word_pattern=r\"[a-zа-яё0-9_]+\"):\n",
    "        self.re_word_pattern = re.compile(word_pattern)\n",
    "        list_vocab = list(set(self.re_word_pattern.findall(text.lower())))\n",
    "        self.vocab_size = len(list_vocab)\n",
    "        self.vocab_id2word = dict(zip(range(len(list_vocab)), list_vocab))\n",
    "        self.vocab_word2id = dict(zip(list_vocab, range(len(list_vocab))))\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return list(map(lambda x: self.vocab_word2id[x], \n",
    "                        self.re_word_pattern.findall(text.lower())))\n",
    "    \n",
    "    def untokenize(self, list_ids):\n",
    "        return list(map(lambda x: self.vocab_id2word[x], list_ids))\n",
    "\n",
    "\n",
    "class Word2vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, emb_dim: int, negative_size = 20):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.center_emb = torch.nn.Embedding(vocab_size, emb_dim)\n",
    "        self.context_emb = torch.nn.Embedding(vocab_size, emb_dim)\n",
    "        self.negative_size = negative_size\n",
    "        \n",
    "    def forward(self, x_window: list):\n",
    "        center_id = len(x_window) // 2  # center index in list x_window \n",
    "        \n",
    "        center_word = self.center_emb(torch.LongTensor([x_window[center_id]])).flatten()\n",
    "        context_words = self.context_emb(torch.LongTensor(x_window[:center_id] + \n",
    "                                                          x_window[center_id + 1:]))\n",
    "        context_noise_words = self.context_emb(torch.randint(0, \n",
    "                                                             self.vocab_size, \n",
    "                                                             (self.negative_size,)))\n",
    "        \n",
    "        positive_score = F.logsigmoid(context_words @ center_word)\n",
    "        negative_score = F.logsigmoid(-(context_noise_words @ center_word))\n",
    "        \n",
    "        return positive_score.sum() + negative_score.sum()\n",
    "    \n",
    "\n",
    "NUM_EPOCH = 20\n",
    "EMB_DIM = 50\n",
    "NEGATIVE_SIZE = 20\n",
    "\n",
    "def train(data: str):\n",
    "    \"\"\"\n",
    "    return: w2v_dict: dict\n",
    "            - key: string (word)\n",
    "            - value: np.array (embedding)\n",
    "    \"\"\"\n",
    "    my_tokenizer = Tokenizer(data)\n",
    "    word2vec_model = Word2vec(my_tokenizer.vocab_size, EMB_DIM, NEGATIVE_SIZE)\n",
    "    loss_func = lambda x: -x\n",
    "\n",
    "    optimizer = torch.optim.AdamW(word2vec_model.parameters(), lr=0.01)\n",
    "\n",
    "    window_size = 9\n",
    "\n",
    "    data_tokens = my_tokenizer.tokenize(data)\n",
    "\n",
    "    for n_epoch in range(NUM_EPOCH):\n",
    "        epoch_loss = 0.\n",
    "        counts = len(data_tokens) - window_size + 1\n",
    "        for position in range(counts):\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(word2vec_model(data_tokens[position:position + window_size]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        #print(epoch_loss / counts)\n",
    "        \n",
    "    return dict(zip(my_tokenizer.vocab_word2id.keys(),\n",
    "                    word2vec_model.context_emb.weight.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5df58da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T13:47:53.630482Z",
     "iopub.status.busy": "2022-10-24T13:47:53.629921Z",
     "iopub.status.idle": "2022-10-24T13:47:54.829413Z",
     "shell.execute_reply": "2022-10-24T13:47:54.828401Z"
    },
    "papermill": {
     "duration": 1.207715,
     "end_time": "2022-10-24T13:47:54.832362",
     "exception": false,
     "start_time": "2022-10-24T13:47:53.624647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(clean_str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754323c",
   "metadata": {
    "papermill": {
     "duration": 0.003191,
     "end_time": "2022-10-24T13:47:54.839162",
     "exception": false,
     "start_time": "2022-10-24T13:47:54.835971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.77516,
   "end_time": "2022-10-24T13:47:55.768369",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-24T13:47:40.993209",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
